#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import os
import uuid
import json
import time
import gradio as gr
import datetime
import requests
from urllib.parse import urljoin

from pilot.configs.model_config import LOGDIR, vicuna_model_server, LLM_MODEL

from pilot.conversation import (
    get_default_conv_template,
    compute_skip_echo_len,
    SeparatorStyle
)

from fastchat.utils import (
    build_logger,
    server_error_msg,
    violates_moderation,
    moderation_msg
)

from fastchat.serve.gradio_patch import Chatbot as grChatbot
from fastchat.serve.gradio_css import code_highlight_css

logger = build_logger("webserver", "webserver.log")
headers = {"User-Agent": "dbgpt Client"}

no_change_btn = gr.Button.update()
enable_btn = gr.Button.update(interactive=True)
disable_btn = gr.Button.update(interactive=True)

enable_moderation = False
models = []

priority = {
    "vicuna-13b": "aaa"
}

def set_global_vars(enable_moderation_):
    global enable_moderation, models
    enable_moderation = enable_moderation_

def load_demo_single(url_params):
    dropdown_update = gr.Dropdown.update(visible=True)
    if "model" in url_params:
        model = url_params["model"]
        if model in models:
            dropdown_update = gr.Dropdown.update(value=model, visible=True)

    state = None
    return (
        state,
        dropdown_update,
        gr.Chatbot.update(visible=True),
        gr.Textbox.update(visible=True),
        gr.Button.update(visible=True),
        gr.Row.update(visible=True),
        gr.Accordion.update(visible=True),
    )


get_window_url_params = """
function() {
    const params = new URLSearchParams(window.location.search);
    url_params = Object.fromEntries(params);
    console.log(url_params);
    gradioURL = window.location.href
    if (!gradioURL.endsWith('?__theme=dark')) {
        window.location.replace(gradioURL + '?__theme=dark');
    }
    return url_params;
    }
"""

def load_demo(url_params, request: gr.Request):
    logger.info(f"load_demo. ip: {request.client.host}. params: {url_params}")
    return load_demo_single(url_params)

def get_conv_log_filename():
    t = datetime.datetime.now()
    name = os.path.join(LOGDIR, f"{t.year}-{t.month:02d}-{t.day:02d}-conv.json")
    return name


def regenerate(state, request: gr.Request):
    logger.info(f"regenerate. ip: {request.client.host}")
    state.messages[-1][-1] = None
    state.skip_next = False
    return (state, state.to_gradio_chatbot(), "") + (disable_btn,) * 5

def clear_history(request: gr.Request):
    logger.info(f"clear_history. ip: {request.client.host}")
    state = None
    return (state, [], "") + (disable_btn,) * 5

def add_text(state, text, request: gr.Request):
    logger.info(f"add_text. ip: {request.client.host}. len:{len(text)}")

    if state is None:
        state = get_default_conv_template("vicuna").copy()
    
    if len(text) <= 0:
        state.skip_next = True
        return (state, state.to_gradio_chatbot(), "") + (no_change_btn,) * 5
    
    if enable_moderation:
        flagged = violates_moderation(text)
        if flagged:
            logger.info(f"violate moderation. ip: {request.client.host}. text: {text}")
            state.skip_next = True
            return (state, state.to_gradio_chatbot(), moderation_msg) + (no_change_btn,) * 5
    text = text[:1536]  # ? 
    state.append_message(state.roles[0], text)      
    state.append_message(state.roles[1], None)
    state.skip_next = False

    return (state, state.to_gradio_chatbot(), "") + (disable_btn,) * 5

def post_process_code(code):
    sep = "\n```"
    if sep in code:
        blocks = code.split(sep)
        if len(blocks) % 2 == 1:
            for i in range(1, len(blocks), 2):
                blocks[i] = blocks[i].replace("\\_", "_")
        code = sep.join(blocks)
    return code

def http_bot(state, temperature, max_new_tokens, request: gr.Request):
    logger.info(f"http_bot. ip: {request.client.host}")
    start_tstamp = time.time()

    model_name = LLM_MODEL 
    temperature = float(temperature)
    max_new_tokens = int(max_new_tokens)

    if state.skip_next:
        yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 5
        return

    if len(state.messages) == state.offset + 2:
        new_state = get_default_conv_template(model_name).copy()
        new_state.conv_id = uuid.uuid4().hex
        new_state.append_message(new_state.roles[0], state.messages[-2][1])
        new_state.append_message(new_state.roles[1], None)
        state = new_state


    prompt = state.get_prompt()
    skip_echo_len = compute_skip_echo_len(prompt)

    logger.info(f"State: {state}")
    payload = {
        "model": model_name,
        "prompt": prompt,
        "temperature": temperature,
        "max_new_tokens": max_new_tokens,
        "stop": state.sep,
    }

    logger.info(f"Request: \n {payload}")
    state.messages[-1][-1] = "â–Œ"
    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5

    try:
        response = requests.post(
            url=urljoin(vicuna_model_server, "generate_stream"),
            headers=headers,
            json=payload,
            stream=True,
            timeout=60,
        )
        for chunk in response.iter_lines(decode_unicode=False, delimiter=b"\0"):
            if chunk:
                data = json.loads(chunk.decode())
                logger.info(f"Response: {data}")
                if data["error_code"] == 0:
                    output = data["text"][skip_echo_len].strip()
                    output = post_process_code(output)
                    state.messages[-1][-1] = output + "â–Œ"
                    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5
                else:
                    output = data["text"] + f" (error_code): {data['error_code']}"
                    state.messages[-1][-1] = output
                    yield (state, state.to_gradio_chatbot()) + (
                        disable_btn,
                        disable_btn,
                        disable_btn,
                        enable_btn,
                        enable_btn
                    )

                    return
                time.sleep(0.02)
    except requests.exceptions.RequestException as e:
        state.messages[-1][-1] = server_error_msg + f" (error_code: 4)"
        yield (state, state.to_gradio_chatbot()) + (
            disable_btn,
            disable_btn,
            disable_btn,
            enable_btn,
            enable_btn
        )
        return

    state.messages[-1][-1] = state.messages[-1][-1][:-1]
    yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 5
    
    finish_tstamp = time.time()
    logger.info(f"{output}")

    with open(get_conv_log_filename(), "a") as flog:
        data = {
            "tstamp": round(finish_tstamp, 4),
            "type": "chat",
            "model": model_name,
            "gen_params": {
                "temperature": temperature,
                "max_new_tokens": max_new_tokens,
            },
            "start": round(start_tstamp, 4),
            "finish": round(finish_tstamp, 4),
            "state": state.dict(),
            "ip": request.client.host,
        }
        flog.write(json.dumps(data), + "\n")

block_css = (
    code_highlight_css
    + """
pre {
    white-space: pre-wrap;       /* Since CSS 2.1 */
    white-space: -moz-pre-wrap;  /* Mozilla, since 1999 */
    white-space: -pre-wrap;      /* Opera 4-6 */
    white-space: -o-pre-wrap;    /* Opera 7 */
    word-wrap: break-word;       /* Internet Explorer 5.5+ */
}
#notice_markdown th {
    display: none;
}
    """
)


def build_single_model_ui():
   
    notice_markdown = """
    # DB-GPT
    
    [DB-GPT](https://github.com/csunny/DB-GPT) æ˜¯ä¸€ä¸ªå®žéªŒæ€§çš„å¼€æºåº”ç”¨ç¨‹åºï¼Œå®ƒåŸºäºŽ[FastChat](https://github.com/lm-sys/FastChat)ï¼Œå¹¶ä½¿ç”¨vicunaä½œä¸ºåŸºç¡€æ¨¡åž‹ã€‚æ­¤å¤–ï¼Œæ­¤ç¨‹åºç»“åˆäº†langchainå’Œllama-indexåŸºäºŽçŽ°æœ‰çŸ¥è¯†åº“è¿›è¡ŒIn-Context Learningæ¥å¯¹å…¶è¿›è¡Œæ•°æ®åº“ç›¸å…³çŸ¥è¯†çš„å¢žå¼º, æ€»çš„æ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºŽæ•°æ®åº“çš„å¤æ‚ä¸”åˆ›æ–°çš„AIå·¥å…·ã€‚å¦‚æžœæ‚¨å¯¹å¦‚ä½•åœ¨å·¥ä½œä¸­ä½¿ç”¨æˆ–å®žæ–½DB-GPTæœ‰ä»»ä½•å…·ä½“é—®é¢˜ï¼Œè¯·è”ç³»æˆ‘, æˆ‘ä¼šå°½åŠ›æä¾›å¸®åŠ©, åŒæ—¶ä¹Ÿæ¬¢è¿Žå¤§å®¶å‚ä¸Žåˆ°é¡¹ç›®å»ºè®¾ä¸­, åšä¸€äº›æœ‰è¶£çš„äº‹æƒ…ã€‚ 
    """
    learn_more_markdown = """ 
        ### Licence
        The service is a research preview intended for non-commercial use only. subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA 
    """

    state = gr.State()
    notice = gr.Markdown(notice_markdown, elem_id="notice_markdown")

    with gr.Accordion("å‚æ•°", open=False, visible=False) as parameter_row:
        temperature = gr.Slider(
            minimum=0.0,
            maximum=1.0,
            value=0.7,
            step=0.1,
            interactive=True,
            label="Temperature",
        )

        max_output_tokens = gr.Slider(
            minimum=0,
            maximum=1024,
            value=512,
            step=64,
            interactive=True,
            label="æœ€å¤§è¾“å‡ºTokenæ•°",
        )

    chatbot = grChatbot(elem_id="chatbot", visible=False).style(height=550)
    with gr.Row():
        with gr.Column(scale=20):
            textbox = gr.Textbox(
                show_label=False,
                placeholder="Enter text and press ENTER",
                visible=False,
            ).style(container=False)

        with gr.Column(scale=2, min_width=50):
            send_btn = gr.Button(value="" "å‘é€", visible=False)

        
    with gr.Row(visible=False) as button_row:
        regenerate_btn = gr.Button(value="ðŸ”„" "é‡æ–°ç”Ÿæˆ", interactive=False)
        clear_btn = gr.Button(value="ðŸ—‘ï¸" "æ¸…ç†", interactive=False)

    gr.Markdown(learn_more_markdown)

    btn_list = [regenerate_btn, clear_btn]
    regenerate_btn.click(regenerate, state, [state, chatbot, textbox] + btn_list).then(
        http_bot,
        [state, temperature, max_output_tokens],
        [state, chatbot] + btn_list,
    )
    clear_btn.click(clear_history, None, [state, chatbot, textbox] + btn_list)
    
    textbox.submit(
        add_text, [state, textbox], [state, chatbot, textbox] + btn_list
    ).then(
        http_bot,
        [state, temperature, max_output_tokens],
        [state, chatbot] + btn_list,
    )

    send_btn.click(
        add_text, [state, textbox], [state, chatbot, textbox] + btn_list
    ).then(
        http_bot,
        [state, temperature, max_output_tokens],
        [state, chatbot] + btn_list
    )

    return state, chatbot, textbox, send_btn, button_row, parameter_row


def build_webdemo():
    with gr.Blocks(
        title="æ•°æ®åº“æ™ºèƒ½åŠ©æ‰‹",
        # theme=gr.themes.Base(),
        theme=gr.themes.Default(),
        css=block_css,
    ) as demo:
        url_params = gr.JSON(visible=False)
        (
            state,
            chatbot,
            textbox,
            send_btn,
            button_row,
            parameter_row,
        ) = build_single_model_ui()

        if args.model_list_mode == "once":
            demo.load(
                load_demo,
                [url_params],
                [
                    state,
                    chatbot,
                    textbox,
                    send_btn,
                    button_row,
                    parameter_row,
                ],
                _js=get_window_url_params,
            )
        else:
            raise ValueError(f"Unknown model list mode: {args.model_list_mode}")
    return demo

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", type=str, default="0.0.0.0")
    parser.add_argument("--port", type=int)
    parser.add_argument("--concurrency-count", type=int, default=10)
    parser.add_argument(
        "--model-list-mode", type=str, default="once", choices=["once", "reload"]
    )
    parser.add_argument("--share", default=False, action="store_true")
    parser.add_argument(
        "--moderate", action="store_true", help="Enable content moderation"
    )
    args = parser.parse_args()
    logger.info(f"args: {args}")

    set_global_vars(args.moderate)

    logger.info(args)
    demo = build_webdemo()
    demo.queue(
        concurrency_count=args.concurrency_count, status_update_rate=10, api_open=False
    ).launch(
        server_name=args.host, server_port=args.port, share=args.share, max_threads=200,
    )