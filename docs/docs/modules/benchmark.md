# Ant Group Data Retrieval Benchmark Dataset Guide

For Text2SQL tasks, we provide a dataset benchmarking capability. It evaluates different large language models (LLMs) and agents on Text2SQL, covering syntax correctness, semantic accuracy, and execution validity. It outputs metrics such as executability rate and accuracy rate, and provides an evaluation report.

1. Open-source Text2SQL dataset repository by Ant Group: [Falcon](https://github.com/eosphoros-ai/Falcon)
2. DB-GPT supports LLM evaluation based on the Falcon benchmark dataset

# Introduction

To objectively and fairly evaluate models on Text2SQL tasks, we provide a benchmarking module and dataset. This module supports comprehensive evaluation of all models in the DB-GPT framework and provides an evaluation report.

The benchmark dataset used by the module, [Falcon](https://github.com/eosphoros-ai/Falcon), is a high-quality and evolving open-source Text2SQL dataset from Ant Group.
The dataset aims to stress-test models in complex, cross-domain analysis scenarios, with a focus on:
 - SQL computation challenges â€” multi-table joins, nested CTEs, window functions, ranking, type casting, regex filters...
 - Language challenges â€” Chinese fuzzy time expressions, colloquial business terms, ellipsis, multi-intent questions...

> The benchmark includes 28 datasets and 90 tables. As of now, 500 Chinese questions of varying difficulty have been officially released.
> 
> Among them: easy: 151, medium: 130, hard: 219.

## Core Features Of Benchmark Dataset
-  âœ… Multi-dimensional evaluation: three-layer checks on syntax correctness, semantic accuracy, and execution validity
-  ğŸ§  Dynamic difficulty levels: 500 Chinese questions from Kaggle datasets (various difficulties), covering multi-step reasoning, complex nested queries, and advanced SQL features
-  âœï¸ Detailed schema annotations: rich schema information including data types, natural language aliases, table relations, and sample data, helping models understand database structures
-  ğŸŒ Real-world scenario modeling: more ambiguous language expressions and more questions collected from Ant Groupâ€™s real production scenarios (in preparation)


# System Design
Core capabilities of the benchmarking module:
- Text2SQL evaluation API: provide APIs to create evaluation tasks
- Benchmark execution framework: run Text2SQL tasks based on the benchmark questions
- Result comparison framework: compare results between the standard answers and LLM-generated SQL, and aggregate the evaluation results
- Dataset installation and database mapping: install the benchmark dataset and map data into the database to provide LLM SQL query service

<p align="center">
  <img src={'/img/module/benchmark.png'} width="600px" />
</p>

# Evaluation Metrics

| Metric             | Formula                                                 | Description                                                                                                                                             |
|--------------------|---------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|
| Executability Rate | Number of syntactically correct samples / Total samples | The proportion of SQL statements generated by the model that are syntactically correct and can execute correctly in the database                        |
| Accuracy Rate      | Number of semantically correct samples / Total samples  | The proportion of SQL statements generated by the model that are syntactically correct, execute correctly in the database, and are semantically correct |


# Dataset Structure

## Standard Benchmark Structure
| Field   | Description                                                                                            | example                                                                      |
|---------|--------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| ç¼–å·      | Question serial number                                                                                 | 1, 2...                                                                      |
| æ•°æ®é›†ID   | Dataset ID                                                                                             | D2025050900161503000025249569, ...                                           |
| ç”¨æˆ·é—®é¢˜    | Question title                                                                                         | å„æ€§åˆ«çš„å¹³å‡å¹´é¾„æ˜¯å¤šå°‘ï¼Œå¹¶æŒ‰å¹´é¾„é¡ºåºæ˜¾ç¤ºç»“æœï¼Ÿ                                                      |
| è‡ªå®šä¹‰æ ‡ç­¾   | Question source, SQL type                                                                              | KAGGLE_DS_1, CTE1                                                            |
| çŸ¥è¯†      | Knowledge context required                                                                             | æš‚æ—                                                                            |
| æ ‡å‡†ç­”æ¡ˆSQL | Correct SQL for the question(based on Alibaba Cloud MaxCompute syntax)                                 | SELECT gender, AVG(age) AS avg_age FROM users GROUP BY gender ORDER BY avg_age |
| æ ‡å‡†ç»“æœ    | Correct SQL query result on the Alibaba Cloud MaxCompute engine (some questions have multiple answers) | `{"æ€§åˆ«":["Female","Male"],"å¹³å‡å¹´é¾„":["27.73","27.84"]}`                              |
| æ˜¯å¦æ’åº    | Whether the question involves sorting                                                                  | `{"æ€§åˆ«":["Female","Male"],"å¹³å‡å¹´é¾„":[27.73,27.84]}`                                  |
| prompt  | Model conversation prompt                                                                              | å·²çŸ¥ä»¥ä¸‹æ•°æ®é›†ï¼ŒåŒ…å«äº†å­—æ®µååŠå…¶é‡‡æ ·ä¿¡æ¯ï¼š...                                                       |


# How To Use

## Environment Setup
- Step1: Upgrade to V0.7.4 and upgrade the metadata database

    For SQLite, the table schema is upgraded automatically by default. For MySQL, you need to run the DDL manually. The file assets/schema/dbgpt.sql contains the complete DDL for the current version. Version-specific DDL changes can be found under assets/schema/upgrade. For example, if you are upgrading from v0.7.1 to v0.7.4, you can run the following DDL:
 
    ```
    mysql -h127.0.0.1 -uroot -p{your_password} < assets/schema/upgrade/v0_7_4/upgrade_to_v0.7.4.sql
    ```

- Step2: Start the DB-GPT service, and wait for the benchmark dataset to load automatically. When you see the log line, the dataset has finished loading (about 1~5 minute).

<p align="left">
  <img src={'/img/module/benchmark/env_load.png'} width="1000px"/>
</p>

- Step3: Register LLM on the DB-GPT platform 
  - Method 1: Configure via configuration file. Reference: [ProxyModel Configuration](http://docs.dbgpt.cn/docs/next/installation/advanced_usage/More_proxyllms)
  - Method 2: Configure via product page. Reference: [Models](http://docs.dbgpt.cn/docs/next/application/llms)

## Create Evaluation Task
- Step1: Click "Create Benchmark" to create an evaluation task
- Step2: Enter the task name and select model list
- Step3: Submit the task

<p align="left">
  <img src={'/img/module/benchmark/benchmark_create.png'} width="1000px"/>
</p>

- Step4: Wait for the task to complete (evaluation may take a long time)

<p align="left">
  <img src={'/img/module/benchmark/benchmark_list.png'} width="1000px"/>
</p>


## View Evaluation Results
- When the status is "Completed", click "View Details" to see the evaluation report
- The report shows:
  - Total number of models, number of questions, numbers of correct, incorrect, and failed questions
  - For each round and model: numbers of executed, correct, incorrect, and failed questions; executability rate; accuracy rate
  - Bar charts for executability rate and accuracy rate

> Correct: the model answered the question correctly. Incorrect: the SQL generated by the model is syntactically correct but semantically wrong. Failed: usually the SQL is syntactically or semantically wrong.

<p align="left">
  <img src={'/img/module/benchmark/benchmark_report.png'} width="1000px"/>
</p>

## Download Evaluation Results
- Click "Download Evaluation Results" to download the detailed Excel report
- The Excel report includes LLM execution details and comparison results (shown in different sheets)

<p align="left">
  <img src={'/img/module/benchmark/excel_info.png'} width="1000px"/>
</p>

## Dataset Details
- Click "View Dataset Details" to view benchmark details
  - Shows tables, fields, and sample data of the Falcon dataset

<p align="left">
  <img src={'/img/module/benchmark/dataset_info.png'} width="1000px"/>
</p>


# Excel Evaluation Result Data Structure

## Excel Evaluation Result Example

### Execution Result Example

<p align="left">
  <img src={'/img/module/benchmark/benchmark_excel_execute_result.png'} width="1000px"/>
</p>

### Execution Result Data Structure
- **Sheet name: dataset_evaluation_result**

| Field     | Description                           | example                                                                                                                                 |
|-----------|---------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| ç¼–å·        | Question serial number                | 1, 2...                                                                                                                                 |
| å¤§æ¨¡å‹åç§°     | Name of the evaluated model           | DeepSeek-V3.1                                                                                                                           |
| è½®æ¬¡        | Evaluation round                      | 1                                                                                                                                       |
| æ•°æ®é›†ID     | Dataset ID for the question           | D2025050900161503000025249569                                                                                                           |
| ç”¨æˆ·é—®é¢˜      | Evaluation question                   | å„æ€§åˆ«çš„å¹³å‡å¹´é¾„æ˜¯å¤šå°‘ï¼Œå¹¶æŒ‰å¹´é¾„é¡ºåºæ˜¾ç¤ºç»“æœï¼Ÿ                                                                                                                 |
| è‡ªå®šä¹‰æ ‡ç­¾     | Question source, SQL type             | KAGGLE_DS_1, CTE1                                                                                                                       |
| çŸ¥è¯†        | Knowledge context required            | æš‚æ—                                                                                                                                       |
| prompt    | Model conversation prompt             | å·²çŸ¥ä»¥ä¸‹æ•°æ®é›†ï¼ŒåŒ…å«äº†å­—æ®µååŠå…¶é‡‡æ ·ä¿¡æ¯ï¼š...                                                                                                                |
| Coté•¿åº¦     | CoT tokens consumed                   | 100                                                                                                                                     |
| LLMè¾“å‡ºç»“æœ   | SQL generated by the LLM              | select gender as `gender`, avg(cast(age as real)) as `average_age` from di_finance_data group by gender order by avg(cast(age as real)) |
| ç»“æœæ‰§è¡Œ      | Query result of the LLM-generated SQL | `{"æ€§åˆ«":["Female","Male"],"å¹³å‡å¹´é¾„":[27.73,27.84]}`                                                                                           |
| æ‰§è¡Œç»“æœçš„æŠ¥é”™ä¿¡æ¯ | Error message if the SQL fails        |                                                                                                                                         |
| traceId   | Log ID                                | æš‚æ—                                                                                                                                       |
| è€—æ—¶ï¼ˆç§’ï¼‰     | Time consumed                         | 10                                                                                                                                      |


## Comparison Result Example

### Comparison Result Example

<p align="left">
  <img src={'/img/module/benchmark/benchmark_excel_compare_result.png'} width="1000px"/>
</p>

### Comparison Result Data Structure
- **Sheet name: benchmark_compare_result**

| Field             | Description                                                                                            | example                                                                                                                                 |
|-------------------|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| serialNo          | Question serial number                                                                                 | 1, 2...                                                                                                                                 |
| analysisModelId   | Dataset ID for the question                                                                            | D2025050900161503000025249569                                                                                                           |
| question          | Evaluation question                                                                                    | å„æ€§åˆ«çš„å¹³å‡å¹´é¾„æ˜¯å¤šå°‘ï¼Œå¹¶æŒ‰å¹´é¾„é¡ºåºæ˜¾ç¤ºç»“æœï¼Ÿ                                                                                                                 |
| selfDefineTags    | Question source, SQL type                                                                              | KAGGLE_DS_1, CTE1                                                                                                                       |
| prompt            | Model conversation prompt                                                                              | å·²çŸ¥ä»¥ä¸‹æ•°æ®é›†ï¼ŒåŒ…å«äº†å­—æ®µååŠå…¶é‡‡æ ·ä¿¡æ¯ï¼š...                                                                                                                |
| standardAnswerSql | Correct SQL for the question(based on Alibaba Cloud MaxCompute syntax)                                 | select gender as `gender`, avg(cast(age as real)) as `average_age` from di_finance_data group by gender order by avg(cast(age as real)) |
| standardAnswer    | Correct SQL query result on the Alibaba Cloud MaxCompute engine (some questions have multiple answers) | `{"æ€§åˆ«": ["Female", "Male"], "å¹³å‡å¹´é¾„": ["27.73", "27.84"]}`                                                                                  |
| llmCode           | Evaluated model name                                                                                   | DeepSeek-V3.1                                                                                                                           |
| llmOutput         | SQL generated by the LLM                                                                               | select gender as `gender`, avg(cast(age as real)) as `average_age` from di_finance_data group by gender order by avg(cast(age as real)) |
| executeResult     | Query result of the LLM-generated SQL                                                                  | `{"æ€§åˆ«":["Female","Male"],"å¹³å‡å¹´é¾„":[27.73,27.84]}`                                                                                           |
| errorMsg          | Comparison error message                                                                               |                                                                                                                                         |
| compareResult     | Comparison result between the reference answer and the LLM output                                      | RIGHT: correct; WRONG: incorrect; FAILED: failed (usually the SQL has issues)                                                           |


# Currently Supported Evaluation Capabilities

## Metrics
| Metric             | Supported |
|--------------------|-----------|
| Executability Rate | âœ…         |
| Accuracy Rate      | âœ…         | 

## Datasets
| Dataset | Supported |
|---------|-----------|
| Falcon  | âœ…         |

## Input/Output Formats
| Format | Supported |
|--------|-----------|
| Excel  | âœ…         |
| CSV    | âŒ         |
| JSON   | âŒ         |
| Yuque  | âŒ         |

## Databases
| Database Type | Supported |
|---------------|-----------|
| SQLite        | âœ…         |
| MySQL         | âŒ         |
| ODPS          | âŒ         |


# Features
- [x] Support single-round multi-model evaluation
- [x] Support Excel File
- [x] Support SQLite Database
- [ ] Support multi-round evaluation
- [ ] Support evaluating agents
- [ ] Support different data sources
- [ ] Support CSV, JSON, Yuque, and other file systems

