# ThoT：解开混乱上下文之谜

## Abstract

1. 背景介绍
    大型语言模型（LLMs）：近年来，LLMs 在自然语言处理领域中取得了突破性进展，能够出色地完成各种与文本相关的任务，比如理解问题、生成回答、总结内容等。然而，在某些复杂的场景下，比如面对大量无关信息或“干扰项”（distractors），模型会出现“遗漏信息”或错误推理的现象。
    挑战：混乱上下文：混乱上下文的典型特征包括干扰信息、长文档中与任务无关的部分等。这些都会导致模型难以识别关键信息，进而影响回答质量。、

2. Thread of Thought (ThoT) 策略的核心思想
    - 灵感来源：ThoT 的设计灵感来源于人类的思维过程。人类在解决复杂问题时，通常会将问题拆解为若干子问题，逐步筛选并整合相关信息，从而得出答案。
    - 工作原理：
        - 上下文分段：
            - 将长文档或混乱的上下文分成多个小的逻辑单元（段落、句子或子任务）。
            - 避免模型被无关信息“淹没”，通过分段简化处理任务。
        - 筛选关键信息：
            - 对每个分段的信息进行筛选，识别出与问题直接相关的部分，丢弃干扰信息。
        - 整合推理：
            - 将筛选出的相关信息整合，通过逐步推理生成最终的回答。

## Instruction
### 背景概述
LLMs 的重要性与广泛应用：LLMs 是现代人工智能技术的核心突破之一，在多种自然语言任务中展现了卓越能力。例如：
- 情感分析：分析文本中情感倾向（例如，正面或负面情绪）。
- 机器翻译：将文本从一种语言翻译到另一种语言。
- 文本摘要生成：从长文档中提取关键信息，生成简洁摘要。

除此之外，LLMs 在多个专业领域也有广泛应用，例如：
- 法律咨询：帮助分析法律案例、撰写法律文档。
- 医学诊断：支持医生解读患者病史、生成诊断建议。
- 混乱上下文（Chaotic Context）的定义与挑战：
- 定义：混乱上下文是指输入文本中包含大量复杂、杂乱的信息源，这些信息可能是相关的，也可能完全无关。
    - 区别于长上下文：
        - 长上下文主要关注文本的长度，而混乱上下文强调信息的复杂性和多样性。
        - 混乱上下文的问题在于模型需要从庞杂的信息中准确提取关键内容，同时忽略无关信息。
- 挑战：在混乱上下文中，模型难以有效提取相关信息，尤其当关键信息隐藏在上下文的中间部分时。

### 现有解决方案与不足
- 现有方法概览：
    - 基于检索增强的生成（RAG）：将检索到的外部知识（例如知识库中的文档）与生成任务相结合，扩展输入上下文。
    - 长上下文扩展：通过模型的输入容量优化（例如位置插值或提示压缩）来处理更长的文本。
    - 链式思维提示（CoT）：提供逐步推理的提示，使模型能够按逻辑顺序回答问题。

- 代表性方法与局限性：
- Xu 等（2023）的方法：优化模型处理长上下文的能力，但需要对 LLMs 进行重新训练，代价高昂。
- Jiang 等（2023）的方法（LongLLMLingua）：通过辅助模型删除无关信息，但存在以下问题：
    - 依赖辅助模型：需要微调额外的模型。
    - 处理未知内容的局限性：辅助模型可能无法适应复杂或未知的上下文。
    - 错误传播：非端到端架构可能将中间步骤中的错误传递到最终输出中。
- 链式思维提示（CoT）：虽然 CoT 在无需重新训练的情况下增强了模型的推理能力，但当上下文信息庞杂时，仍可能遗漏关键信息。

### ThoT 的背景与核心理念
- 背景问题：
    - LLMs 在处理混乱上下文（Chaotic Context）时容易被无关信息干扰，难以精准提取对问题最相关的内容。
    - 当前一些复杂的解决方案（如多阶段提示、多路径采样）虽然有效，但复杂度高，难以直接应用到实际场景。

- ThoT 的核心概念：
    - 灵感来源：ThoT 借鉴人类思维的方式。人类在处理复杂信息时，会保持“思维的连续性”，逐步筛选和理解信息。
    - 主要功能：
        - 系统分段：将长文本分为多个部分，逐段分析，以降低信息复杂度。
        - 逐步分析与总结：在分析每一段信息的过程中进行总结，从而提高对整体上下文的理解能力。
        - 平衡注意力：确保对文档各部分的注意力分配合理，避免过度依赖某一部分，导致遗漏其他重要信息。

- 优势：
    - 精简高效：与其他复杂的提示方法相比，ThoT 不需要多阶段操作，而是作为一个“即插即用”的模块直接整合到现有的 LLMs 和提示框架中。
    - 提升推理能力：通过结构化的分段分析和筛选信息，增强 LLMs 在复杂语境下的推理表现。

### ThoT 的应用场景与技术实现
- 应用场景：
    - 长尾问答（Long-Tail QA）：例如 PopQA 和 EntityQ 数据集，这些数据集中包含的知识通常是 LLMs 预训练时未见过的，因此需要模型依赖上下文推理，而不是依赖记忆。
    - 多轮对话（Multi-Turn Conversations）：日常对话中上下文信息庞杂且层层递进。ThoT 通过分段处理和逐步总结，可以减少对无关对话的干扰。

- 技术实现：
    1. 分段处理（Segmentation）：
        - 将混乱的长上下文切分为逻辑片段。
        - 每个片段单独分析，确保模型专注于小范围的关键信息。
    2. 逐步总结与整合（Stepwise Summarization and Integration）：
        - 分析完每一段后，生成总结，作为输入提供给后续的推理过程。
        - 最终将各段总结信息进行整合，得出结论。
    3. 平衡注意力（Balanced Attention Allocation）：
        - 对每段文本的权重分配保持平衡，避免因为位置问题（例如关键信息在中间段落）而导致遗漏。

## Related Work

### 长上下文大语言模型

1. 背景说明：传统上下文窗口的局限性
    大语言模型在处理长文本时，以往的做法是使用预定义的固定窗口（即上下文窗口），但这种方法在面对更复杂或超长文本时会遇到瓶颈，无法充分利用整个上下文信息。因此，研究者提出了各种创新方法以解决这些问题。

2. 并行上下文窗口（PCW）：
    - 原理：将大段文本划分为多个较小的窗口，每个窗口独立处理，避免信息在单一窗口中出现过多的干扰。
    - 优点：通过独立的注意力机制提高对上下文的管理能力，减少信息丢失或干扰。

3. 位置索引对齐（Chen 等人的方法）：
    - 改进点：与预训练时的最大位置索引对齐，确保模型在扩展上下文长度时不需要从零开始重新训练。
    - 微调需求小：模型只需很少的微调即可适应更长的上下文，节省计算资源。

4. 膨胀注意力（LongNet）：
    - 机制：通过膨胀注意力机制，模型的注意力范围随着距离成指数级增长，使其可以高效地处理更长距离的上下文关联。
    - 优势：不再需要线性扩展的计算成本来覆盖长距离上下文，提高了计算效率。

5. 注意力收敛（Xiao 等人）：
    - 现象描述：在长文本处理过程中，初始 Token 的 Key-Value 状态保留可以增强后续窗口的注意力机制。
    - 优化效果：这一策略帮助模型更好地集中注意力于重要信息，从而提升性能。

6. 线性偏置注意力（ALiBi）：
    - 技术特点：为查询键（Query-Key）注意力分数引入线性距离偏置，使模型对长序列的处理具有更好的效率和表现。
    - 效果对比：与专门针对长序列优化的模型相比，ALiBi 在性能上几乎持平，但实现更加简单。

7. 长上下文 vs 混乱上下文：
    - 长上下文：主要指上下文长度较长的情况，信息可能较为整齐排列，但超出了模型的处理范围。
    - 混乱上下文：不仅仅是上下文长，而是信息高度混杂，包括大量无关、相似的信息，这会对模型的理解和推理能力造成更大挑战。

### 大语言模型的推理能力

1. 背景说明：复杂推理任务的挑战与 LLMs 的进展
    大语言模型在推理任务中取得了很大进步，尤其是在需要多步骤思考和推理的任务中。为了让模型能够像人类一样解决复杂问题，研究者开发了多种推理框架和策略，逐步改进了 LLM 的推理能力。

2. 思维链（Chain of Thought, CoT）：
    - 方法概述：通过生成中间推理步骤，帮助模型在逐步解决问题的过程中保持逻辑一致性。
    - 应用场景：例如在算术运算、常识推理和符号推理任务中，CoT 方法能显著提升准确率。

3. 意义：它为后续更复杂的推理方法（如 ToT 和 GoT）奠定了理论基础。
    - 思维图（Graph of Thoughts, GoT）：
    - 创新点：将 LLM 的输出形式化为图结构，通过图形化的方式组织和表示推理步骤。
    - 优势：这种方法可以捕捉更复杂的推理路径，避免传统线性方法的局限性，从而提高模型的效率和问题解决能力。

4. 思维树（Tree of Thoughts, ToT）：
    - 扩展 CoT 的思路：与线性的 CoT 不同，ToT 将推理过程组织成树形结构，允许模型探索多种可能的推理路径。
    - 成功案例：例如在解决 24 点游戏（需要尝试多个可能的运算路径）时，ToT 表现出了很高的效率和准确性。

5. 从难到易（Least-to-Most）提示：
    - 方法描述：将复杂问题分解为更小、更简单的子问题，然后逐步解决这些子问题，最终得出复杂问题的答案。
    - 效果：特别适用于需要符号操作的复杂任务，如逻辑推理和数学公式推导。

6. 非线性推理（GoT 的扩展）：
    - 方法特点：与线性的 CoT 方法相比，GoT 的非线性推理方法可以更灵活地探索多种解决方案，适应更多样化的任务。
    - 实验表现：在数学和金融领域的数据集上，非线性方法显著优于传统的线性推理方法。

7. 现有方法的局限性：
    - 问题点：尽管这些方法在提升推理能力方面表现出色，但它们主要聚焦于长上下文或复杂推理任务，对混乱上下文（即信息过载且无关信息干扰多的情况）并没有提供足够的解决方案。
    - 混乱上下文的挑战：当输入包含大量相似但无关的信息时，模型可能难以有效提取关键细节，并容易受到干扰。

### 知识跟踪（Knowledge Following）在长上下文中的表现

1. 长上下文处理的挑战
    - 问题来源：
        - 在长上下文中，信息量巨大，模型必须在大量相关和不相关的信息中找到关键内容。
        - 即使 LLM 有较大的上下文窗口，性能也可能因为注意力被分散而下降，尤其是在信息被“埋藏”得较深时。
    - 性能下降原因：
        - LLM 的注意力机制通常无法有效覆盖整个长文本，容易导致遗漏关键信息。
        - 过长的上下文可能超出模型的输入长度限制，进一步降低其处理能力。

2. StreamingLLM 框架
    - 介绍：
        - StreamingLLM 是一个专为流式应用设计的框架，解决了传统 LLM 难以处理无限长序列的问题。
        - 优点：无需对现有的 LLM 进行额外微调，即可高效处理动态生成的长文本序列（例如，实时聊天或实时日志分析）。
    - 实现方法：
        - 模型分段地处理长序列，并保持段与段之间的上下文信息流动。
        - 通过动态更新注意力窗口，模型能够连续处理输入的流数据，而不会丢失关键上下文。

3. 检索增强（Retrieval Augmentation）
    - 核心思想：
        - 通过外部检索机制，将与当前任务相关的内容检索出来，作为模型的辅助输入，减少模型对大容量输入的依赖。
    - 研究成果：
        - Xu 等人（2023）发现，一个具备 4K 上下文窗口的 LLM，通过检索增强技术，可以达到性能更强的 16K 上下文窗口模型的水平。
        - 关键点：检索增强的效果甚至可以替代通过位置插值技术进行的模型微调（后者需要大量计算资源和时间）。

4. LongLLMLingua 和提示压缩
    - 提示压缩（Prompt Compression）：通过对输入提示进行压缩（去除冗余或不相关信息），优化模型的输入。
    - 作用：
        - 增强模型对关键信息的感知能力。
        - 减少输入长度，提高推理效率。
    - 研究结果：
        - Jiang 等人（2023）提出的 LongLLMLingua 技术，显著提升了 LLM 在长上下文任务中的表现。
        - 意义：模型不再局限于机械地处理长文本，而是通过更智能的提示优化，快速聚焦于重要信息。

Thot zero shot 示例：

    输入
        输入部分描述了任务：作为内容审查员，你被提供了关于某个问题的多个检索段落，并需要回答这个问题。

    检索段落
        段落1：介绍自动售书机。
        段落2：提到卡尔斯巴德法令，并迅速过渡到安东·菲利普·雷克勒姆（Anton Philipp Reclam）。
        段落3：提供了一个在莱比锡的地址。
        段落4：指出雷克勒姆出版社留在莱比锡。
        段落5：讨论德尔坎（Delcam），一个供应商。
        段落6：提到居民被称为“recrinos”。
        段落7：提到“政治上不可靠”的作家。
        段落8：谈到Recheio是最大的。
        段落9：通过互联网大量传输数据。
        段落10：提到无需支付任何版税。

    问题
        提出的问题是：“雷克勒姆在哪里成立？”

    第一输出
        在第一输出中，总结了相关段落的信息：
            段落2：表明安东·菲利普·雷克勒姆成立了自己的出版社。
            段落4：确认雷克勒姆出版社留在莱比锡。
            段落7：提到德国分裂，暗示了历史背景。

    分析
        根据相关段落，可以推断出雷克勒姆最初是在德国莱比锡成立的。提到德国分裂的段落暗示了可能有搬迁或重大变化，但最初的成立地点是莱比锡。

    第二输出
        最终答案明确陈述：“雷克勒姆最初在莱比锡成立。”

    总结
        这个过程包括筛选多个段落以找到相关信息，总结关键点，然后综合这些信息来准确回答问题。流程图有效地展示了如何将分散的信息整合起来，提供一个清晰而简洁的答案。
    
## ThoT 方法论

### 概述
- 问题：CoT 的局限性：
    - CoT 提示方法对复杂和无序的上下文往往难以驾驭，因为其推理逻辑容易被信息的混乱程度所干扰（如同时存在交织和零散的信息）。这可能导致逻辑推理链条中断或丢失关键内容。
- 解决方案：ThoT 的优势：
    - 逻辑连续性：ThoT 提示能够保持逻辑推理的连贯性，即使面对混乱的上下文也不会被淹没。
    - 逐步分析：通过将复杂上下文分成可管理的部分，ThoT 提示逐步总结和分析信息，确保模型不会遗漏关键信息。
    - 简单提示设计：无需像 CoT 那样依赖多个复杂提示步骤，ThoT 仅需两次提示即可完成推理任务。

- ThoT 提示 vs. 提示压缩方法（Prompt Compression）
- 提示压缩的不足：
    - 不熟悉的内容：当上下文包含模型未见过的知识或背景信息时，提示压缩无法有效提取相关信息。
    - 复杂内容：特别复杂或高度专业化的内容可能会让提示压缩失效。

    这些方法通常需要对大语言模型（LLMs）进行显著修改，如利用额外的数据集重新训练或微调模型（Xu 等人，2023；Jiang 等人，2023）。

- ThoT 的解决方案：
    - 无需修改模型：ThoT 提示方法是一种通用解决方案，不需要额外的数据集、重新训练或微调模型。
    - 更高的适配性：ThoT 提示不仅能有效管理复杂和混乱的上下文，还简化了提示过程，从而降低了实现复杂推理任务的门槛。

### 第一步：启动推理过程
    这个阶段的目的是为了解析和理解一个给定的上下文或问题。它使用特定的指令来引导LLM开始这一过程。

#### 处理流程
1. 初始提示的设计
    - 提示是为了帮助LLM更好地理解和处理输入信息而设计的。这里的提示特别强调了“逐步带领我解析这个上下文，将其分为易于管理的部分，并在过程中总结和分析”。这样的提示旨在鼓励LLM以一种有组织的方式处理信息，而不是一次性试图理解整个复杂的问题。
2. 模板的运用
    - 该方法引入了一个模板，用来整合上下文X（即需要解析的混乱信息）、查询Q（即用户提出的具体问题），以及触发句子T（即开始解析过程的指令）。模板的格式是[X] Q: [Q] [T] A:，其中A代表LLM将要生成的回答。
    - 例如，如果上下文X是一个关于某个主题的复杂文档，查询Q是用户对该文档的一个具体问题，那么触发句子T会告诉LLM如何处理这些信息，即逐步解析上下文并进行总结和分析。
3. 模拟人类认知策略
    - 整个过程模仿了人类面对复杂信息时的认知方式，即把信息分解成易于消化的小块，提炼出关键点，并持续专注于材料的理解。通过这种方式，即使是复杂的或者混乱的信息也可以被有效地解析和理解。
4. 递增的方法
    - 使用上述方法可以促进更加结构化和连贯的推理流程。它不是试图一下子解决所有问题，而是采取渐进的方式来构建对上下文的理解。这在处理那些充满不确定性和复杂性的上下文时尤为有用，因为它可以帮助保持焦点，避免因信息过载而导致的误解或困惑。
5. 最终目标
    - 最终，这种方法的目的是让LLM能够更高效地辅助用户理解和解决问题，同时确保每个步骤都是清晰且有条理的，从而实现对整个上下文的深刻理解。

#### （P as “[X] Q: [Q] [T]A:” ）
1. 关键元素解读
    - 混乱上下文 𝑋：这是模型需要处理的大量输入信息，可能包含相关和无关的内容，例如不同来源的文本、段落或数据。此类上下文通常呈现信息超载和逻辑混乱的特点。
    - 查询问题 𝑄：用户提出的具体问题。例如，“Where was Reclam founded?”（Reclam 是在哪里成立的？）。
    - 触发句 𝑇：指引模型进入推理状态的核心指令。这句短语明确提示模型需要采取某种策略，例如分步分析。

2. 模板解析
    模板通过以下公式化的方式组织信息：

    - 𝑋：混乱上下文，提供了所有背景信息。
    - Q：明确问题，表明用户的需求。
    - T：触发推理的指令，引导模型按特定方式处理信息。
    - A:：指示模型开始生成答案。

3. 示例：
    - [X]: Retrieved Passage 
        - 1 is: to introduce book vending machines, ... Retrieved Passage 
        - 2 is: of the Carlsbad Decrees, it quickly ...
    - [Q]: Where was Reclam founded? 
    - [T]: Walk me through this context in manageable parts step by step, summarizing and analyzing as we go. 
    - [A]: 

#### 例子
- X: (上下文)
    - 全球变暖是一个紧迫的环境问题，它导致了全球气温升高、极端天气事件增加以及海平面上升等后果。特别地，北极和南极地区的冰盖正在经历显著的变化，这些变化不仅影响当地的生态系统，还对全球气候模式产生深远的影响。科学研究表明，极地冰盖的融化速度在过去几十年中明显加快，这主要归因于人类活动引发的温室气体排放。
- Q：(查询)
    - 该报告中关于全球变暖对极地冰盖影响的关键发现是什么？
- T：(触发句子)
    - 请逐步带领我解析这个上下文，将其分为易于管理的部分，并在过程中总结和分析。

### 第二步：精炼结论
    这一阶段的目标是在第一步的基础上进一步提炼信息，确保最终的答案既简洁又准确。

1. 模板构造：
    - 采用模板“[P] [Z] [A]”，其中：
        - [P] 是原始的提示文本，包含了背景信息和问题。
        - [Z] 是模型根据[P]生成的详细回答。
        - [A] 是一个结论标记，通常是一个触发句子，如“因此，答案是：”，它引导模型总结并给出最终答案。

2. 作用机制：
    - 延续思考过程：通过[A]中的触发句子，模型被鼓励继续其思考过程，筛选已经生成的内容，从中识别出最核心的信息。
    - 提高精度：这种方法有助于聚焦于最关键的信息点，使最终的答案更加精准和明确。

3. 优点：
    - 解决限制：相比之前的单一提示方法，这种两层提示系统能够更好地处理复杂和混乱的信息，而不需要对模型进行复杂的调整或重新训练。
    - 对齐认知模式：该方法不仅提高了模型处理复杂信息的能力，还使得其推理过程更加符合人类的认知习惯，从而更好地满足用户需求。

4. 例子（接着上面的例子）
    - P：(提示词)
        - [全球变暖是一个紧迫的环境问题，它导致了全球气温升高、极端天气事件增加以及海平面上升等后果。特别地，北极和南极地区的冰盖正在经历显著的变化，这些变化不仅影响当地的生态系统，还对全球气候模式产生深远的影响。科学研究表明，极地冰盖的融化速度在过去几十年中明显加快，这主要归因于人类活动引发的温室气体排放。] 
        - Q: 该报告中关于全球变暖对极地冰盖影响的关键发现是什么？ 
        - 请逐步带领我解析这个上下文，将其分为易于管理的部分，并在过程中总结和分析。 A:
    - Z：(模型生成的回答)
        
        根据提供的上下文，全球变暖对极地冰盖的主要影响包括：
        1. 极地冰盖的融化速度在过去几十年中显著加快。
        2. 这种加速的融化主要是由人类活动引起的温室气体排放造成的。
        3. 冰盖的变化不仅影响当地生态系统，也对全球气候模式产生了深远的影响。
        4. 随着冰盖融化，海平面上升的风险也在增加，这对沿海地区构成了威胁。
        5. 研究还指出，冰盖下的海洋温度上升也是加速融化的一个因素。

        通过这些信息，我们可以看到全球变暖对极地冰盖造成了多方面的影响，从生态系统的改变到全球气候变化，再到海平面上升的风险。
    - A：(回答标记)
        - **因此，答案是：** 全球变暖导致极地冰盖融化速度加快，这一现象主要由人类活动引起的温室气体排放造成，进而影响当地生态系统并改变全球气候模式，同时增加了海平面上升的风险。


## CoT VS Thot

![sample](/imgs/thot.avif)


