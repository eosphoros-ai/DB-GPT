# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 0.3.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-28 15:05+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/getting_started.md:1 da78e5535cec40a5af378654db5bdd95
msgid "Quickstart Guide"
msgstr "使用指南"

#: ../../getting_started/getting_started.md:3 7e54f808055c44578a2364e9a6c95f69
msgid "Welcome to DB-GPT!"
msgstr "欢迎来到DB-GPT!"

#: ../../getting_started/getting_started.md:5 3d46f319cc404984a9f3fbf36e3e14f7
msgid ""
"DB-GPT is an experimental open-source project that uses localized GPT "
"large models to interact with your data and environment. With this "
"solution, you can be assured that there is no risk of data leakage, and "
"your data is 100% private and secure."
msgstr ""
"随着大模型的发布迭代，大模型变得越来越智能，在使用大模型的过程当中，遇到极大的数据安全与隐私挑战。在利用大模型能力的过程中我们的私密数据跟环境需要掌握自己的手里，完全可控，避免任何的数据隐私泄露以及安全风险。基于此"
"，我们发起了DB-GPT项目，为所有以数据库为基础的场景，构建一套完整的私有大模型解决方案。 "
"此方案因为支持本地部署，所以不仅仅可以应用于独立私有环境，而且还可以根据业务模块独立部署隔离，让大模型的能力绝对私有、安全、可控。"

#: ../../getting_started/getting_started.md:9 777caeb99fb9458f89b7699f9b790dbf
msgid ""
"Our vision is to make it easier and more convenient to build applications"
" around databases and llm."
msgstr "我们的愿景是让围绕数据库构建大模型应用更简单，更方便。"

#: ../../getting_started/getting_started.md:11 02310b9fc7934df093a2b7f53d82b809
msgid "What can I do with DB-GPT?"
msgstr "通过DB-GPT我能做什么"

#: ../../getting_started/getting_started.md:13 ec30099749e94de085ac9d45edc35019
msgid "Chat Data with your Datasource."
msgstr "和自己的数据聊天，进行数据分析"

#: ../../getting_started/getting_started.md:14 cf5e7961b73447abb788388d843889d7
msgid "Private domain knowledge question answering."
msgstr "私有领域的知识问答"

#: ../../getting_started/getting_started.md:15 60b5ab4b672e4d0589c765843243c66f
msgid "Quickly provide private LLM Model deployment."
msgstr "快速构建私有大模型部署"

#: ../../getting_started/getting_started.md:17 a5f7f27e3b904eb3b8ad3d457d683c10
msgid "Usage with DB-GPT."
msgstr "DB-GPT使用姿势."

#: ../../getting_started/getting_started.md:19 5d1914c6fd6d43a59b83412383706fb5
msgid ""
"Follow DB-GPT application [install tutorial](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html)."
msgstr ""
"先安装部署应用[安装教程](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html)."

#: ../../getting_started/getting_started.md:21 9d9ed9a6cdbc4dbdbb2d4009b446c392
#, fuzzy
msgid ""
"Follow DB-GPT [application usage](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/application/chatdb/chatdb.html)."
msgstr ""
"先安装部署应用[安装教程](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html)."

#: ../../getting_started/getting_started.md:24 319b71993fcc499a89f477b499a21a84
msgid ""
"If you encounter any issues while using DB-GPT (whether it's during "
"installation or usage), you can refer to the [FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/deploy/deploy_faq.html) "
"section for assistance."
msgstr ""
"如果你在使用DB-GPT过程中遇到什么问题(无论是安装还是使用)，可以查看[FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/deploy/deploy_faq.html)\""

#: ../../getting_started/getting_started.md:27 7cb0e2f7384c473e9626e59052c114e1
msgid "🗺️ Ecosystem"
msgstr ""

#: ../../getting_started/getting_started.md:29 6067623edfda45c5a7fdfa80ffd886f4
msgid "Github: https://github.com/eosphoros-ai/DB-GPT"
msgstr "Github: https://github.com/eosphoros-ai/DB-GPT"

#: ../../getting_started/getting_started.md:30 2eb9498deaac4a928e1df453d71fcddf
msgid "PyPi:"
msgstr "PyPi:"

#: ../../getting_started/getting_started.md:31 f321d5c1e4bd4556959cd947a23055b9
msgid "DB-GPT: https://pypi.org/simple/db-gpt."
msgstr "DB-GPT: https://pypi.org/simple/db-gpt."

#: ../../getting_started/getting_started.md:33 b31a2fa10f0d4c28b8068b44654dfbf9
msgid "Associated projects"
msgstr "关联的项目"

#: ../../getting_started/getting_started.md:35 c8b6d0da11fc471a9fefcef8f85500e1
msgid ""
"🧪 DB-GPT-Hub: https://github.com/eosphoros-ai/DB-GPT-Hub | an "
"experimental project to implement Text-to-SQL parsing using LLMs"
msgstr ""
"🧪 DB-GPT-Hub: https://github.com/eosphoros-ai/DB-GPT-Hub | 基于开源大模型的Text-"
"to-SQL实验性项目"

#: ../../getting_started/getting_started.md:37 a76faff98a9a49d893a3e15f920c054a
msgid ""
"🏡 DB-GPT-Web: https://github.com/eosphoros-ai/DB-GPT-Web | Web "
"application for DB-GPT."
msgstr ""
"🏡 DB-GPT-Web: https://github.com/eosphoros-ai/DB-GPT-Web | Web 应用 for DB-"
"GPT."

#: ../../getting_started/getting_started.md:38 be54eeb941564d799cd68a6cfe1bfffc
msgid ""
"🚀 DB-GPT-Plugins: https://github.com/eosphoros-ai/DB-GPT-Web | DB-GPT "
"Plugins Repo, Which support AutoGPT plugin."
msgstr ""
"🚀 DB-GPT-Plugins: https://github.com/eosphoros-ai/DB-GPT-Web | DB-GPT "
"Plugins Repo, Which support AutoGPT plugin."

#~ msgid "4.2. Run with docker compose"
#~ msgstr "4.2. Run with docker compose"

#~ msgid ""
#~ "1.This project relies on a local "
#~ "MySQL database service, which you need"
#~ " to install locally. We recommend "
#~ "using Docker for installation."
#~ msgstr "本项目依赖一个本地的 MySQL 数据库服务，你需要本地安装，推荐直接使用 Docker 安装。"

#~ msgid "prepare server sql script"
#~ msgstr "准备db-gpt server sql脚本"

#~ msgid ""
#~ "This tutorial gives you a quick "
#~ "walkthrough about use DB-GPT with "
#~ "you environment and data."
#~ msgstr "本教程为您提供了关于如何使用DB-GPT的使用指南。"

#~ msgid "Installation"
#~ msgstr "安装"

#~ msgid "To get started, install DB-GPT with the following steps."
#~ msgstr "请按照以下步骤安装DB-GPT"

#~ msgid "1. Hardware Requirements"
#~ msgstr "1. 硬件要求"

#~ msgid ""
#~ "As our project has the ability to"
#~ " achieve ChatGPT performance of over "
#~ "85%, there are certain hardware "
#~ "requirements. However, overall, the project"
#~ " can be deployed and used on "
#~ "consumer-grade graphics cards. The specific"
#~ " hardware requirements for deployment are"
#~ " as follows:"
#~ msgstr "由于我们的项目有能力达到85%以上的ChatGPT性能，所以对硬件有一定的要求。但总体来说，我们在消费级的显卡上即可完成项目的部署使用，具体部署的硬件说明如下:"

#~ msgid "GPU"
#~ msgstr "GPU"

#~ msgid "VRAM Size"
#~ msgstr "显存大小"

#~ msgid "Performance"
#~ msgstr "显存大小"

#~ msgid "RTX 4090"
#~ msgstr "RTX 4090"

#~ msgid "24 GB"
#~ msgstr "24 GB"

#~ msgid "Smooth conversation inference"
#~ msgstr "可以流畅的进行对话推理，无卡顿"

#~ msgid "RTX 3090"
#~ msgstr "RTX 3090"

#~ msgid "Smooth conversation inference, better than V100"
#~ msgstr "可以流畅进行对话推理，有卡顿感，但好于V100"

#~ msgid "V100"
#~ msgstr "V100"

#~ msgid "16 GB"
#~ msgstr "16 GB"

#~ msgid "Conversation inference possible, noticeable stutter"
#~ msgstr "可以进行对话推理，有明显卡顿"

#~ msgid "2. Install"
#~ msgstr "2. 安装"

#~ msgid ""
#~ "We use [Chroma embedding "
#~ "database](https://github.com/chroma-core/chroma) as "
#~ "the default for our vector database "
#~ "and use SQLite as the default for"
#~ " our database, so there is no "
#~ "need for special installation. If you"
#~ " choose to connect to other "
#~ "databases, you can follow our tutorial"
#~ " for installation and configuration.  For"
#~ " the entire installation process of "
#~ "DB-GPT, we use the miniconda3 virtual"
#~ " environment. Create a virtual environment"
#~ " and install the Python dependencies."
#~ msgstr ""
#~ "向量数据库我们默认使用的是Chroma内存数据库，所以无需特殊安装，如果有需要连接其他的同学，可以按照我们的教程进行安装配置"
#~ "。整个DB-GPT的安装过程，我们使用的是miniconda3的虚拟环境。创建虚拟环境，并安装python依赖包"

#~ msgid "Before use DB-GPT Knowledge Management"
#~ msgstr "使用知识库管理功能之前"

#~ msgid ""
#~ "Once the environment is installed, we"
#~ " have to create a new folder "
#~ "\"models\" in the DB-GPT project, "
#~ "and then we can put all the "
#~ "models downloaded from huggingface in "
#~ "this directory"
#~ msgstr ""
#~ "环境安装完成后，我们必须在DB-"
#~ "GPT项目中创建一个新文件夹\"models\"，然后我们可以把从huggingface下载的所有模型放到这个目录下。"

#~ msgid "Notice make sure you have install git-lfs"
#~ msgstr "确保你已经安装了git-lfs"

#~ msgid ""
#~ "The model files are large and will"
#~ " take a long time to download. "
#~ "During the download, let's configure the"
#~ " .env file, which needs to be "
#~ "copied and created from the "
#~ ".env.template"
#~ msgstr "模型文件很大，需要很长时间才能下载。在下载过程中，让我们配置.env文件，它需要从。env.template中复制和创建。"

#~ msgid "cp .env.template .env"
#~ msgstr "cp .env.template .env"

#~ msgid ""
#~ "You can configure basic parameters in"
#~ " the .env file, for example setting"
#~ " LLM_MODEL to the model to be "
#~ "used"
#~ msgstr "您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。"

#~ msgid ""
#~ "([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) "
#~ "based on llama-2 has been released, "
#~ "we recommend you set `LLM_MODEL=vicuna-"
#~ "13b-v1.5` to try this model)"
#~ msgstr ""

#~ msgid "3. Run"
#~ msgstr "3. 运行"

#~ msgid ""
#~ "You can refer to this document to"
#~ " obtain the Vicuna weights: "
#~ "[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md"
#~ "#model-weights) ."
#~ msgstr ""
#~ "关于基础模型, 可以根据[Vicuna](https://github.com/lm-"
#~ "sys/FastChat/blob/main/README.md#model-weights) 合成教程进行合成。"

#~ msgid ""
#~ "If you have difficulty with this "
#~ "step, you can also directly use "
#~ "the model from [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) as "
#~ "a replacement."
#~ msgstr ""
#~ "如果此步有困难的同学，也可以直接使用[此链接](https://huggingface.co/Tribbiani/vicuna-"
#~ "7b)上的模型进行替代。"

#~ msgid ""
#~ "set .env configuration set your vector"
#~ " store type, eg:VECTOR_STORE_TYPE=Chroma, now "
#~ "we support Chroma and Milvus(version >"
#~ " 2.1)"
#~ msgstr ""
#~ "在.env文件设置向量数据库环境变量，eg:VECTOR_STORE_TYPE=Chroma, 目前我们支持了 "
#~ "Chroma and Milvus(version >2.1) "

#~ msgid "1.Run db-gpt server"
#~ msgstr "运行模型服务"

#~ msgid "Open http://localhost:5000 with your browser to see the product."
#~ msgstr "打开浏览器访问http://localhost:5000"

#~ msgid ""
#~ "If you want to access an external"
#~ " LLM service, you need to 1.set "
#~ "the variables LLM_MODEL=YOUR_MODEL_NAME "
#~ "MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in "
#~ "the .env file. 2.execute dbgpt_server.py "
#~ "in light mode"
#~ msgstr "如果你想访问外部的大模型服务，1.需要在.env文件设置模型名和外部模型服务地址。2.使用light模式启动服务"

#~ msgid ""
#~ "If you want to learn about "
#~ "dbgpt-webui, read https://github./csunny/DB-"
#~ "GPT/tree/new-page-framework/datacenter"
#~ msgstr ""
#~ "如果你想了解DB-GPT前端服务，访问https://github.com/csunny/DB-GPT/tree"
#~ "/new-page-framework/datacenter"

#~ msgid "4. Docker (Experimental)"
#~ msgstr "4. Docker (Experimental)"

#~ msgid "4.1 Building Docker image"
#~ msgstr "4.1 Building Docker image"

#~ msgid "Review images by listing them:"
#~ msgstr "Review images by listing them:"

#~ msgid "Output should look something like the following:"
#~ msgstr "Output should look something like the following:"

#~ msgid ""
#~ "`eosphorosai/dbgpt` is the base image, "
#~ "which contains the project's base "
#~ "dependencies and a sqlite database. "
#~ "`eosphorosai/dbgpt-allinone` build from "
#~ "`eosphorosai/dbgpt`, which contains a mysql"
#~ " database."
#~ msgstr ""

#~ msgid "You can pass some parameters to docker/build_all_images.sh."
#~ msgstr "You can pass some parameters to docker/build_all_images.sh."

#~ msgid ""
#~ "You can execute the command `bash "
#~ "docker/build_all_images.sh --help` to see more"
#~ " usage."
#~ msgstr ""
#~ "You can execute the command `bash "
#~ "docker/build_all_images.sh --help` to see more"
#~ " usage."

#~ msgid "4.2. Run all in one docker container"
#~ msgstr "4.2. Run all in one docker container"

#~ msgid "**Run with local model and SQLite database**"
#~ msgstr "**Run with local model**"

#~ msgid ""
#~ "`-e LLM_MODEL=vicuna-13b`, means we use"
#~ " vicuna-13b as llm model, see "
#~ "/pilot/configs/model_config.LLM_MODEL_CONFIG"
#~ msgstr ""
#~ "`-e LLM_MODEL=vicuna-13b`, means we use"
#~ " vicuna-13b as llm model, see "
#~ "/pilot/configs/model_config.LLM_MODEL_CONFIG"

#~ msgid ""
#~ "`-v /data/models:/app/models`, means we mount"
#~ " the local model file directory "
#~ "`/data/models` to the docker container "
#~ "directory `/app/models`, please replace it "
#~ "with your model file directory."
#~ msgstr ""
#~ "`-v /data/models:/app/models`, means we mount"
#~ " the local model file directory "
#~ "`/data/models` to the docker container "
#~ "directory `/app/models`, please replace it "
#~ "with your model file directory."

#~ msgid "You can see log with command:"
#~ msgstr "You can see log with command:"

#~ msgid "**Run with local model and MySQL database**"
#~ msgstr "**Run with local model**"

#~ msgid "**Run with openai interface**"
#~ msgstr "**Run with openai interface**"

#~ msgid ""
#~ "`-e LLM_MODEL=proxyllm`, means we use "
#~ "proxy llm(openai interface, fastchat "
#~ "interface...)"
#~ msgstr ""
#~ "`-e LLM_MODEL=proxyllm`, means we use "
#~ "proxy llm(openai interface, fastchat "
#~ "interface...)"

#~ msgid ""
#~ "`-v /data/models/text2vec-large-chinese:/app/models"
#~ "/text2vec-large-chinese`, means we mount"
#~ " the local text2vec model to the "
#~ "docker container."
#~ msgstr ""
#~ "`-v /data/models/text2vec-large-chinese:/app/models"
#~ "/text2vec-large-chinese`, means we mount"
#~ " the local text2vec model to the "
#~ "docker container."

#~ msgid "4.3. Run with docker compose"
#~ msgstr ""

#~ msgid ""
#~ "You can open docker-compose.yml in "
#~ "the project root directory to see "
#~ "more details."
#~ msgstr ""
#~ "You can open docker-compose.yml in "
#~ "the project root directory to see "
#~ "more details."

#~ msgid "5. Multiple GPUs"
#~ msgstr "5. Multiple GPUs"

#~ msgid ""
#~ "DB-GPT will use all available gpu"
#~ " by default. And you can modify "
#~ "the setting `CUDA_VISIBLE_DEVICES=0,1` in "
#~ "`.env` file to use the specific "
#~ "gpu IDs."
#~ msgstr ""
#~ "DB-GPT will use all available gpu"
#~ " by default. And you can modify "
#~ "the setting `CUDA_VISIBLE_DEVICES=0,1` in "
#~ "`.env` file to use the specific "
#~ "gpu IDs."

#~ msgid ""
#~ "Optionally, you can also specify the "
#~ "gpu ID to use before the starting"
#~ " command, as shown below:"
#~ msgstr ""
#~ "Optionally, you can also specify the "
#~ "gpu ID to use before the starting"
#~ " command, as shown below:"

#~ msgid ""
#~ "You can modify the setting "
#~ "`MAX_GPU_MEMORY=xxGib` in `.env` file to "
#~ "configure the maximum memory used by "
#~ "each GPU."
#~ msgstr ""

#~ msgid "6. Not Enough Memory"
#~ msgstr ""

#~ msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
#~ msgstr ""

#~ msgid ""
#~ "You can modify the setting "
#~ "`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` in "
#~ "`.env` file to use quantization(8-bit "
#~ "quantization is enabled by default)."
#~ msgstr ""

#~ msgid ""
#~ "Llama-2-70b with 8-bit quantization can "
#~ "run with 80 GB of VRAM, and "
#~ "4-bit quantization can run with 48 "
#~ "GB of VRAM."
#~ msgstr ""
#~ "Llama-2-70b with 8-bit quantization can "
#~ "run with 80 GB of VRAM, and "
#~ "4-bit quantization can run with 48 "
#~ "GB of VRAM."

#~ msgid ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."
#~ msgstr ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."

#~ msgid ""
#~ "Here are some of the VRAM size "
#~ "usage of the models we tested in"
#~ " some common scenarios."
#~ msgstr ""
#~ "Here are some of the VRAM size "
#~ "usage of the models we tested in"
#~ " some common scenarios."

#~ msgid "Model"
#~ msgstr "Model"

#~ msgid "Quantize"
#~ msgstr "Quantize"

#~ msgid "vicuna-7b-v1.5"
#~ msgstr "vicuna-7b-v1.5"

#~ msgid "4-bit"
#~ msgstr "4-bit"

#~ msgid "8 GB"
#~ msgstr "24 GB"

#~ msgid "8-bit"
#~ msgstr "8-bit"

#~ msgid "12 GB"
#~ msgstr "24 GB"

#~ msgid "vicuna-13b-v1.5"
#~ msgstr "vicuna-13b-v1.5"

#~ msgid "20 GB"
#~ msgstr "24 GB"

#~ msgid "llama-2-7b"
#~ msgstr "llama-2-7b"

#~ msgid "llama-2-13b"
#~ msgstr "llama-2-13b"

#~ msgid "llama-2-70b"
#~ msgstr "llama-2-70b"

#~ msgid "48 GB"
#~ msgstr "24 GB"

#~ msgid "80 GB"
#~ msgstr "24 GB"

#~ msgid "baichuan-7b"
#~ msgstr ""

#~ msgid "baichuan-13b"
#~ msgstr "baichuan-13b"

#~ msgid ""
#~ "[应用使用教程](https://db-"
#~ "gpt.readthedocs.io/en/latest/getting_started/application/chatdb/chatdb.html)."
#~ msgstr ""

#~ msgid "🗺️ 生态"
#~ msgstr "🗺️ 生态"
