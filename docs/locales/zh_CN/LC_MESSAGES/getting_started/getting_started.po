# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 0.3.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-23 17:00+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/getting_started.md:1 f5e3f55f96de414183847de3e1eb0a75
msgid "Quickstart Guide"
msgstr "ä½¿ç”¨æŒ‡å—"

#: ../../getting_started/getting_started.md:3 39ac3167d0044868b9d9efca953e73c5
msgid "Welcome to DB-GPT!"
msgstr "æ¬¢è¿æ¥åˆ°DB-GPT!"

#: ../../getting_started/getting_started.md:5 bdc699f3da554c6eb46eba0636fe7bda
msgid ""
"DB-GPT is an experimental open-source project that uses localized GPT "
"large models to interact with your data and environment. With this "
"solution, you can be assured that there is no risk of data leakage, and "
"your data is 100% private and secure."
msgstr ""
"éšç€å¤§æ¨¡å‹çš„å‘å¸ƒè¿­ä»£ï¼Œå¤§æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šæ™ºèƒ½ï¼Œåœ¨ä½¿ç”¨å¤§æ¨¡å‹çš„è¿‡ç¨‹å½“ä¸­ï¼Œé‡åˆ°æå¤§çš„æ•°æ®å®‰å…¨ä¸éšç§æŒ‘æˆ˜ã€‚åœ¨åˆ©ç”¨å¤§æ¨¡å‹èƒ½åŠ›çš„è¿‡ç¨‹ä¸­æˆ‘ä»¬çš„ç§å¯†æ•°æ®è·Ÿç¯å¢ƒéœ€è¦æŒæ¡è‡ªå·±çš„æ‰‹é‡Œï¼Œå®Œå…¨å¯æ§ï¼Œé¿å…ä»»ä½•çš„æ•°æ®éšç§æ³„éœ²ä»¥åŠå®‰å…¨é£é™©ã€‚åŸºäºæ­¤"
"ï¼Œæˆ‘ä»¬å‘èµ·äº†DB-GPTé¡¹ç›®ï¼Œä¸ºæ‰€æœ‰ä»¥æ•°æ®åº“ä¸ºåŸºç¡€çš„åœºæ™¯ï¼Œæ„å»ºä¸€å¥—å®Œæ•´çš„ç§æœ‰å¤§æ¨¡å‹è§£å†³æ–¹æ¡ˆã€‚ "
"æ­¤æ–¹æ¡ˆå› ä¸ºæ”¯æŒæœ¬åœ°éƒ¨ç½²ï¼Œæ‰€ä»¥ä¸ä»…ä»…å¯ä»¥åº”ç”¨äºç‹¬ç«‹ç§æœ‰ç¯å¢ƒï¼Œè€Œä¸”è¿˜å¯ä»¥æ ¹æ®ä¸šåŠ¡æ¨¡å—ç‹¬ç«‹éƒ¨ç½²éš”ç¦»ï¼Œè®©å¤§æ¨¡å‹çš„èƒ½åŠ›ç»å¯¹ç§æœ‰ã€å®‰å…¨ã€å¯æ§ã€‚"

#: ../../getting_started/getting_started.md:7 7d1a79640331431e85d5c54075dca1fd
msgid ""
"Our vision is to make it easier and more convenient to build applications"
" around databases and llm."
msgstr "æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯è®©å›´ç»•æ•°æ®åº“æ„å»ºå¤§æ¨¡å‹åº”ç”¨æ›´ç®€å•ï¼Œæ›´æ–¹ä¾¿ã€‚"

#: ../../getting_started/getting_started.md:9 f400d65e13e0405d8aea576c02a7b02a
msgid "What can I do with DB-GPT?"
msgstr "é€šè¿‡DB-GPTæˆ‘èƒ½åšä»€ä¹ˆ"

#: ../../getting_started/getting_started.md:10 2b161262a21b474ca7c191ed0f9ab6b0
msgid "Chat Data with your Datasource."
msgstr "å’Œè‡ªå·±çš„æ•°æ®èŠå¤©ï¼Œè¿›è¡Œæ•°æ®åˆ†æ"

#: ../../getting_started/getting_started.md:11 0f1322f19cea45768b5bde4448dfea5e
msgid "Private domain knowledge question answering."
msgstr "ç§æœ‰é¢†åŸŸçš„çŸ¥è¯†é—®ç­”"

#: ../../getting_started/getting_started.md:12 9dc0e76f46c64304aabb80fcf4a2a3f2
msgid "Quickly provide private LLM Model deployment."
msgstr "å¿«é€Ÿæ„å»ºç§æœ‰å¤§æ¨¡å‹éƒ¨ç½²"

#: ../../getting_started/getting_started.md:14 ad5daeae9621455ba447735fd55648cc
msgid "Usage with DB-GPT."
msgstr "DB-GPTä½¿ç”¨å§¿åŠ¿."

#: ../../getting_started/getting_started.md:15 870d82fd402444aab3028764607ff468
msgid ""
"Follow DB-GPT application [install tutorial](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html)."
msgstr ""
"å…ˆå®‰è£…éƒ¨ç½²åº”ç”¨[å®‰è£…æ•™ç¨‹](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html)."

#: ../../getting_started/getting_started.md:16 c7806bcd1b9a43eda8a86402f9903dfe
#, fuzzy
msgid ""
"Follow DB-GPT [application usage](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/application/chatdb/chatdb.html)."
msgstr ""
"å…ˆå®‰è£…éƒ¨ç½²åº”ç”¨[å®‰è£…æ•™ç¨‹](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html)."

#: ../../getting_started/getting_started.md:18 5dd30b81ed39476f871ed31edd01c043
msgid ""
"If you encounter any issues while using DB-GPT (whether it's during "
"installation or usage), you can refer to the [FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/deploy/deploy_faq.html) "
"section for assistance."
msgstr ""
"å¦‚æœä½ åœ¨ä½¿ç”¨DB-GPTè¿‡ç¨‹ä¸­é‡åˆ°ä»€ä¹ˆé—®é¢˜(æ— è®ºæ˜¯å®‰è£…è¿˜æ˜¯ä½¿ç”¨)ï¼Œå¯ä»¥æŸ¥çœ‹[FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/deploy/deploy_faq.html)\""

#: ../../getting_started/getting_started.md:21 c6202be38f3641098eedca1ef31665b3
msgid "ğŸ—ºï¸ ç”Ÿæ€"
msgstr "ğŸ—ºï¸ ç”Ÿæ€"

#: ../../getting_started/getting_started.md:23 d55c843f8a044e099e61a86ef5a70eaf
msgid "Github: https://github.com/eosphoros-ai/DB-GPT"
msgstr "Github: https://github.com/eosphoros-ai/DB-GPT"

#: ../../getting_started/getting_started.md:24 7e279ce6cce346ecb3e58537f2d8ca24
msgid "PyPi:"
msgstr "PyPi:"

#: ../../getting_started/getting_started.md:25 ca92a6bc95c94996b4e672be7ee43f1b
msgid "DB-GPT: https://pypi.org/simple/db-gpt."
msgstr "DB-GPT: https://pypi.org/simple/db-gpt."

#: ../../getting_started/getting_started.md:27 cc44973d1a78434fa3dfe511618b08d9
msgid "Associated projects"
msgstr "å…³è”çš„é¡¹ç›®"

#: ../../getting_started/getting_started.md:30 93b80349645943b38d16afd80ee076da
msgid ""
"ğŸ§ª DB-GPT-Hub: https://github.com/eosphoros-ai/DB-GPT-Hub | an "
"experimental project to implement Text-to-SQL parsing using LLMs"
msgstr "ğŸ§ª DB-GPT-Hub: https://github.com/eosphoros-ai/DB-GPT-Hub | åŸºäºå¼€æºå¤§æ¨¡å‹çš„Text-to-SQLå®éªŒæ€§é¡¹ç›®"

#: ../../getting_started/getting_started.md:31 2c061d9eee624a4b994bb6af3361591b
msgid ""
"ğŸ¡ DB-GPT-Web: https://github.com/eosphoros-ai/DB-GPT-Web | Web "
"application for DB-GPT."
msgstr "ğŸ¡ DB-GPT-Web: https://github.com/eosphoros-ai/DB-GPT-Web | Web "
"åº”ç”¨ for DB-GPT."

#: ../../getting_started/getting_started.md:32 6564f208ecd04a3fae6dd5f12cfbad12
msgid ""
"ğŸš€ DB-GPT-Plugins: https://github.com/eosphoros-ai/DB-GPT-Web | DB-GPT "
"Plugins Repo, Which support AutoGPT plugin."
msgstr "ğŸš€ DB-GPT-Plugins: https://github.com/eosphoros-ai/DB-GPT-Web | DB-GPT "
"Plugins Repo, Which support AutoGPT plugin."

#~ msgid "4.2. Run with docker compose"
#~ msgstr "4.2. Run with docker compose"

#~ msgid ""
#~ "1.This project relies on a local "
#~ "MySQL database service, which you need"
#~ " to install locally. We recommend "
#~ "using Docker for installation."
#~ msgstr "æœ¬é¡¹ç›®ä¾èµ–ä¸€ä¸ªæœ¬åœ°çš„ MySQL æ•°æ®åº“æœåŠ¡ï¼Œä½ éœ€è¦æœ¬åœ°å®‰è£…ï¼Œæ¨èç›´æ¥ä½¿ç”¨ Docker å®‰è£…ã€‚"

#~ msgid "prepare server sql script"
#~ msgstr "å‡†å¤‡db-gpt server sqlè„šæœ¬"

#~ msgid ""
#~ "This tutorial gives you a quick "
#~ "walkthrough about use DB-GPT with "
#~ "you environment and data."
#~ msgstr "æœ¬æ•™ç¨‹ä¸ºæ‚¨æä¾›äº†å…³äºå¦‚ä½•ä½¿ç”¨DB-GPTçš„ä½¿ç”¨æŒ‡å—ã€‚"

#~ msgid "Installation"
#~ msgstr "å®‰è£…"

#~ msgid "To get started, install DB-GPT with the following steps."
#~ msgstr "è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®‰è£…DB-GPT"

#~ msgid "1. Hardware Requirements"
#~ msgstr "1. ç¡¬ä»¶è¦æ±‚"

#~ msgid ""
#~ "As our project has the ability to"
#~ " achieve ChatGPT performance of over "
#~ "85%, there are certain hardware "
#~ "requirements. However, overall, the project"
#~ " can be deployed and used on "
#~ "consumer-grade graphics cards. The specific"
#~ " hardware requirements for deployment are"
#~ " as follows:"
#~ msgstr "ç”±äºæˆ‘ä»¬çš„é¡¹ç›®æœ‰èƒ½åŠ›è¾¾åˆ°85%ä»¥ä¸Šçš„ChatGPTæ€§èƒ½ï¼Œæ‰€ä»¥å¯¹ç¡¬ä»¶æœ‰ä¸€å®šçš„è¦æ±‚ã€‚ä½†æ€»ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šå³å¯å®Œæˆé¡¹ç›®çš„éƒ¨ç½²ä½¿ç”¨ï¼Œå…·ä½“éƒ¨ç½²çš„ç¡¬ä»¶è¯´æ˜å¦‚ä¸‹:"

#~ msgid "GPU"
#~ msgstr "GPU"

#~ msgid "VRAM Size"
#~ msgstr "æ˜¾å­˜å¤§å°"

#~ msgid "Performance"
#~ msgstr "æ˜¾å­˜å¤§å°"

#~ msgid "RTX 4090"
#~ msgstr "RTX 4090"

#~ msgid "24 GB"
#~ msgstr "24 GB"

#~ msgid "Smooth conversation inference"
#~ msgstr "å¯ä»¥æµç•…çš„è¿›è¡Œå¯¹è¯æ¨ç†ï¼Œæ— å¡é¡¿"

#~ msgid "RTX 3090"
#~ msgstr "RTX 3090"

#~ msgid "Smooth conversation inference, better than V100"
#~ msgstr "å¯ä»¥æµç•…è¿›è¡Œå¯¹è¯æ¨ç†ï¼Œæœ‰å¡é¡¿æ„Ÿï¼Œä½†å¥½äºV100"

#~ msgid "V100"
#~ msgstr "V100"

#~ msgid "16 GB"
#~ msgstr "16 GB"

#~ msgid "Conversation inference possible, noticeable stutter"
#~ msgstr "å¯ä»¥è¿›è¡Œå¯¹è¯æ¨ç†ï¼Œæœ‰æ˜æ˜¾å¡é¡¿"

#~ msgid "2. Install"
#~ msgstr "2. å®‰è£…"

#~ msgid ""
#~ "We use [Chroma embedding "
#~ "database](https://github.com/chroma-core/chroma) as "
#~ "the default for our vector database "
#~ "and use SQLite as the default for"
#~ " our database, so there is no "
#~ "need for special installation. If you"
#~ " choose to connect to other "
#~ "databases, you can follow our tutorial"
#~ " for installation and configuration.  For"
#~ " the entire installation process of "
#~ "DB-GPT, we use the miniconda3 virtual"
#~ " environment. Create a virtual environment"
#~ " and install the Python dependencies."
#~ msgstr ""
#~ "å‘é‡æ•°æ®åº“æˆ‘ä»¬é»˜è®¤ä½¿ç”¨çš„æ˜¯Chromaå†…å­˜æ•°æ®åº“ï¼Œæ‰€ä»¥æ— éœ€ç‰¹æ®Šå®‰è£…ï¼Œå¦‚æœæœ‰éœ€è¦è¿æ¥å…¶ä»–çš„åŒå­¦ï¼Œå¯ä»¥æŒ‰ç…§æˆ‘ä»¬çš„æ•™ç¨‹è¿›è¡Œå®‰è£…é…ç½®"
#~ "ã€‚æ•´ä¸ªDB-GPTçš„å®‰è£…è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯miniconda3çš„è™šæ‹Ÿç¯å¢ƒã€‚åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶å®‰è£…pythonä¾èµ–åŒ…"

#~ msgid "Before use DB-GPT Knowledge Management"
#~ msgstr "ä½¿ç”¨çŸ¥è¯†åº“ç®¡ç†åŠŸèƒ½ä¹‹å‰"

#~ msgid ""
#~ "Once the environment is installed, we"
#~ " have to create a new folder "
#~ "\"models\" in the DB-GPT project, "
#~ "and then we can put all the "
#~ "models downloaded from huggingface in "
#~ "this directory"
#~ msgstr ""
#~ "ç¯å¢ƒå®‰è£…å®Œæˆåï¼Œæˆ‘ä»¬å¿…é¡»åœ¨DB-"
#~ "GPTé¡¹ç›®ä¸­åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹\"models\"ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥æŠŠä»huggingfaceä¸‹è½½çš„æ‰€æœ‰æ¨¡å‹æ”¾åˆ°è¿™ä¸ªç›®å½•ä¸‹ã€‚"

#~ msgid "Notice make sure you have install git-lfs"
#~ msgstr "ç¡®ä¿ä½ å·²ç»å®‰è£…äº†git-lfs"

#~ msgid ""
#~ "The model files are large and will"
#~ " take a long time to download. "
#~ "During the download, let's configure the"
#~ " .env file, which needs to be "
#~ "copied and created from the "
#~ ".env.template"
#~ msgstr "æ¨¡å‹æ–‡ä»¶å¾ˆå¤§ï¼Œéœ€è¦å¾ˆé•¿æ—¶é—´æ‰èƒ½ä¸‹è½½ã€‚åœ¨ä¸‹è½½è¿‡ç¨‹ä¸­ï¼Œè®©æˆ‘ä»¬é…ç½®.envæ–‡ä»¶ï¼Œå®ƒéœ€è¦ä»ã€‚env.templateä¸­å¤åˆ¶å’Œåˆ›å»ºã€‚"

#~ msgid "cp .env.template .env"
#~ msgstr "cp .env.template .env"

#~ msgid ""
#~ "You can configure basic parameters in"
#~ " the .env file, for example setting"
#~ " LLM_MODEL to the model to be "
#~ "used"
#~ msgstr "æ‚¨å¯ä»¥åœ¨.envæ–‡ä»¶ä¸­é…ç½®åŸºæœ¬å‚æ•°ï¼Œä¾‹å¦‚å°†LLM_MODELè®¾ç½®ä¸ºè¦ä½¿ç”¨çš„æ¨¡å‹ã€‚"

#~ msgid ""
#~ "([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) "
#~ "based on llama-2 has been released, "
#~ "we recommend you set `LLM_MODEL=vicuna-"
#~ "13b-v1.5` to try this model)"
#~ msgstr ""

#~ msgid "3. Run"
#~ msgstr "3. è¿è¡Œ"

#~ msgid ""
#~ "You can refer to this document to"
#~ " obtain the Vicuna weights: "
#~ "[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md"
#~ "#model-weights) ."
#~ msgstr ""
#~ "å…³äºåŸºç¡€æ¨¡å‹, å¯ä»¥æ ¹æ®[Vicuna](https://github.com/lm-"
#~ "sys/FastChat/blob/main/README.md#model-weights) åˆæˆæ•™ç¨‹è¿›è¡Œåˆæˆã€‚"

#~ msgid ""
#~ "If you have difficulty with this "
#~ "step, you can also directly use "
#~ "the model from [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) as "
#~ "a replacement."
#~ msgstr ""
#~ "å¦‚æœæ­¤æ­¥æœ‰å›°éš¾çš„åŒå­¦ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨[æ­¤é“¾æ¥](https://huggingface.co/Tribbiani/vicuna-"
#~ "7b)ä¸Šçš„æ¨¡å‹è¿›è¡Œæ›¿ä»£ã€‚"

#~ msgid ""
#~ "set .env configuration set your vector"
#~ " store type, eg:VECTOR_STORE_TYPE=Chroma, now "
#~ "we support Chroma and Milvus(version >"
#~ " 2.1)"
#~ msgstr ""
#~ "åœ¨.envæ–‡ä»¶è®¾ç½®å‘é‡æ•°æ®åº“ç¯å¢ƒå˜é‡ï¼Œeg:VECTOR_STORE_TYPE=Chroma, ç›®å‰æˆ‘ä»¬æ”¯æŒäº† "
#~ "Chroma and Milvus(version >2.1) "

#~ msgid "1.Run db-gpt server"
#~ msgstr "è¿è¡Œæ¨¡å‹æœåŠ¡"

#~ msgid "Open http://localhost:5000 with your browser to see the product."
#~ msgstr "æ‰“å¼€æµè§ˆå™¨è®¿é—®http://localhost:5000"

#~ msgid ""
#~ "If you want to access an external"
#~ " LLM service, you need to 1.set "
#~ "the variables LLM_MODEL=YOUR_MODEL_NAME "
#~ "MODEL_SERVER=YOUR_MODEL_SERVERï¼ˆeg:http://localhost:5000ï¼‰ in "
#~ "the .env file. 2.execute dbgpt_server.py "
#~ "in light mode"
#~ msgstr "å¦‚æœä½ æƒ³è®¿é—®å¤–éƒ¨çš„å¤§æ¨¡å‹æœåŠ¡ï¼Œ1.éœ€è¦åœ¨.envæ–‡ä»¶è®¾ç½®æ¨¡å‹åå’Œå¤–éƒ¨æ¨¡å‹æœåŠ¡åœ°å€ã€‚2.ä½¿ç”¨lightæ¨¡å¼å¯åŠ¨æœåŠ¡"

#~ msgid ""
#~ "If you want to learn about "
#~ "dbgpt-webui, read https://github./csunny/DB-"
#~ "GPT/tree/new-page-framework/datacenter"
#~ msgstr ""
#~ "å¦‚æœä½ æƒ³äº†è§£DB-GPTå‰ç«¯æœåŠ¡ï¼Œè®¿é—®https://github.com/csunny/DB-GPT/tree"
#~ "/new-page-framework/datacenter"

#~ msgid "4. Docker (Experimental)"
#~ msgstr "4. Docker (Experimental)"

#~ msgid "4.1 Building Docker image"
#~ msgstr "4.1 Building Docker image"

#~ msgid "Review images by listing them:"
#~ msgstr "Review images by listing them:"

#~ msgid "Output should look something like the following:"
#~ msgstr "Output should look something like the following:"

#~ msgid ""
#~ "`eosphorosai/dbgpt` is the base image, "
#~ "which contains the project's base "
#~ "dependencies and a sqlite database. "
#~ "`eosphorosai/dbgpt-allinone` build from "
#~ "`eosphorosai/dbgpt`, which contains a mysql"
#~ " database."
#~ msgstr ""

#~ msgid "You can pass some parameters to docker/build_all_images.sh."
#~ msgstr "You can pass some parameters to docker/build_all_images.sh."

#~ msgid ""
#~ "You can execute the command `bash "
#~ "docker/build_all_images.sh --help` to see more"
#~ " usage."
#~ msgstr ""
#~ "You can execute the command `bash "
#~ "docker/build_all_images.sh --help` to see more"
#~ " usage."

#~ msgid "4.2. Run all in one docker container"
#~ msgstr "4.2. Run all in one docker container"

#~ msgid "**Run with local model and SQLite database**"
#~ msgstr "**Run with local model**"

#~ msgid ""
#~ "`-e LLM_MODEL=vicuna-13b`, means we use"
#~ " vicuna-13b as llm model, see "
#~ "/pilot/configs/model_config.LLM_MODEL_CONFIG"
#~ msgstr ""
#~ "`-e LLM_MODEL=vicuna-13b`, means we use"
#~ " vicuna-13b as llm model, see "
#~ "/pilot/configs/model_config.LLM_MODEL_CONFIG"

#~ msgid ""
#~ "`-v /data/models:/app/models`, means we mount"
#~ " the local model file directory "
#~ "`/data/models` to the docker container "
#~ "directory `/app/models`, please replace it "
#~ "with your model file directory."
#~ msgstr ""
#~ "`-v /data/models:/app/models`, means we mount"
#~ " the local model file directory "
#~ "`/data/models` to the docker container "
#~ "directory `/app/models`, please replace it "
#~ "with your model file directory."

#~ msgid "You can see log with command:"
#~ msgstr "You can see log with command:"

#~ msgid "**Run with local model and MySQL database**"
#~ msgstr "**Run with local model**"

#~ msgid "**Run with openai interface**"
#~ msgstr "**Run with openai interface**"

#~ msgid ""
#~ "`-e LLM_MODEL=proxyllm`, means we use "
#~ "proxy llm(openai interface, fastchat "
#~ "interface...)"
#~ msgstr ""
#~ "`-e LLM_MODEL=proxyllm`, means we use "
#~ "proxy llm(openai interface, fastchat "
#~ "interface...)"

#~ msgid ""
#~ "`-v /data/models/text2vec-large-chinese:/app/models"
#~ "/text2vec-large-chinese`, means we mount"
#~ " the local text2vec model to the "
#~ "docker container."
#~ msgstr ""
#~ "`-v /data/models/text2vec-large-chinese:/app/models"
#~ "/text2vec-large-chinese`, means we mount"
#~ " the local text2vec model to the "
#~ "docker container."

#~ msgid "4.3. Run with docker compose"
#~ msgstr ""

#~ msgid ""
#~ "You can open docker-compose.yml in "
#~ "the project root directory to see "
#~ "more details."
#~ msgstr ""
#~ "You can open docker-compose.yml in "
#~ "the project root directory to see "
#~ "more details."

#~ msgid "5. Multiple GPUs"
#~ msgstr "5. Multiple GPUs"

#~ msgid ""
#~ "DB-GPT will use all available gpu"
#~ " by default. And you can modify "
#~ "the setting `CUDA_VISIBLE_DEVICES=0,1` in "
#~ "`.env` file to use the specific "
#~ "gpu IDs."
#~ msgstr ""
#~ "DB-GPT will use all available gpu"
#~ " by default. And you can modify "
#~ "the setting `CUDA_VISIBLE_DEVICES=0,1` in "
#~ "`.env` file to use the specific "
#~ "gpu IDs."

#~ msgid ""
#~ "Optionally, you can also specify the "
#~ "gpu ID to use before the starting"
#~ " command, as shown below:"
#~ msgstr ""
#~ "Optionally, you can also specify the "
#~ "gpu ID to use before the starting"
#~ " command, as shown below:"

#~ msgid ""
#~ "You can modify the setting "
#~ "`MAX_GPU_MEMORY=xxGib` in `.env` file to "
#~ "configure the maximum memory used by "
#~ "each GPU."
#~ msgstr ""

#~ msgid "6. Not Enough Memory"
#~ msgstr ""

#~ msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
#~ msgstr ""

#~ msgid ""
#~ "You can modify the setting "
#~ "`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` in "
#~ "`.env` file to use quantization(8-bit "
#~ "quantization is enabled by default)."
#~ msgstr ""

#~ msgid ""
#~ "Llama-2-70b with 8-bit quantization can "
#~ "run with 80 GB of VRAM, and "
#~ "4-bit quantization can run with 48 "
#~ "GB of VRAM."
#~ msgstr ""
#~ "Llama-2-70b with 8-bit quantization can "
#~ "run with 80 GB of VRAM, and "
#~ "4-bit quantization can run with 48 "
#~ "GB of VRAM."

#~ msgid ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."
#~ msgstr ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."

#~ msgid ""
#~ "Here are some of the VRAM size "
#~ "usage of the models we tested in"
#~ " some common scenarios."
#~ msgstr ""
#~ "Here are some of the VRAM size "
#~ "usage of the models we tested in"
#~ " some common scenarios."

#~ msgid "Model"
#~ msgstr "Model"

#~ msgid "Quantize"
#~ msgstr "Quantize"

#~ msgid "vicuna-7b-v1.5"
#~ msgstr "vicuna-7b-v1.5"

#~ msgid "4-bit"
#~ msgstr "4-bit"

#~ msgid "8 GB"
#~ msgstr "24 GB"

#~ msgid "8-bit"
#~ msgstr "8-bit"

#~ msgid "12 GB"
#~ msgstr "24 GB"

#~ msgid "vicuna-13b-v1.5"
#~ msgstr "vicuna-13b-v1.5"

#~ msgid "20 GB"
#~ msgstr "24 GB"

#~ msgid "llama-2-7b"
#~ msgstr "llama-2-7b"

#~ msgid "llama-2-13b"
#~ msgstr "llama-2-13b"

#~ msgid "llama-2-70b"
#~ msgstr "llama-2-70b"

#~ msgid "48 GB"
#~ msgstr "24 GB"

#~ msgid "80 GB"
#~ msgstr "24 GB"

#~ msgid "baichuan-7b"
#~ msgstr ""

#~ msgid "baichuan-13b"
#~ msgstr "baichuan-13b"

#~ msgid ""
#~ "[åº”ç”¨ä½¿ç”¨æ•™ç¨‹](https://db-"
#~ "gpt.readthedocs.io/en/latest/getting_started/application/chatdb/chatdb.html)."
#~ msgstr ""

