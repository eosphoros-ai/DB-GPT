# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT ğŸ‘ğŸ‘ 0.3.6\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-17 19:39+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/cluster/cluster.rst:72
msgid "Cluster deployment"
msgstr "é›†ç¾¤éƒ¨ç½²"

#: ../../getting_started/install/cluster/cluster.rst:2
#: bc5bb85c846b4ad19aeeccdd016f3ce8
#, fuzzy
msgid "LLM Deployment"
msgstr "é›†ç¾¤éƒ¨ç½²"

#: ../../getting_started/install/cluster/cluster.rst:3
#: e1cebf0518db423fbc78e39945a423fa
msgid ""
"In the exploration and implementation of AI model applications, it can be"
" challenging to directly integrate with model services. Currently, there "
"is no established standard for deploying large models, and new models and"
" inference methods are constantly being released. As a result, a "
"significant amount of time is spent adapting to the ever-changing "
"underlying model environment. This, to some extent, hinders the "
"exploration and implementation of AI model applications."
msgstr ""
"åœ¨AIGCåº”ç”¨æ¢ç´¢å’Œç”Ÿäº§è½åœ°ä¸­ï¼Œéš¾ä»¥é¿å…ç›´æ¥ä¸æ¨¡å‹æœåŠ¡å¯¹æ¥ï¼Œä½†æ˜¯ç›®å‰å¤§æ¨¡å‹çš„æ¨ç†éƒ¨ç½²ç›®å‰è¿˜æ²¡æœ‰ä¸€ä¸ªäº‹å®æ ‡å‡†ï¼Œä¸æ–­æœ‰æ–°çš„æ¨¡å‹å‘å¸ƒï¼Œä¹Ÿä¸æ–­æœ‰æ–°çš„è®­ç»ƒå’Œæ¨ç†æ–¹æ³•è¢«æå‡ºï¼Œè€Œæˆ‘ä»¬å°±ä¸å¾—ä¸èŠ±è´¹ç›¸å½“ä¸€éƒ¨åˆ†æ—¶é—´æ¥é€‚é…å¤šå˜çš„åº•å±‚æ¨¡å‹ç¯å¢ƒï¼Œè€Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šåˆ¶çº¦äº†"
" AIGC åº”ç”¨çš„æ¢ç´¢å’Œè½åœ°ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:5
#: c6179ac327734b7ca7b87612988dad29
msgid ""
"We divide the deployment of large models into two layers: the model "
"inference layer and the model deployment layer. The model inference layer"
" corresponds to model inference frameworks such as vLLM, TGI, and "
"TensorRT. The model deployment layer interfaces with the inference layer "
"below and provides model serving capabilities above. We refer to this "
"layer's framework as the model deployment framework. Positioned above the"
" inference frameworks, the model deployment framework offers capabilities"
" such as multiple model instances, multiple inference frameworks, "
"multiple service protocols, multi-cloud support, automatic scaling, and "
"observability."
msgstr ""
"æˆ‘ä»¬å°†å¤§æ¨¡å‹æ¨ç†éƒ¨ç½²åˆ†ä¸ºä¸¤å±‚ï¼šæ¨¡å‹æ¨ç†å±‚ã€æ¨¡å‹éƒ¨ç½²å±‚ã€‚æ¨¡å‹æ¨ç†å±‚ï¼Œå¯¹åº”æ¨¡å‹æ¨ç†æ¡†æ¶ vLLMã€TGI å’Œ TensorRT "
"ç­‰ã€‚æ¨¡å‹éƒ¨ç½²å±‚å‘ä¸‹å¯¹æ¥æ¨ç†å±‚ï¼Œå‘ä¸Šæä¾›æ¨¡å‹æœåŠ¡èƒ½åŠ›ï¼Œè¿™ä¸€å±‚çš„æ¡†æ¶æˆ‘ä»¬ç§°ä¸ºæ¨¡å‹éƒ¨ç½²æ¡†æ¶ï¼Œæ¨¡å‹éƒ¨ç½²æ¡†æ¶åœ¨æ¨ç†æ¡†æ¶ä¹‹ä¸Šï¼Œæä¾›äº†å¤šæ¨¡å‹å®ä¾‹ã€å¤šæ¨ç†æ¡†æ¶ã€å¤šæœåŠ¡åè®®ã€å¤šäº‘ã€è‡ªåŠ¨æ‰©ç¼©å®¹å’Œå¯è§‚æµ‹æ€§ç­‰èƒ½åŠ›ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:7
#: 61bae2fc8e3347248ecf084a3977e448
msgid ""
"In order to deploy DB-GPT to multiple nodes, you can deploy a cluster. "
"The cluster architecture diagram is as follows:"
msgstr "ä¸ºäº†èƒ½å°†DB-GPTéƒ¨ç½²åˆ°å¤šä¸ªèŠ‚ç‚¹ä¸Šï¼Œä½ å¯ä»¥éƒ¨ç½²ä¸€ä¸ªé›†ç¾¤ï¼Œé›†ç¾¤çš„æ¶æ„å›¾å¦‚ä¸‹:"

#: ../../getting_started/install/cluster/cluster.rst:14
#: af8d74ac3c5747b3934d02200afbb4ba
msgid "Design of DB-GPT:"
msgstr "è®¾è®¡ç›®æ ‡"

#: ../../getting_started/install/cluster/cluster.rst:16
#: ab9f332105ac490097501798d7b6cf15
msgid ""
"DB-GPT is designed as a llm deployment framework, taking into account the"
" above design objectives."
msgstr "æ”¯æŒå¤šæ¨¡å‹å’Œå¤šæ¨ç†æ¡†æ¶"

#: ../../getting_started/install/cluster/cluster.rst:18
#: 281c38e2e84940098eeeb435db6d1f05
msgid ""
"Support for llm and inference frameworks: DB-GPT supports the "
"simultaneous deployment of llm and is compatible with multiple inference "
"frameworks such as vLLM, TGI, and TensorRT."
msgstr ""
"åœ¨ DB-GPT ä¸­ï¼Œç›´æ¥æä¾›äº†å¯¹ FastChatã€vLLMå’Œ llama.cpp çš„æ— ç¼æ”¯æŒï¼Œç†è®ºä¸Šå®ƒä»¬æ”¯æŒæ¨¡å‹ DB-GPT "
"éƒ½æ”¯æŒï¼Œå¦‚æœæ‚¨å¯¹æ¨ç†é€Ÿåº¦å’Œå¹¶å‘èƒ½åŠ›æœ‰éœ€æ±‚ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ vLLMï¼Œå¦‚æœæ‚¨å¸Œæœ› CPU æˆ–è€… mac çš„ "
"m1/m2æ€§èƒ½ä¹Ÿè·å¾—ä¸é”™çš„æ¨ç†æ€§èƒ½ï¼Œå¯ä»¥ä½¿ç”¨ llama.cppï¼Œæ­¤å¤–ï¼ŒDB-GPT è¿˜æ”¯æŒäº†å¾ˆå¤šä»£ç†æ¨¡å‹ï¼ˆopenaiã€azure "
"openaiã€google bardã€æ–‡å¿ƒä¸€è¨€ã€é€šä¹‰åƒé—®å’Œæ™ºè°±AIç­‰ï¼‰ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:20
#: ec7e111f2db64c7fa926b1491020ae73
msgid ""
"Scalability and stability: DB-GPT has good scalability, allowing easy "
"addition of new models and inference frameworks. It utilizes a "
"distributed architecture and automatic scaling capabilities to handle "
"high concurrency and large-scale requests, ensuring system stability."
msgstr "è‰¯å¥½çš„æ‰©å±•æ€§å’Œç¨³å®šæ€§â€"

#: ../../getting_started/install/cluster/cluster.rst:22
#: 49566c3e708c4ef3a6135ea6245a5417
msgid ""
"Performance optimization: DB-GPT undergoes performance optimization to "
"provide fast and efficient model inference capabilities, preventing it "
"from becoming a performance bottleneck during inference."
msgstr "æ¡†æ¶æ€§èƒ½ â€œä¸æ‹–åè…¿â€"

#: ../../getting_started/install/cluster/cluster.rst:24
#: 0ae41617a7904dcfadd64ec921d3987e
msgid ""
"Management and observability capabilities: DB-GPT offers management and "
"monitoring functionalities, including model deployment and configuration "
"management, performance monitoring, and logging. It can generate reports "
"on model performance and service status to promptly identify and resolve "
"issues."
msgstr "ç®¡ç†ä¸å¯è§‚æµ‹æ€§èƒ½åŠ›"

#: ../../getting_started/install/cluster/cluster.rst:26
#: 7c7c762642754c8d8e8b7d4eaad55384
msgid ""
"Lightweight: DB-GPT is designed as a lightweight framework to improve "
"deployment efficiency and save resources. It employs efficient algorithms"
" and optimization strategies to minimize resource consumption while "
"maintaining sufficient functionality and performance."
msgstr "è½»é‡åŒ–"

#: ../../getting_started/install/cluster/cluster.rst:29
#: 32c1d24c20ed4155ad05c505a355ebaf
msgid "1.Support for multiple models and inference frameworks"
msgstr "1.æ”¯æŒå¤šæ¨¡å‹å’Œå¤šæ¨ç†æ¡†æ¶"

#: ../../getting_started/install/cluster/cluster.rst:30
#: b0d80a26a0d14ab4a6a82bbdc693a9cc
msgid ""
"The field of large models is evolving rapidly, with new models being "
"released and new methods being proposed for model training and inference."
" We believe that this situation will continue for some time."
msgstr "å½“å‰å¤§æ¨¡å‹é¢†åŸŸå‘å±•å¯è°“æ—¥æ–°æœˆå¼‚ï¼Œä¸æ–­æœ‰æ–°çš„æ¨¡å‹å‘å¸ƒï¼Œåœ¨æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æ–¹é¢ï¼Œä¹Ÿä¸æ–­æœ‰æ–°çš„æ–¹æ³•è¢«æå‡ºã€‚æˆ‘ä»¬åˆ¤æ–­ï¼Œè¿™æ ·æƒ…å†µè¿˜ä¼šæŒç»­ä¸€æ®µæ—¶é—´ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:32
#: 8b7c3830e5d64567bef9244bd0c4442d
msgid ""
"For most users exploring and implementing AI applications, this situation"
" has its pros and cons. The benefits are apparent, as it brings new "
"opportunities and advancements. However, one drawback is that users may "
"feel compelled to constantly try and explore new models and inference "
"frameworks."
msgstr ""
"å¤§äºå¤§éƒ¨åˆ† AIGC "
"åº”ç”¨åœºæ™¯æ¢ç´¢å’Œè½åœ°çš„ç”¨æˆ·æ¥è¯´ï¼Œè¿™ç§æƒ…å†µæœ‰åˆ©ä¹Ÿæœ‰å¼Šï¼Œåˆ©æ— éœ€å¤šè¨€ï¼Œè€Œå¼Šç«¯ä¹‹ä¸€å°±åœ¨äºè¢«â€œç‰µç€é¼»å­èµ°â€ï¼Œéœ€è¦ä¸æ–­å»å°è¯•å’Œæ¢ç´¢æ–°çš„æ¨¡å‹ã€æ–°çš„æ¨ç†æ¡†æ¶ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:34
#: b2f7bf8f9ef4406989a366d66e59794b
msgid ""
"In DB-GPT, seamless support is provided for FastChat, vLLM, and "
"llama.cpp. In theory, any model supported by these frameworks is also "
"supported by DB-GPT. If you have requirements for faster inference speed "
"and concurrency, you can directly use vLLM. If you want good inference "
"performance on CPU or Apple's M1/M2 chips, you can use llama.cpp. "
"Additionally, DB-GPT also supports various proxy models from OpenAI, "
"Azure OpenAI, Google BARD, Wenxin Yiyan, Tongyi Qianwen, and Zhipu AI, "
"among others."
msgstr ""
"åœ¨ DB-GPT ä¸­ï¼Œç›´æ¥æä¾›äº†å¯¹ FastChatã€vLLMå’Œ llama.cpp çš„æ— ç¼æ”¯æŒï¼Œç†è®ºä¸Šå®ƒä»¬æ”¯æŒæ¨¡å‹ DB-GPT "
"éƒ½æ”¯æŒï¼Œå¦‚æœæ‚¨å¯¹æ¨ç†é€Ÿåº¦å’Œå¹¶å‘èƒ½åŠ›æœ‰éœ€æ±‚ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ vLLMï¼Œå¦‚æœæ‚¨å¸Œæœ› CPU æˆ–è€… mac çš„ "
"m1/m2æ€§èƒ½ä¹Ÿè·å¾—ä¸é”™çš„æ¨ç†æ€§èƒ½ï¼Œå¯ä»¥ä½¿ç”¨ llama.cppï¼Œæ­¤å¤–ï¼ŒDB-GPT è¿˜æ”¯æŒäº†å¾ˆå¤šä»£ç†æ¨¡å‹ï¼ˆopenaiã€azure "
"openaiã€google bardã€æ–‡å¿ƒä¸€è¨€ã€é€šä¹‰åƒé—®å’Œæ™ºè°±AIç­‰ï¼‰ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:37
#: 9f894d801c364d58814f295222567992
msgid "2.Have good scalability and stability"
msgstr "2.æ‰©å±•æ€§å’Œç¨³å®šæ€§è¦è¶³å¤Ÿå¥½"

#: ../../getting_started/install/cluster/cluster.rst:38
#: 5423f1f5f0e94804becd3caa500b4046
msgid ""
"A comprehensive model deployment framework consists of several "
"components: the Model Worker, which directly interfaces with the "
"underlying inference frameworks; the Model Controller, which manages and "
"maintains multiple model components; and the Model API, which provides "
"external model serving capabilities."
msgstr ""
"ä¸€ä¸ªæ¯”è¾ƒå®Œå–„æ¨¡å‹éƒ¨ç½²æ¡†æ¶éœ€è¦å¤šä¸ªéƒ¨åˆ†ç»„æˆï¼Œä¸åº•å±‚æ¨ç†æ¡†æ¶ç›´æ¥å¯¹æ¥çš„ Model Workerï¼Œç®¡ç†å’Œç»´æŠ¤å¤šä¸ªæ¨¡å‹ç»„ä»¶çš„ Model "
"Controller ä»¥åŠå¯¹å¤–æä¾›æ¨¡å‹æœåŠ¡èƒ½åŠ›çš„ Model APIã€‚"

#: ../../getting_started/install/cluster/cluster.rst:40
#: 6e0948e9a239405ca7d90543569f35fa
msgid ""
"The Model Worker plays a crucial role and needs to be highly extensible. "
"It can be specialized for deploying large language models, embedding "
"models, or other types of models. The choice of Model Worker depends on "
"the deployment environment, such as a regular physical server "
"environment, a Kubernetes environment, or specific cloud environments "
"provided by various cloud service providers."
msgstr ""
"å…¶ä¸­ Model Worker å¿…é¡»è¦å¯ä»¥æ‰©å±•ï¼Œå¯ä»¥æ˜¯ä¸“é—¨éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹çš„ Model Workerï¼Œä¹Ÿå¯ä»¥æ˜¯ç”¨æ¥éƒ¨ç½² Embedding æ¨¡å‹çš„"
" Model Workerã€‚"

#: ../../getting_started/install/cluster/cluster.rst:42
#: 54eba96c95c847e6af77ba94114419ab
msgid ""
"Having different Model Worker options allows users to select the most "
"suitable one based on their specific requirements and infrastructure. "
"This flexibility enables efficient deployment and utilization of models "
"across different environments."
msgstr "å½“ç„¶ä¹Ÿå¯ä»¥æ ¹æ®éƒ¨ç½²çš„ç¯å¢ƒï¼Œå¦‚æ™®é€šç‰©ç†æœºç¯å¢ƒã€kubernetes ç¯å¢ƒä»¥åŠä¸€äº›ç‰¹å®šäº‘æœåŠ¡å•†æä¾›çš„äº‘ç¯å¢ƒç­‰æ¥é€‰æ‹©ä¸åŒ Model Worker"

#: ../../getting_started/install/cluster/cluster.rst:44
#: 693ec0c1b9274f64a1d8fcbd5a8a273d
msgid ""
"The Model Controller, responsible for managing model metadata, also needs"
" to be scalable. Different deployment environments and model management "
"requirements may call for different choices of Model Controllers."
msgstr ""
"ç”¨æ¥ç®¡ç†æ¨¡å‹å…ƒæ•°æ®çš„ Model Controller ä¹Ÿéœ€è¦å¯æ‰©å±•ï¼Œä¸åŒçš„éƒ¨ç½²ç¯å¢ƒå·²ç»ä¸åŒçš„æ¨¡å‹ç®¡æ§è¦æ±‚æ¥é€‰æ‹©ä¸åŒçš„ Model "
"Controllerã€‚"

#: ../../getting_started/install/cluster/cluster.rst:46
#: 616c0dc43dd84069bde396f1cc99e316
msgid ""
"Furthermore, I believe that model serving shares many similarities with "
"traditional microservices. In microservices, a service can have multiple "
"instances, and all instances are registered in a central registry. "
"Service consumers retrieve the list of instances based on the service "
"name from the registry and select a specific instance for invocation "
"using a load balancing strategy."
msgstr "å¦å¤–ï¼Œåœ¨æˆ‘çœ‹æ¥ï¼Œæ¨¡å‹æœåŠ¡ä¸ä¼ ç»Ÿçš„å¾®æœåŠ¡æœ‰å¾ˆå¤šå…±é€šä¹‹å¤„ï¼Œåœ¨å¾®æœåŠ¡ä¸­ï¼Œå¾®æœåŠ¡ä¸­æŸä¸ªæœåŠ¡å¯ä»¥æœ‰å¤šä¸ªæœåŠ¡å®ä¾‹ï¼Œæ‰€æœ‰çš„æœåŠ¡å®ä¾‹éƒ½ç»Ÿä¸€æ³¨å†Œåˆ°æ³¨å†Œä¸­å¿ƒï¼ŒæœåŠ¡è°ƒç”¨æ–¹æ ¹æ®æœåŠ¡åç§°ä»æ³¨å†Œä¸­å¿ƒæ‹‰å–è¯¥æœåŠ¡åå¯¹åº”çš„æœåŠ¡åˆ—è¡¨ï¼Œç„¶åæ ¹æ®ä¸€å®šçš„è´Ÿè½½å‡è¡¡ç­–ç•¥é€‰æ‹©æŸä¸ªå…·ä½“çš„æœåŠ¡å®ä¾‹å»è°ƒç”¨ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:48
#: 83389d65894f44598a0eda3984a41cb3
msgid ""
"Similarly, in model deployment, a model can have multiple instances, and "
"all instances can be registered in a model registry. Model service "
"consumers retrieve the list of instances based on the model name from the"
" registry and select a specific instance for invocation using a model-"
"specific load balancing strategy."
msgstr "è€Œåœ¨æ¨¡å‹éƒ¨ç½²ä¸­ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘è¿™æ ·çš„æ¶æ„ï¼ŒæŸä¸€ä¸ªæ¨¡å‹å¯ä»¥æœ‰å¤šä¸ªæ¨¡å‹å®ä¾‹ï¼Œæ‰€æœ‰çš„æ¨¡å‹å®ä¾‹éƒ½ç»Ÿä¸€æ³¨å†Œåˆ°æ¨¡å‹æ³¨å†Œä¸­å¿ƒï¼Œç„¶åæ¨¡å‹æœåŠ¡è°ƒç”¨æ–¹æ ¹æ®æ¨¡å‹åç§°åˆ°æ³¨å†Œä¸­å¿ƒå»æ‹‰å–æ¨¡å‹å®ä¾‹åˆ—è¡¨ï¼Œç„¶åæ ¹æ®æ¨¡å‹çš„è´Ÿè½½å‡è¡¡ç­–ç•¥å»è°ƒç”¨æŸä¸ªå…·ä½“çš„çš„æ¨¡å‹å®ä¾‹ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:50
#: 8524b6f0536446a6900715aaefcdee98
msgid ""
"Introducing a model registry, responsible for storing model instance "
"metadata, enables such an architecture. The model registry can leverage "
"existing service registries used in microservices (such as Nacos, Eureka,"
" etcd, Consul, etc.) as implementations. This ensures high availability "
"of the entire deployment system."
msgstr ""
"è¿™é‡Œæˆ‘ä»¬å¼•å…¥æ¨¡å‹æ³¨å†Œä¸­å¿ƒï¼Œå®ƒè´Ÿè´£å­˜å‚¨ Model Controller ä¸­çš„æ¨¡å‹å®ä¾‹å…ƒæ•°æ®ï¼Œå®ƒå¯ä»¥ç›´æ¥ä½¿ç”¨å¾®æœåŠ¡ä¸­çš„æ³¨å†Œä¸­å¿ƒä½œä¸ºå®ç°ï¼ˆå¦‚ "
"nacosã€eurekaã€etcd å’Œ consul ç­‰ï¼‰ï¼Œè¿™æ ·æ•´ä¸ªéƒ¨ç½²ç³»ç»Ÿä¾¿å¯ä»¥åšåˆ°é«˜å¯ç”¨ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:53
#: ff904ff9192248bda12b5ccae28df26f
msgid "3.High performance for framework."
msgstr "3.æ¡†æ¶æ€§èƒ½â€œä¸æ‹–åè…¿â€"

#: ../../getting_started/install/cluster/cluster.rst:54
#: c6baf9f2a059487bbf7c3996e401effb
msgid ""
"and optimization are complex tasks, and inappropriate framework designs "
"can increase this complexity. In our view, to ensure that the deployment "
"framework does not lag behind in terms of performance, there are two main"
" areas of focus:"
msgstr "æ¡†æ¶å±‚ä¸åº”è¯¥æˆä¸ºæ¨¡å‹æ¨ç†æ€§èƒ½çš„ç“¶é¢ˆï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œç¡¬ä»¶åŠæ¨ç†æ¡†æ¶å†³å®šäº†æ¨¡å‹æœåŠ¡çš„æœåŠ¡èƒ½åŠ›ï¼Œæ¨¡å‹çš„æ¨ç†éƒ¨ç½²å’Œä¼˜åŒ–æ˜¯ä¸€é¡¹å¤æ‚çš„å·¥ç¨‹ï¼Œè€Œä¸æ°å½“çš„æ¡†æ¶è®¾è®¡å´å¯èƒ½å¢åŠ è¿™ç§å¤æ‚åº¦ï¼Œåœ¨æˆ‘ä»¬çœ‹æ¥ï¼Œéƒ¨ç½²æ¡†æ¶ä¸ºäº†åœ¨æ€§èƒ½ä¸Šâ€œä¸æ‹–åè…¿â€ï¼Œæœ‰ä¸¤ä¸ªä¸»è¦å…³æ³¨ç‚¹ï¼š"

#: ../../getting_started/install/cluster/cluster.rst:56
#: f74418d5394b4afd96578c28ff306116
msgid ""
"Avoid excessive encapsulation: The more encapsulation and longer the "
"chain, the more challenging it becomes to identify performance issues."
msgstr "é¿å…è¿‡å¤šçš„å°è£…ï¼šå°è£…è¶Šå¤šã€é“¾è·¯è¶Šé•¿ï¼Œæ€§èƒ½é—®é¢˜è¶Šéš¾ä»¥æ’æŸ¥ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:58
#: 692bc702a7f54d48b67c36ac1dc38867
msgid ""
"High-performance communication design: High-performance communication "
"involves various aspects that cannot be elaborated in detail here. "
"However, considering that Python occupies a prominent position in current"
" AIGC applications, asynchronous interfaces are crucial for service "
"performance in Python. Therefore, the model serving layer should only "
"provide asynchronous interfaces and be compatible with the layers that "
"interface with the model inference framework. If the model inference "
"framework offers asynchronous interfaces, direct integration should be "
"implemented. Otherwise, synchronous-to-asynchronous task conversion "
"should be used to provide support."
msgstr ""
"é«˜æ€§èƒ½çš„é€šä¿¡è®¾è®¡ï¼šé«˜æ€§èƒ½é€šä¿¡æ¶‰åŠçš„ç‚¹å¾ˆå¤šï¼Œè¿™é‡Œä¸åšè¿‡å¤šé˜è¿°ã€‚ç”±äºç›®å‰ AIGC åº”ç”¨ä¸­ï¼ŒPython å æ®é¢†å¯¼åœ°ä½ï¼Œåœ¨ Python "
"ä¸­ï¼Œå¼‚æ­¥æ¥å£å¯¹äºæœåŠ¡çš„æ€§èƒ½è‡³å…³é‡è¦ï¼Œå› æ­¤ï¼Œæ¨¡å‹æœåŠ¡å±‚åªæä¾›å¼‚æ­¥æ¥å£ï¼Œä¸æ¨¡å‹æ¨ç†æ¡†æ¶å¯¹æ¥çš„å±‚åšå…¼å®¹ï¼Œå¦‚æœæ¨¡å‹æ¨ç†æ¡†æ¶æä¾›äº†å¼‚æ­¥æ¥å£åˆ™ç›´æ¥å¯¹æ¥ï¼Œå¦åˆ™ä½¿ç”¨åŒæ­¥è½¬å¼‚æ­¥çš„ä»»åŠ¡çš„æ–¹å¼æ”¯æŒã€‚"

#: ../../getting_started/install/cluster/cluster.rst:61
#: 3b2bed671a264a13a61b7337e4577185
msgid "4.Management and monitoring capabilities."
msgstr "4.å…·å¤‡ä¸€å®šçš„ç®¡ç†å’Œç›‘æ§èƒ½åŠ›"

#: ../../getting_started/install/cluster/cluster.rst:62
#: 010c26d97cb748d28f86d9d58bdb3c6d
msgid ""
"In the exploration or production implementation of AIGC (Artificial "
"Intelligence and General Computing) applications, it is important for the"
" model deployment system to have certain management capabilities. This "
"involves controlling the deployed model instances through APIs or "
"command-line interfaces, such as for online/offline management, "
"restarting, and debugging."
msgstr ""
"åœ¨ AIGC åº”ç”¨æ¢ç´¢ä¸­æˆ–è€… AIGC åº”ç”¨ç”Ÿäº§è½åœ°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ¨¡å‹éƒ¨ç½²ç³»ç»Ÿèƒ½å…·å¤‡ä¸€å®šç®¡ç†èƒ½åŠ›ï¼šé€šè¿‡ API "
"æˆ–è€…å‘½ä»¤è¡Œç­‰å¯¹éƒ¨ç½²çš„æ¨¡å‹å®ä¾‹è¿›è¡Œä¸€å®šç®¡æ§ï¼ˆå¦‚ä¸Šçº¿ã€ä¸‹çº¿ã€é‡å¯å’Œ debug ç­‰ï¼‰ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:64
#: 49f450fdb5f24d578b4cb8427e57ec15
msgid ""
"Observability is a crucial capability in production systems, and I "
"believe it is equally, if not more, important in AIGC applications. This "
"is because user experiences and interactions with the system are more "
"complex. In addition to traditional observability metrics, we are also "
"interested in user input information and corresponding contextual "
"information, which specific model instance and parameters were invoked, "
"the content and response time of model outputs, user feedback, and more."
msgstr ""
"å¯è§‚æµ‹æ€§æ˜¯ç”Ÿäº§ç³»ç»Ÿçš„ä¸€é¡¹é‡è¦èƒ½åŠ›ï¼Œä¸ªäººè®¤ä¸ºåœ¨ AIGC "
"åº”ç”¨ä¸­ï¼Œå¯è§‚æµ‹æ€§åŒæ ·é‡è¦ï¼Œç”šè‡³æ›´åŠ é‡è¦ï¼Œå› ä¸ºç”¨æˆ·çš„ä½“éªŒã€ç”¨æˆ·ä¸ç³»ç»Ÿçš„äº¤äº’è¡Œä¸ºæ›´å¤æ‚ï¼Œé™¤äº†ä¼ ç»Ÿçš„è§‚æµ‹æŒ‡æ ‡å¤–ï¼Œæˆ‘ä»¬è¿˜æ›´åŠ å…³å¿ƒç”¨æˆ·çš„è¾“å…¥ä¿¡æ¯åŠå…¶å¯¹åº”çš„åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯ã€è°ƒç”¨äº†å“ªä¸ªæ¨¡å‹å®ä¾‹å’Œæ¨¡å‹å‚æ•°ã€æ¨¡å‹è¾“å‡ºçš„å†…å®¹å’Œå“åº”æ—¶é—´ã€ç”¨æˆ·åé¦ˆç­‰ç­‰ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:66
#: 18c940b2e65d4f57ba54b9671ac02254
msgid ""
"By analyzing this information, we can identify performance bottlenecks in"
" model services and gather user experience data (e.g., response latency, "
"problem resolution, and user satisfaction extracted from user content). "
"These insights serve as important foundations for further optimizing the "
"entire application."
msgstr "æˆ‘ä»¬å¯ä»¥ä»è¿™äº›ä¿¡æ¯ä¸­å‘ç°ä¸€éƒ¨åˆ†æ¨¡å‹æœåŠ¡çš„æ€§èƒ½ç“¶é¢ˆï¼Œä»¥åŠä¸€éƒ¨åˆ†ç”¨æˆ·ä½“éªŒæ•°æ®ï¼ˆå“åº”å»¶è¿Ÿå¦‚ä½•ï¼Ÿæ˜¯å¦è§£å†³äº†ç”¨æˆ·çš„é—®é¢˜ä¹ŸåŠç”¨æˆ·å†…å®¹ä¸­æå–å‡ºç”¨æˆ·æ»¡æ„åº¦ç­‰ç­‰ï¼‰ï¼Œè¿™äº›éƒ½æ˜¯æ•´ä¸ªåº”ç”¨è¿›ä¸€æ­¥ä¼˜åŒ–çš„é‡è¦ä¾æ®ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:68
#: a1aa65d7b0694b75a8a298090b3cbfac
#, fuzzy
msgid ""
"On :ref:`Deploying on standalone mode <standalone-index>`. Standalone "
"Deployment."
msgstr "å…³äº :ref:`åœ¨æœ¬åœ°æœºå™¨ä¸Šéƒ¨ç½² <local-cluster-index>`ã€‚æœ¬åœ°é›†ç¾¤éƒ¨ç½²ã€‚"

#: ../../getting_started/install/cluster/cluster.rst:69
#: 2d74de97891c4a31806ce286c3818631
#, fuzzy
msgid ""
"On :ref:`Deploying on cluster mode <local-cluster-index>`. Cluster "
"Deployment."
msgstr "å…³äº :ref:`åœ¨æœ¬åœ°æœºå™¨ä¸Šéƒ¨ç½² <local-cluster-index>`ã€‚æœ¬åœ°é›†ç¾¤éƒ¨ç½²ã€‚"

#~ msgid ""
#~ "When it comes to model deployment, "
#~ "performance is of utmost importance. The"
#~ " framework should be optimized to "
#~ "ensure efficient and fast model "
#~ "inference capabilities. It should not "
#~ "become a performance bottleneck and "
#~ "should be capable of handling high "
#~ "volumes of requests without compromising "
#~ "response times."
#~ msgstr "æ¡†æ¶å±‚ä¸åº”è¯¥æˆä¸ºæ¨¡å‹æ¨ç†æ€§èƒ½çš„ç“¶é¢ˆï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œç¡¬ä»¶åŠæ¨ç†æ¡†æ¶å†³å®šäº†æ¨¡å‹æœåŠ¡çš„æœåŠ¡èƒ½åŠ›ï¼Œæ¨¡å‹çš„æ¨ç†éƒ¨ç½²å’Œä¼˜åŒ–æ˜¯ä¸€é¡¹å¤æ‚çš„å·¥ç¨‹ï¼Œè€Œä¸æ°å½“çš„æ¡†æ¶è®¾è®¡å´å¯èƒ½å¢åŠ è¿™ç§å¤æ‚åº¦ï¼Œåœ¨æˆ‘ä»¬çœ‹æ¥ï¼Œéƒ¨ç½²æ¡†æ¶ä¸ºäº†åœ¨æ€§èƒ½ä¸Šâ€œä¸æ‹–åè…¿â€ï¼Œæœ‰ä¸¤ä¸ªä¸»è¦å…³æ³¨ç‚¹ï¼š"

#~ msgid ""
#~ "To achieve this, the framework can "
#~ "employ various performance optimization "
#~ "techniques. This may include utilizing "
#~ "efficient algorithms, leveraging hardware "
#~ "acceleration (such as GPUs or "
#~ "specialized AI chips), optimizing memory "
#~ "usage, and implementing parallel processing"
#~ " techniques to maximize throughput."
#~ msgstr ""

#~ msgid ""
#~ "By prioritizing performance optimization, the"
#~ " framework can provide seamless and "
#~ "efficient model inference, enabling real-"
#~ "time and high-performance applications "
#~ "without impeding the overall system "
#~ "performance."
#~ msgstr ""

#~ msgid ""
#~ "To ensure the stability and reliability"
#~ " of model deployment, the framework "
#~ "needs to provide management and "
#~ "monitoring functionalities. This includes "
#~ "managing the lifecycle of models, such"
#~ " as model registration, updates, and "
#~ "deletion. Additionally, the framework should"
#~ " offer monitoring and logging of "
#~ "performance metrics, resource utilization, and"
#~ " system health to promptly identify "
#~ "and resolve potential issues."
#~ msgstr ""
#~ "åœ¨ AIGC åº”ç”¨æ¢ç´¢ä¸­æˆ–è€… AIGC "
#~ "åº”ç”¨ç”Ÿäº§è½åœ°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ¨¡å‹éƒ¨ç½²ç³»ç»Ÿèƒ½å…·å¤‡ä¸€å®šç®¡ç†èƒ½åŠ›ï¼šé€šè¿‡ API "
#~ "æˆ–è€…å‘½ä»¤è¡Œç­‰å¯¹éƒ¨ç½²çš„æ¨¡å‹å®ä¾‹è¿›è¡Œä¸€å®šç®¡æ§ï¼ˆå¦‚ä¸Šçº¿ã€ä¸‹çº¿ã€é‡å¯å’Œ debug ç­‰ï¼‰ã€‚"

#~ msgid ""
#~ "Management capabilities may involve user "
#~ "permission management, model versioning, and"
#~ " configuration management to facilitate "
#~ "team collaboration and manage multiple "
#~ "versions and configurations of models."
#~ msgstr ""

#~ msgid ""
#~ "Monitoring capabilities can include real-"
#~ "time monitoring of model performance "
#~ "metrics such as inference latency and"
#~ " throughput. Furthermore, monitoring system "
#~ "resource usage, such as CPU, memory, "
#~ "network, and system health, along with"
#~ " error logging, can be valuable for"
#~ " diagnostics and troubleshooting."
#~ msgstr ""
#~ "å¯è§‚æµ‹æ€§æ˜¯ç”Ÿäº§ç³»ç»Ÿçš„ä¸€é¡¹é‡è¦èƒ½åŠ›ï¼Œä¸ªäººè®¤ä¸ºåœ¨ AIGC "
#~ "åº”ç”¨ä¸­ï¼Œå¯è§‚æµ‹æ€§åŒæ ·é‡è¦ï¼Œç”šè‡³æ›´åŠ é‡è¦ï¼Œå› ä¸ºç”¨æˆ·çš„ä½“éªŒã€ç”¨æˆ·ä¸ç³»ç»Ÿçš„äº¤äº’è¡Œä¸ºæ›´å¤æ‚ï¼Œé™¤äº†ä¼ ç»Ÿçš„è§‚æµ‹æŒ‡æ ‡å¤–ï¼Œæˆ‘ä»¬è¿˜æ›´åŠ å…³å¿ƒç”¨æˆ·çš„è¾“å…¥ä¿¡æ¯åŠå…¶å¯¹åº”çš„åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯ã€è°ƒç”¨äº†å“ªä¸ªæ¨¡å‹å®ä¾‹å’Œæ¨¡å‹å‚æ•°ã€æ¨¡å‹è¾“å‡ºçš„å†…å®¹å’Œå“åº”æ—¶é—´ã€ç”¨æˆ·åé¦ˆç­‰ç­‰ã€‚"

#~ msgid ""
#~ "By providing management and monitoring "
#~ "capabilities, the framework can assist "
#~ "users in effectively managing and "
#~ "maintaining deployed models, ensuring system"
#~ " stability and reliability, and enabling"
#~ " timely responses to and resolution "
#~ "of issues, thus enhancing overall system"
#~ " efficiency and availability."
#~ msgstr ""

