# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-29 20:50+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy/deploy.md:1
#: b4f766ca21d241e2849ee0a277a0e8f0
msgid "Installation From Source"
msgstr "源码安装"

#: ../../getting_started/install/deploy/deploy.md:3
#: 9cf72ef201ba4c7a99da8d7de9249cf4
msgid ""
"This tutorial gives you a quick walkthrough about use DB-GPT with you "
"environment and data."
msgstr "本教程为您提供了关于如何使用DB-GPT的使用指南。"

#: ../../getting_started/install/deploy/deploy.md:5
#: b488acb9552043df96e9f01277375b56
msgid "Installation"
msgstr "安装"

#: ../../getting_started/install/deploy/deploy.md:7
#: e1eb3aafea0c4b82b8d8163b947677dd
msgid "To get started, install DB-GPT with the following steps."
msgstr "请按照以下步骤安装DB-GPT"

#: ../../getting_started/install/deploy/deploy.md:9
#: 4139c4e62e874dc58136b1f8fe0715fe
msgid "1. Hardware Requirements"
msgstr "1. 硬件要求"

#: ../../getting_started/install/deploy/deploy.md:10
#: c34a204cfa6e4973bfd94e683195c17b
msgid ""
"As our project has the ability to achieve ChatGPT performance of over "
"85%, there are certain hardware requirements. However, overall, the "
"project can be deployed and used on consumer-grade graphics cards. The "
"specific hardware requirements for deployment are as follows:"
msgstr "由于我们的项目有能力达到85%以上的ChatGPT性能，所以对硬件有一定的要求。但总体来说，我们在消费级的显卡上即可完成项目的部署使用，具体部署的硬件说明如下:"

#: ../../getting_started/install/deploy/deploy.md
#: 3a92203e861b42c9af3d4b687d83de5e
msgid "GPU"
msgstr "GPU"

#: ../../getting_started/install/deploy/deploy.md
#: 6050741571574eb8b9e498a5b3a7e347 c0a7e2aecb4b48949c3e5a4d479ee7b5
msgid "VRAM Size"
msgstr "显存"

#: ../../getting_started/install/deploy/deploy.md
#: 247159f568e4476ca6c5e78015c7a8f0
msgid "Performance"
msgstr "Performance"

#: ../../getting_started/install/deploy/deploy.md
#: 871113cbc58743ef989a366b76e8c645
msgid "RTX 4090"
msgstr "RTX 4090"

#: ../../getting_started/install/deploy/deploy.md
#: 81327b7e9a984ec99cae779743d174df c237f392162c42d28ec694d17c3f281c
msgid "24 GB"
msgstr "24 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 6e19f23bae05467ba03f1ebb194e0c03
msgid "Smooth conversation inference"
msgstr "Smooth conversation inference"

#: ../../getting_started/install/deploy/deploy.md
#: 714a48b2c4a943819819a6af034f1998
msgid "RTX 3090"
msgstr "RTX 3090"

#: ../../getting_started/install/deploy/deploy.md
#: 06dae55d443c48b1b3fbab85222c3adb
msgid "Smooth conversation inference, better than V100"
msgstr "Smooth conversation inference, better than V100"

#: ../../getting_started/install/deploy/deploy.md
#: 5d50db167b244d65a8be1dab4acda37d
msgid "V100"
msgstr "V100"

#: ../../getting_started/install/deploy/deploy.md
#: 0d72262c85d148d8b1680d1d9f8fa2c9 e10db632889444a78e123773a30f23cf
msgid "16 GB"
msgstr "16 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 1c0379e653cf46f19d83535c568c54c8 aee8eb48e7804572af351dcfaea5b0fb
msgid "Conversation inference possible, noticeable stutter"
msgstr "Conversation inference possible, noticeable stutter"

#: ../../getting_started/install/deploy/deploy.md
#: 5bc90343dcef48c197438f01efe52bfc
msgid "T4"
msgstr "T4"

#: ../../getting_started/install/deploy/deploy.md:19
#: c9b5f973d19645d39b1892c00526afa7
msgid ""
"if your VRAM Size is not enough, DB-GPT supported 8-bit quantization and "
"4-bit quantization."
msgstr "如果你的显存不够，DB-GPT支持8-bit和4-bit量化版本"

#: ../../getting_started/install/deploy/deploy.md:21
#: 5e488271eede411d882f62ec8524dd4a
msgid ""
"Here are some of the VRAM size usage of the models we tested in some "
"common scenarios."
msgstr "这里是量化版本的相关说明"

#: ../../getting_started/install/deploy/deploy.md
#: 2cc65f16fa364088bedd0e58b6871ec8
msgid "Model"
msgstr "Model"

#: ../../getting_started/install/deploy/deploy.md
#: d0e1a0d418f74e4b9f5922b17f0c8fcf
msgid "Quantize"
msgstr "Quantize"

#: ../../getting_started/install/deploy/deploy.md
#: 460b418ab7eb402eae7a0f86d1fda4bf 5e456423a9fa4c0392b08d32f3082f6f
msgid "vicuna-7b-v1.5"
msgstr "vicuna-7b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 0f290c12b9324a07affcfd66804b82d7 29c81ce163e749b99035942a3b18582a
#: 3a4f4325774d452f8c174cac5fe8de47 584f986a1afb4086a0382a9f7e79c55f
#: 994c744ac67249f4a43b3bba360c0bbf aa9c82f660454143b9212842ffe0e0d6
#: ac7b00313284410b9253c4a768a30f0c
msgid "4-bit"
msgstr "4-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 27401cbb0f2542e2aaa449a586aad2d1 2a1d2d10001f4d9f9b9961c28c592280
#: b69a59c6e4a7458c91be814a98502632
msgid "8 GB"
msgstr "8 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 0a15518df1b94492b610e47f3c7bb4f6 1f1852ceae0b4c21a020dc9ef4f8b20b
#: 89ad803f6bd24b5d9708a6d4bd48a54f ac7c222678d34637a03546dcb5949668
#: b12e1599bdcb4d27ad4e4a83f12de916 c80ba4ddc1634093842a6f284b7b22bb
#: f63b900e4b844b3196c4c221b36d31f7
msgid "8-bit"
msgstr "8-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 02f72ed48b784b05b2fcaf4ea33fcba8 17285314376044bf9d9a82f9001f39dc
#: 403178173a784bdf8d02fe856849a434 4875c6b595484091b622602d9ef0d3e8
#: 4b11125d4b0c40c488bffb130f4f2b9f e2418c76e7e04101821f29650d111a4a
msgid "12 GB"
msgstr "12 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 01dfd16f70cf4128a49ca7bc79f77042 a615efffecb24addba759d05ef61a1c0
msgid "vicuna-13b-v1.5"
msgstr "vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 412ddfa6e6fb4567984f757cf74b3bfc 529650341d96466a93153d58ddef0ec9
#: 6176929d59bb4e31a37cbba8a81a489f
msgid "20 GB"
msgstr "20 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 566b7aa7bc88421a9364cef6bfbeae48 ae32a218d07e44c796ca511972ea2cb0
msgid "llama-2-7b"
msgstr "llama-2-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 1ac748eb518b4017accb98873fe1a8e5 528109c765e54b3caf284e7794abd468
msgid "llama-2-13b"
msgstr "llama-2-13b"

#: ../../getting_started/install/deploy/deploy.md
#: dfb5c0fa9e82423ab1de9256b3b3f215 f861be75871d40849f896859d0b8be4c
msgid "llama-2-70b"
msgstr "llama-2-70b"

#: ../../getting_started/install/deploy/deploy.md
#: 5568529a82cd4c49812ab2fd46ff9bf0
msgid "48 GB"
msgstr "48 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 4ba730f4faa64df9a0a9f72cb3eb0c88
msgid "80 GB"
msgstr "80 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 47221748d6d5417abc25e28b6905bc6f 6023d535095a4cb9a99343c2dfddc927
msgid "baichuan-7b"
msgstr "baichuan-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 55011d4e0bed451dbdda75cb8b258fa5 bc296e4bd582455ca64afc74efb4ebc8
msgid "baichuan-13b"
msgstr "baichuan-13b"

#: ../../getting_started/install/deploy/deploy.md:40
#: 4bfd52634a974776933c93227f419cdb
msgid "2. Install"
msgstr "2. Install"

#: ../../getting_started/install/deploy/deploy.md:45
#: 647f09001d4c4124bed11da272306946
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration.  For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. [How to install "
"Miniconda](https://docs.conda.io/en/latest/miniconda.html)"
msgstr ""
"目前使用Sqlite作为默认数据库，因此DB-"
"GPT快速部署不需要部署相关数据库服务。如果你想使用其他数据库，需要先部署相关数据库服务。我们目前使用Miniconda进行python环境和包依赖管理[安装"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy/deploy.md:54
#: bf9fcf320ca94dbd855016088800b1a9
msgid "Before use DB-GPT Knowledge"
msgstr "在使用知识库之前"

#: ../../getting_started/install/deploy/deploy.md:60
#: e0cb6cb46a474c4ca16edf73c82b58ca
msgid ""
"Once the environment is installed, we have to create a new folder "
"\"models\" in the DB-GPT project, and then we can put all the models "
"downloaded from huggingface in this directory"
msgstr "如果你已经安装好了环境需要创建models, 然后到huggingface官网下载模型"

#: ../../getting_started/install/deploy/deploy.md:63
#: 03b1bf35528d4cdeb735047aa840d6fe
msgid "Notice make sure you have install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:65
#: f8183907e7c044f695f86943b412d84a
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:67
#: 3bc042bd5cac4007afc9f68e7b5044fe
msgid "ubuntu:app-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:69
#: 5915ed1290e84ed9b6782c6733d88891
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:86
#: 104f1e75b0a54300af440ca3b64217a3
msgid ""
"The model files are large and will take a long time to download. During "
"the download, let's configure the .env file, which needs to be copied and"
" created from the .env.template"
msgstr "模型文件很大，需要很长时间才能下载。在下载过程中，让我们配置.env文件，它需要从。env.template中复制和创建。"

#: ../../getting_started/install/deploy/deploy.md:88
#: 228c6729c23f4e17b0475b834d7edb01
msgid ""
"if you want to use openai llm service, see [LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"
msgstr ""
"如果想使用openai大模型服务, 可以参考[LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"

#: ../../getting_started/install/deploy/deploy.md:91
#: c444514ba77b46468721888fe7df9e74
msgid "cp .env.template .env"
msgstr "cp .env.template .env"

#: ../../getting_started/install/deploy/deploy.md:94
#: 1514e937757e461189b369da73884a6c
msgid ""
"You can configure basic parameters in the .env file, for example setting "
"LLM_MODEL to the model to be used"
msgstr "您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。"

#: ../../getting_started/install/deploy/deploy.md:96
#: 4643cdf76bd947fdb86fc4691b98935c
msgid ""
"([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。([Vicuna-v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5)， "
"目前Vicuna-v1.5模型(基于llama2)已经开源了，我们推荐你使用这个模型通过设置LLM_MODEL=vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md:98
#: acf91810f12b4ad0bd830299eb24850f
msgid "3. Run"
msgstr "3. Run"

#: ../../getting_started/install/deploy/deploy.md:100
#: ea82d67451724c2399f8903ea3c52dff
msgid "**(Optional) load examples into SQLlite**"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:105
#: a00987ec21364389b7feec58b878c2a1
msgid "On windows platform:"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:110
#: db5c000e6abe4e1cb94e6f4f14247eb7
msgid "1.Run db-gpt server"
msgstr "1.Run db-gpt server"

#: ../../getting_started/install/deploy/deploy.md:116
#: dbeecff230174132b85d1d4549d3c07e
msgid "Open http://localhost:5000 with your browser to see the product."
msgstr "打开浏览器访问http://localhost:5000"

#: ../../getting_started/install/deploy/deploy.md:119
#: 22d6321e6226472e878a95d3c8a9aad8
msgid "If you want to access an external LLM service, you need to"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:121
#: 561dfe9a864540d6ac582f0977b2c9ad
msgid ""
"1.set the variables LLM_MODEL=YOUR_MODEL_NAME, "
"MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in the .env "
"file."
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:123
#: 55ceca48e40147a99ab4d23392349156
msgid "2.execute dbgpt_server.py in light mode"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:126
#: 02d42956a2734c739ad1cb9ce59142ce
msgid ""
"If you want to learn about dbgpt-webui, read https://github./csunny/DB-"
"GPT/tree/new-page-framework/datacenter"
msgstr ""
"如果你想了解web-ui, 请访问https://github./csunny/DB-GPT/tree/new-page-"
"framework/datacenter"

#: ../../getting_started/install/deploy/deploy.md:132
#: d813eb43b97445a08e058d336249e6f6
#, fuzzy
msgid "Multiple GPUs"
msgstr "4. Multiple GPUs"

#: ../../getting_started/install/deploy/deploy.md:134
#: 0ac795f274d24de7b37f9584763e113d
msgid ""
"DB-GPT will use all available gpu by default. And you can modify the "
"setting `CUDA_VISIBLE_DEVICES=0,1` in `.env` file to use the specific gpu"
" IDs."
msgstr "DB-GPT默认加载可利用的gpu，你也可以通过修改 在`.env`文件 `CUDA_VISIBLE_DEVICES=0,1`来指定gpu IDs"

#: ../../getting_started/install/deploy/deploy.md:136
#: 2be557e2b5414d478d375bce0474558d
msgid ""
"Optionally, you can also specify the gpu ID to use before the starting "
"command, as shown below:"
msgstr "你也可以指定gpu ID启动"

#: ../../getting_started/install/deploy/deploy.md:146
#: 222f1ebb5cb64675a0c319552d14303e
msgid ""
"You can modify the setting `MAX_GPU_MEMORY=xxGib` in `.env` file to "
"configure the maximum memory used by each GPU."
msgstr "同时你可以通过在.env文件设置`MAX_GPU_MEMORY=xxGib`修改每个GPU的最大使用内存"

#: ../../getting_started/install/deploy/deploy.md:148
#: fb92349f9fe049d5b23b9ead17caf895
#, fuzzy
msgid "Not Enough Memory"
msgstr "5. Not Enough Memory"

#: ../../getting_started/install/deploy/deploy.md:150
#: 30a1105d728a474c9cd14638feab4b59
msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
msgstr "DB-GPT 支持 8-bit quantization 和 4-bit quantization."

#: ../../getting_started/install/deploy/deploy.md:152
#: eb2e576379434bfa828c98ee374149f5
msgid ""
"You can modify the setting `QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` "
"in `.env` file to use quantization(8-bit quantization is enabled by "
"default)."
msgstr "你可以通过在.env文件设置`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`"

#: ../../getting_started/install/deploy/deploy.md:154
#: eeaecfd77d8546a6afc1357f9f1684bf
msgid ""
"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit"
" quantization can run with 48 GB of VRAM."
msgstr ""
"Llama-2-70b with 8-bit quantization 可以运行在80GB VRAM机器， 4-bit "
"quantization可以运行在 48 GB  VRAM"

#~ msgid ""
#~ "Notice make sure you have install "
#~ "git-lfs centos:yum install git-lfs "
#~ "ubuntu:app-get install git-lfs "
#~ "macos:brew install git-lfs"
#~ msgstr ""
#~ "注意下载模型之前确保git-lfs已经安ubuntu:app-get install "
#~ "git-lfs macos:brew install git-lfs"

#~ msgid ""
#~ "You can refer to this document to"
#~ " obtain the Vicuna weights: "
#~ "[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md"
#~ "#model-weights) ."
#~ msgstr ""
#~ "你可以参考如何获取Vicuna weights文档[Vicuna](https://github.com/lm-"
#~ "sys/FastChat/blob/main/README.md#model-weights) ."

#~ msgid ""
#~ "If you have difficulty with this "
#~ "step, you can also directly use "
#~ "the model from [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) as "
#~ "a replacement."
#~ msgstr ""
#~ "如果觉得模型太大你也可以下载vicuna-7b [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) "

#~ msgid ""
#~ "If you want to access an external"
#~ " LLM service, you need to 1.set "
#~ "the variables LLM_MODEL=YOUR_MODEL_NAME "
#~ "MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in "
#~ "the .env file. 2.execute dbgpt_server.py "
#~ "in light mode"
#~ msgstr ""
#~ "如果你想访问外部的大模型服务(是通过DB-"
#~ "GPT/pilot/server/llmserver.py启动的模型服务)，1.需要在.env文件设置模型名和外部模型服务地址。2.使用light模式启动服务"

#~ msgid ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."
#~ msgstr ""
#~ "注意，需要安装[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)涉及的所有的依赖"
