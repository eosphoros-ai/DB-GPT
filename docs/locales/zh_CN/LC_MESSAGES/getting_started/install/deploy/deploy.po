# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT ğŸ‘ğŸ‘ 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-16 23:15+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy/deploy.md:1
#: de443fce549545518824a89604028a2e
msgid "Installation From Source"
msgstr "æºç å®‰è£…"

#: ../../getting_started/install/deploy/deploy.md:3
#: d7b1a80599004c589c9045eba98cc5c9
msgid ""
"This tutorial gives you a quick walkthrough about use DB-GPT with you "
"environment and data."
msgstr "æœ¬æ•™ç¨‹ä¸ºæ‚¨æä¾›äº†å…³äºå¦‚ä½•ä½¿ç”¨DB-GPTçš„ä½¿ç”¨æŒ‡å—ã€‚"

#: ../../getting_started/install/deploy/deploy.md:5
#: 0ba98573194c4108aedaa2669915e949
msgid "Installation"
msgstr "å®‰è£…"

#: ../../getting_started/install/deploy/deploy.md:7
#: b8f465fcee2b45009bb1c6356df06b20
msgid "To get started, install DB-GPT with the following steps."
msgstr "è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®‰è£…DB-GPT"

#: ../../getting_started/install/deploy/deploy.md:9
#: fd5031c97e304023bd6880cd10d58413
msgid "1. Hardware Requirements"
msgstr "1. ç¡¬ä»¶è¦æ±‚"

#: ../../getting_started/install/deploy/deploy.md:10
#: 05f570b3999f465982c2648f658aed82
msgid ""
"As our project has the ability to achieve ChatGPT performance of over "
"85%, there are certain hardware requirements. However, overall, the "
"project can be deployed and used on consumer-grade graphics cards. The "
"specific hardware requirements for deployment are as follows:"
msgstr "ç”±äºæˆ‘ä»¬çš„é¡¹ç›®æœ‰èƒ½åŠ›è¾¾åˆ°85%ä»¥ä¸Šçš„ChatGPTæ€§èƒ½ï¼Œæ‰€ä»¥å¯¹ç¡¬ä»¶æœ‰ä¸€å®šçš„è¦æ±‚ã€‚ä½†æ€»ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šå³å¯å®Œæˆé¡¹ç›®çš„éƒ¨ç½²ä½¿ç”¨ï¼Œå…·ä½“éƒ¨ç½²çš„ç¡¬ä»¶è¯´æ˜å¦‚ä¸‹:"

#: ../../getting_started/install/deploy/deploy.md
#: 5c5ee902c51d4e44aeeac3fa99910098
msgid "GPU"
msgstr "GPU"

#: ../../getting_started/install/deploy/deploy.md
#: a3199d1f11474451a06a11503c4e8c74 e3d7c2003b444cb886aec34aaba4acfe
msgid "VRAM Size"
msgstr "æ˜¾å­˜"

#: ../../getting_started/install/deploy/deploy.md
#: 3bd4ce6f9201483fa579d42ebf8cf556
msgid "Performance"
msgstr "Performance"

#: ../../getting_started/install/deploy/deploy.md
#: 8256a27b6a534edea5646589d65eb34e
msgid "RTX 4090"
msgstr "RTX 4090"

#: ../../getting_started/install/deploy/deploy.md
#: 25c1f69adc5d4a058dbd28ea4414c3f8 ed85dab6725b4f0baf13ff67a7032777
msgid "24 GB"
msgstr "24 GB"

#: ../../getting_started/install/deploy/deploy.md
#: f57d2a02d8344a3d9870c1c21728249d
msgid "Smooth conversation inference"
msgstr "Smooth conversation inference"

#: ../../getting_started/install/deploy/deploy.md
#: aa1e607b65964d43ad93fc9b3cff7712
msgid "RTX 3090"
msgstr "RTX 3090"

#: ../../getting_started/install/deploy/deploy.md
#: c0220f95d58543b498bdf896b2c1a2a1
msgid "Smooth conversation inference, better than V100"
msgstr "Smooth conversation inference, better than V100"

#: ../../getting_started/install/deploy/deploy.md
#: acf0daf6aa764953b43464c8d6688dd8
msgid "V100"
msgstr "V100"

#: ../../getting_started/install/deploy/deploy.md
#: 902f8c48bdad47d587acb1990b4d45b7 e53c23b23b414025be52191beb6d33da
msgid "16 GB"
msgstr "16 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 68f4b835131c4753b1ba690f3b34daea fac3351a3901481c9e0c5204d6790c75
msgid "Conversation inference possible, noticeable stutter"
msgstr "Conversation inference possible, noticeable stutter"

#: ../../getting_started/install/deploy/deploy.md
#: d4b9ff72353b4a10bff0647bf50bfe5c
msgid "T4"
msgstr "T4"

#: ../../getting_started/install/deploy/deploy.md:19
#: ddc9544667654f539ca91ac7e8af1268
msgid ""
"if your VRAM Size is not enough, DB-GPT supported 8-bit quantization and "
"4-bit quantization."
msgstr "å¦‚æœä½ çš„æ˜¾å­˜ä¸å¤Ÿï¼ŒDB-GPTæ”¯æŒ8-bitå’Œ4-bité‡åŒ–ç‰ˆæœ¬"

#: ../../getting_started/install/deploy/deploy.md:21
#: a6ec9822bc754670bbfc1a8a75e71eb2
msgid ""
"Here are some of the VRAM size usage of the models we tested in some "
"common scenarios."
msgstr "è¿™é‡Œæ˜¯é‡åŒ–ç‰ˆæœ¬çš„ç›¸å…³è¯´æ˜"

#: ../../getting_started/install/deploy/deploy.md
#: b307fe62a5564cadbf3f2d1387165c6b
msgid "Model"
msgstr "Model"

#: ../../getting_started/install/deploy/deploy.md
#: 718fb2ff4fcc488aba8963fc6ad5ea8c
msgid "Quantize"
msgstr "Quantize"

#: ../../getting_started/install/deploy/deploy.md
#: 6079a14fca3d43bfbf14021fcd1534c7 785489b458ca4578bfd586c495b5abb9
msgid "vicuna-7b-v1.5"
msgstr "vicuna-7b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 1d6a5c19584247d89fb2eb98bcaecc83 278d03ee54e749e1b5f20204ddc36149
#: 69c01cb441894f059e91400502cd33ae 7fa7d4922bfb4b3bb44b98ea02ff7e78
#: b8b4566e3a994919b9821cd536504936 d6f4afc865cb40b085b5fc79a09bc7f9
#: ef05aa05a2d2411a91449ccc18a76211
msgid "4-bit"
msgstr "4-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 1266b6e1dde64dab9e6d8bba2f3f6d09 8ab98ed2c80c48ab9e9694131ffcac67
#: b94deb7b80c24ce8a694984511e5a02a
msgid "8 GB"
msgstr "8 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 065f1cf1a1b94ad5803f95f8f019d882 0689708416e14942a76c2808a26bc26e
#: 29dc55e7659a4d6a999a347c346e1327 5f0fa6c729db4cd7ab42dbdc73ca4e40
#: 6401e59dc85541a0b20cb2d2c26e4fd0 9071acd973b24d5582f8d879d5e55931
#: 96f12483ac7447baab6592538cfd567c
msgid "8-bit"
msgstr "8-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 2d56e3dc1f6a4035a770f7b94c8e0f96 5eebdf37bc544624be5d1b6dabda4716
#: b9fd2505b4644257b91777bc68d5f41e e7056c195656413f92a0c78b5d14219c
#: e7b87586700e4da0aaccff0b4c7c54f7 eb5ad729ae784c7cb8dd52fbb12699ae
msgid "12 GB"
msgstr "12 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 529ead731c98461b8cb5452c4e72ab23 7cce32961a654ed2a31edc82724e6a1f
msgid "vicuna-13b-v1.5"
msgstr "vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 0085b850f3574ba6bf3b3654123882dd 69b2df6df91c49b2b26f6749bf6dc657
#: 714e9441566e4c8bbdeaad944e64c699
msgid "20 GB"
msgstr "20 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 133b65fb88f74645ae5db5cd0009bb35 1e7dedf510e94a47b23eaef61f9687b1
msgid "llama-2-7b"
msgstr "llama-2-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 0951d03bb6544a2391dcd72eea47c1a7 89f93c8aadc84a0d97d3d89ee55d06bf
msgid "llama-2-13b"
msgstr "llama-2-13b"

#: ../../getting_started/install/deploy/deploy.md
#: 6e5a32858b20441daa4b2584faa46ec4 8bcd62d8cf4f49aebb7d97cd9e015252
msgid "llama-2-70b"
msgstr "llama-2-70b"

#: ../../getting_started/install/deploy/deploy.md
#: 7f7333221b014cc6857fd9a9e358d85c
msgid "48 GB"
msgstr "48 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 77c24e304e9e4de7b62f99ce29a66a70
msgid "80 GB"
msgstr "80 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 32c04dc45efb45bcb516640a6d15cce1 e04ad78be6774c32bc53ddd7951cedae
msgid "baichuan-7b"
msgstr "baichuan-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 0fe379939b164e56b0d93113e85fbd98 3400143cf1b94edfbf5da63ed388b08c
msgid "baichuan-13b"
msgstr "baichuan-13b"

#: ../../getting_started/install/deploy/deploy.md:40
#: 7a05f116e0904d0d84d9fc98e5465494
msgid "2. Install"
msgstr "2. Install"

#: ../../getting_started/install/deploy/deploy.md:45
#: 8f4d6c2b69cb46288f593b6c2aa7701e
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration.  For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. [How to install "
"Miniconda](https://docs.conda.io/en/latest/miniconda.html)"
msgstr ""
"ç›®å‰ä½¿ç”¨Sqliteä½œä¸ºé»˜è®¤æ•°æ®åº“ï¼Œå› æ­¤DB-"
"GPTå¿«é€Ÿéƒ¨ç½²ä¸éœ€è¦éƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–æ•°æ®åº“ï¼Œéœ€è¦å…ˆéƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚æˆ‘ä»¬ç›®å‰ä½¿ç”¨Minicondaè¿›è¡Œpythonç¯å¢ƒå’ŒåŒ…ä¾èµ–ç®¡ç†[å®‰è£…"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy/deploy.md:54
#: 3ffaf7fed0c8422b9ceb2ab82d6ddd4d
msgid "Before use DB-GPT Knowledge"
msgstr "åœ¨ä½¿ç”¨çŸ¥è¯†åº“ä¹‹å‰"

#: ../../getting_started/install/deploy/deploy.md:60
#: 2c2ef86e379d4db18bdfdba6133a0b2f
msgid ""
"Once the environment is installed, we have to create a new folder "
"\"models\" in the DB-GPT project, and then we can put all the models "
"downloaded from huggingface in this directory"
msgstr "å¦‚æœä½ å·²ç»å®‰è£…å¥½äº†ç¯å¢ƒéœ€è¦åˆ›å»ºmodels, ç„¶ååˆ°huggingfaceå®˜ç½‘ä¸‹è½½æ¨¡å‹"

#: ../../getting_started/install/deploy/deploy.md:63
#: 73a766538b3d4cfaa8d7a68b3c9915b8
msgid ""
"Notice make sure you have install git-lfs centos:yum install git-lfs "
"ubuntu:app-get install git-lfs macos:brew install git-lfs"
msgstr ""
"æ³¨æ„ä¸‹è½½æ¨¡å‹ä¹‹å‰ç¡®ä¿git-lfså·²ç»å®‰ubuntu:app-get install git-lfs macos:brew install "
"git-lfs"

#: ../../getting_started/install/deploy/deploy.md:83
#: 3c26909ece094ecb9f6343d15cca394a
msgid ""
"The model files are large and will take a long time to download. During "
"the download, let's configure the .env file, which needs to be copied and"
" created from the .env.template"
msgstr "æ¨¡å‹æ–‡ä»¶å¾ˆå¤§ï¼Œéœ€è¦å¾ˆé•¿æ—¶é—´æ‰èƒ½ä¸‹è½½ã€‚åœ¨ä¸‹è½½è¿‡ç¨‹ä¸­ï¼Œè®©æˆ‘ä»¬é…ç½®.envæ–‡ä»¶ï¼Œå®ƒéœ€è¦ä»ã€‚env.templateä¸­å¤åˆ¶å’Œåˆ›å»ºã€‚"

#: ../../getting_started/install/deploy/deploy.md:85
#: efab7120927d4b3f90e591d736b927a3
msgid ""
"if you want to use openai llm service, see [LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"
msgstr "å¦‚æœæƒ³ä½¿ç”¨openaiå¤§æ¨¡å‹æœåŠ¡, å¯ä»¥å‚è€ƒ[LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"

#: ../../getting_started/install/deploy/deploy.md:88
#: 2009fcaad7c34ebfaa900215650256fc
msgid "cp .env.template .env"
msgstr "cp .env.template .env"

#: ../../getting_started/install/deploy/deploy.md:91
#: ee97ddf25daf45e3bc32b33693af447a
msgid ""
"You can configure basic parameters in the .env file, for example setting "
"LLM_MODEL to the model to be used"
msgstr "æ‚¨å¯ä»¥åœ¨.envæ–‡ä»¶ä¸­é…ç½®åŸºæœ¬å‚æ•°ï¼Œä¾‹å¦‚å°†LLM_MODELè®¾ç½®ä¸ºè¦ä½¿ç”¨çš„æ¨¡å‹ã€‚"

#: ../../getting_started/install/deploy/deploy.md:93
#: a86fd88e1d0f4925b8d0dbc27535663b
msgid ""
"([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"æ‚¨å¯ä»¥åœ¨.envæ–‡ä»¶ä¸­é…ç½®åŸºæœ¬å‚æ•°ï¼Œä¾‹å¦‚å°†LLM_MODELè®¾ç½®ä¸ºè¦ä½¿ç”¨çš„æ¨¡å‹ã€‚([Vicuna-v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5)ï¼Œ "
"ç›®å‰Vicuna-v1.5æ¨¡å‹(åŸºäºllama2)å·²ç»å¼€æºäº†ï¼Œæˆ‘ä»¬æ¨èä½ ä½¿ç”¨è¿™ä¸ªæ¨¡å‹é€šè¿‡è®¾ç½®LLM_MODEL=vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md:95
#: 5395445ea6324e7c9e15485fad084937
msgid "3. Run"
msgstr "3. Run"

#: ../../getting_started/install/deploy/deploy.md:96
#: cbbc83183f0d49bdb16a3df18adbe8b2
msgid ""
"You can refer to this document to obtain the Vicuna weights: "
"[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md#model-"
"weights) ."
msgstr "ä½ å¯ä»¥å‚è€ƒå¦‚ä½•è·å–Vicuna weightsæ–‡æ¡£[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md#model-"
"weights) ."

#: ../../getting_started/install/deploy/deploy.md:98
#: e0ffb578c7894520bbb850b257e7773c
msgid ""
"If you have difficulty with this step, you can also directly use the "
"model from [this link](https://huggingface.co/Tribbiani/vicuna-7b) as a "
"replacement."
msgstr "å¦‚æœè§‰å¾—æ¨¡å‹å¤ªå¤§ä½ ä¹Ÿå¯ä»¥ä¸‹è½½vicuna-7b [this link](https://huggingface.co/Tribbiani/vicuna-7b) "


#: ../../getting_started/install/deploy/deploy.md:103
#: 590c7c07cf5347b4aeee0809185c7f45
msgid "1.Run db-gpt server"
msgstr "1.Run db-gpt server"

#: ../../getting_started/install/deploy/deploy.md:108
#: cc1f6d2e37464a4291ee7d33d9ebd75f
msgid "Open http://localhost:5000 with your browser to see the product."
msgstr "æ‰“å¼€æµè§ˆå™¨è®¿é—®http://localhost:5000"

#: ../../getting_started/install/deploy/deploy.md:110
#: 7eef6b17573e4300aa6b693200461f58
msgid ""
"If you want to access an external LLM service, you need to 1.set the "
"variables LLM_MODEL=YOUR_MODEL_NAME "
"MODEL_SERVER=YOUR_MODEL_SERVERï¼ˆeg:http://localhost:5000ï¼‰ in the .env "
"file. 2.execute dbgpt_server.py in light mode"
msgstr ""
"å¦‚æœä½ æƒ³è®¿é—®å¤–éƒ¨çš„å¤§æ¨¡å‹æœåŠ¡(æ˜¯é€šè¿‡DB-"
"GPT/pilot/server/llmserver.pyå¯åŠ¨çš„æ¨¡å‹æœåŠ¡)ï¼Œ1.éœ€è¦åœ¨.envæ–‡ä»¶è®¾ç½®æ¨¡å‹åå’Œå¤–éƒ¨æ¨¡å‹æœåŠ¡åœ°å€ã€‚2.ä½¿ç”¨lightæ¨¡å¼å¯åŠ¨æœåŠ¡"

#: ../../getting_started/install/deploy/deploy.md:113
#: 2fa89081574d4d3a92a4c7d33b090d02
msgid ""
"If you want to learn about dbgpt-webui, read https://github./csunny/DB-"
"GPT/tree/new-page-framework/datacenter"
msgstr ""
"å¦‚æœä½ æƒ³äº†è§£web-ui, è¯·è®¿é—®https://github./csunny/DB-GPT/tree/new-page-"
"framework/datacenter"

#: ../../getting_started/install/deploy/deploy.md:120
#: 3b825bc956a0406fb8464e51cfeb769e
msgid "4. Multiple GPUs"
msgstr "4. Multiple GPUs"

#: ../../getting_started/install/deploy/deploy.md:122
#: 568ea5e67ad745858870e66c42ba6833
msgid ""
"DB-GPT will use all available gpu by default. And you can modify the "
"setting `CUDA_VISIBLE_DEVICES=0,1` in `.env` file to use the specific gpu"
" IDs."
msgstr "DB-GPTé»˜è®¤åŠ è½½å¯åˆ©ç”¨çš„gpuï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡ä¿®æ”¹ åœ¨`.env`æ–‡ä»¶ `CUDA_VISIBLE_DEVICES=0,1`æ¥æŒ‡å®šgpu IDs"

#: ../../getting_started/install/deploy/deploy.md:124
#: c5b980733d7a4c8d997123ff5524a055
msgid ""
"Optionally, you can also specify the gpu ID to use before the starting "
"command, as shown below:"
msgstr "ä½ ä¹Ÿå¯ä»¥æŒ‡å®šgpu IDå¯åŠ¨"

#: ../../getting_started/install/deploy/deploy.md:134
#: 2a5d283a614644d1bb98bbe721aee8e1
msgid ""
"You can modify the setting `MAX_GPU_MEMORY=xxGib` in `.env` file to "
"configure the maximum memory used by each GPU."
msgstr "åŒæ—¶ä½ å¯ä»¥é€šè¿‡åœ¨.envæ–‡ä»¶è®¾ç½®`MAX_GPU_MEMORY=xxGib`ä¿®æ”¹æ¯ä¸ªGPUçš„æœ€å¤§ä½¿ç”¨å†…å­˜"

#: ../../getting_started/install/deploy/deploy.md:136
#: c29c956d3071455bb11694df721e6612
msgid "5. Not Enough Memory"
msgstr "5. Not Enough Memory"

#: ../../getting_started/install/deploy/deploy.md:138
#: 0174e92fdbfa4af08063c89f6bbe3957
msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
msgstr "DB-GPT æ”¯æŒ 8-bit quantization å’Œ 4-bit quantization."

#: ../../getting_started/install/deploy/deploy.md:140
#: 277f67fa08a541b3bd1fe77cdab39757
msgid ""
"You can modify the setting `QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` "
"in `.env` file to use quantization(8-bit quantization is enabled by "
"default)."
msgstr "ä½ å¯ä»¥é€šè¿‡åœ¨.envæ–‡ä»¶è®¾ç½®`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`"

#: ../../getting_started/install/deploy/deploy.md:142
#: 00884fdf7c9a4f8c983ee52bfbb820aa
msgid ""
"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit"
" quantization can run with 48 GB of VRAM."
msgstr ""
"Llama-2-70b with 8-bit quantization å¯ä»¥è¿è¡Œåœ¨ 80 GB VRAMæœºå™¨ï¼Œ 4-bit "
"quantization å¯ä»¥è¿è¡Œåœ¨ 48 GB  VRAM"

#: ../../getting_started/install/deploy/deploy.md:144
#: a73698444bb4426ca779cc126497a2e0
msgid ""
"Note: you need to install the latest dependencies according to "
"[requirements.txt](https://github.com/eosphoros-ai/DB-"
"GPT/blob/main/requirements.txt)."
msgstr ""
"æ³¨æ„ï¼Œéœ€è¦å®‰è£…[requirements.txt](https://github.com/eosphoros-ai/DB-"
"GPT/blob/main/requirements.txt)æ¶‰åŠçš„æ‰€æœ‰çš„ä¾èµ–"

