# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT ğŸ‘ğŸ‘ 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-20 22:29+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy/deploy.md:1
#: 7bcf028ff0884ea88f25b7e2c9608153
msgid "Installation From Source"
msgstr "æºç å®‰è£…"

#: ../../getting_started/install/deploy/deploy.md:3
#: 61f0b1135c84423bbaeb5f9f0942ad7d
msgid ""
"This tutorial gives you a quick walkthrough about use DB-GPT with you "
"environment and data."
msgstr "æœ¬æ•™ç¨‹ä¸ºæ‚¨æä¾›äº†å…³äºå¦‚ä½•ä½¿ç”¨DB-GPTçš„ä½¿ç”¨æŒ‡å—ã€‚"

#: ../../getting_started/install/deploy/deploy.md:5
#: d7622cd5f69f4a32b3c8e979c6b9f601
msgid "Installation"
msgstr "å®‰è£…"

#: ../../getting_started/install/deploy/deploy.md:7
#: 4368072b6384496ebeaff3c09ca2f888
msgid "To get started, install DB-GPT with the following steps."
msgstr "è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®‰è£…DB-GPT"

#: ../../getting_started/install/deploy/deploy.md:9
#: 0dfdf8ac6e314fe7b624a685d9beebd5
msgid "1. Hardware Requirements"
msgstr "1. ç¡¬ä»¶è¦æ±‚"

#: ../../getting_started/install/deploy/deploy.md:10
#: cff920f8732f4f1da3063ec2bc099271
msgid ""
"DB-GPT can be deployed on servers with low hardware requirements or on "
"servers with high hardware requirements."
msgstr "DB-GPTå¯ä»¥éƒ¨ç½²åœ¨å¯¹ç¡¬ä»¶è¦æ±‚ä¸é«˜çš„æœåŠ¡å™¨ï¼Œä¹Ÿå¯ä»¥éƒ¨ç½²åœ¨å¯¹ç¡¬ä»¶è¦æ±‚é«˜çš„æœåŠ¡å™¨"

#: ../../getting_started/install/deploy/deploy.md:12
#: 8e3818824d6146c6b265731c277fbd0b
#, fuzzy
msgid "Low hardware requirements"
msgstr "1. ç¡¬ä»¶è¦æ±‚"

#: ../../getting_started/install/deploy/deploy.md:13
#: ca95d66526994173ac1fea20bdea5d67
msgid ""
"The low hardware requirements mode is suitable for integrating with "
"third-party LLM services' APIs, such as OpenAI, Tongyi, Wenxin, or "
"Llama.cpp."
msgstr "Low hardware requirementsæ¨¡å¼é€‚ç”¨äºå¯¹æ¥ç¬¬ä¸‰æ–¹æ¨¡å‹æœåŠ¡çš„api,æ¯”å¦‚OpenAI, é€šä¹‰åƒé—®, æ–‡å¿ƒ.cppã€‚"

#: ../../getting_started/install/deploy/deploy.md:15
#: 83fc53cc1b4248139f69f490b859ad8d
msgid "DB-GPT provides set proxy api to support LLM api."
msgstr "DB-GPTå¯ä»¥é€šè¿‡è®¾ç½®proxy apiæ¥æ”¯æŒç¬¬ä¸‰æ–¹å¤§æ¨¡å‹æœåŠ¡"

#: ../../getting_started/install/deploy/deploy.md:17
#: 418a9f24eafc4571b74d86c3f1e57a2d
msgid "As our project has the ability to achieve ChatGPT performance of over 85%,"
msgstr "ç”±äºæˆ‘ä»¬çš„é¡¹ç›®æœ‰èƒ½åŠ›è¾¾åˆ°85%ä»¥ä¸Šçš„ChatGPTæ€§èƒ½"

#: ../../getting_started/install/deploy/deploy.md:19
#: 6f85149ab0024cc99e43804206a595ed
#, fuzzy
msgid "High hardware requirements"
msgstr "1. ç¡¬ä»¶è¦æ±‚"

#: ../../getting_started/install/deploy/deploy.md:20
#: 31635ffff5084814a14deb3220dd2c17
#, fuzzy
msgid ""
"The high hardware requirements mode is suitable for independently "
"deploying LLM services, such as Llama series models, Baichuan, ChatGLM, "
"Vicuna, and other private LLM service. there are certain hardware "
"requirements. However, overall, the project can be deployed and used on "
"consumer-grade graphics cards. The specific hardware requirements for "
"deployment are as follows:"
msgstr ""
"High hardware requirementsæ¨¡å¼é€‚ç”¨äºéœ€è¦ç‹¬ç«‹éƒ¨ç½²ç§æœ‰å¤§æ¨¡å‹æœåŠ¡ï¼Œæ¯”å¦‚Llamaç³»åˆ—æ¨¡å‹ï¼ŒBaichuan, "
"chatglmï¼Œvicunaç­‰ç§æœ‰å¤§æ¨¡å‹æ‰€ä»¥å¯¹ç¡¬ä»¶æœ‰ä¸€å®šçš„è¦æ±‚ã€‚ä½†æ€»ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šå³å¯å®Œæˆé¡¹ç›®çš„éƒ¨ç½²ä½¿ç”¨ï¼Œå…·ä½“éƒ¨ç½²çš„ç¡¬ä»¶è¯´æ˜å¦‚ä¸‹:"

#: ../../getting_started/install/deploy/deploy.md
#: d806b90be1614ad3b2e06c92f4b17e5c
msgid "GPU"
msgstr "GPU"

#: ../../getting_started/install/deploy/deploy.md
#: 4b02f41145484389ace0b547384ac269 bbba2ff3fab94482a1761264264deef9
msgid "VRAM Size"
msgstr "æ˜¾å­˜"

#: ../../getting_started/install/deploy/deploy.md
#: 0ea63c2dcc0e43858a61e01d59ad09f9
msgid "Performance"
msgstr "Performance"

#: ../../getting_started/install/deploy/deploy.md
#: 6521683eb91e450c928a72688550a63d
msgid "RTX 4090"
msgstr "RTX 4090"

#: ../../getting_started/install/deploy/deploy.md
#: bb6340c9cdc048fbb0ed55defc1aaeb6 d991b39845ee404198e1a1e35cc416f3
msgid "24 GB"
msgstr "24 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 4134d3a89d364e33b2bdf1c7667e4755
msgid "Smooth conversation inference"
msgstr "ä¸æ»‘çš„å¯¹è¯ä½“éªŒ"

#: ../../getting_started/install/deploy/deploy.md
#: 096ff425ac7646a990a7133961c6e6af
msgid "RTX 3090"
msgstr "RTX 3090"

#: ../../getting_started/install/deploy/deploy.md
#: ecf670cdbec3493f804e6a785a83c608
msgid "Smooth conversation inference, better than V100"
msgstr "ä¸æ»‘çš„å¯¹è¯ä½“éªŒï¼Œæ€§èƒ½å¥½äºV100"

#: ../../getting_started/install/deploy/deploy.md
#: 837b14e0a3d243bda0df7ab35b70b7e7
msgid "V100"
msgstr "V100"

#: ../../getting_started/install/deploy/deploy.md
#: 3b20a087c8e342c89ccb807ffc3817c2 b8b6b45253084436a5893896b35a2bd5
msgid "16 GB"
msgstr "16 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 772e18bb0ace4f7ea68b51bfc05816ce 9351389a1fac479cbe67b1f8c2c37de5
msgid "Conversation inference possible, noticeable stutter"
msgstr "Conversation inference possible, noticeable stutter"

#: ../../getting_started/install/deploy/deploy.md
#: aadb62bf48bb49d99a714bcdf3092260
msgid "T4"
msgstr "T4"

#: ../../getting_started/install/deploy/deploy.md:30
#: 4de80d9fcf34470bae806d829836b7d7
#, fuzzy
msgid ""
"If your VRAM Size is not enough, DB-GPT supported 8-bit quantization and "
"4-bit quantization."
msgstr "å¦‚æœä½ çš„æ˜¾å­˜ä¸å¤Ÿï¼ŒDB-GPTæ”¯æŒ8-bitå’Œ4-bité‡åŒ–ç‰ˆæœ¬"

#: ../../getting_started/install/deploy/deploy.md:32
#: 00d81cbf48b549f3b9128d3840d01b2e
msgid ""
"Here are some of the VRAM size usage of the models we tested in some "
"common scenarios."
msgstr "è¿™é‡Œæ˜¯é‡åŒ–ç‰ˆæœ¬çš„ç›¸å…³è¯´æ˜"

#: ../../getting_started/install/deploy/deploy.md
#: dc346f2bca794bb7ae34b330e82ccbcf
msgid "Model"
msgstr "Model"

#: ../../getting_started/install/deploy/deploy.md
#: 8de6cd40de78460ba774650466f8df26
msgid "Quantize"
msgstr "Quantize"

#: ../../getting_started/install/deploy/deploy.md
#: 3e412b8f4852482ab07a0f546e37ae7f f30054e0558b41a192cc9a2462b299ec
msgid "vicuna-7b-v1.5"
msgstr "vicuna-7b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 14358fa40cf94614acf39a803987631f 2a3f52b26b444783be04ffa795246a03
#: 3956734b19aa44c3be08d56348b47a38 751034ca7d00447895fda1d9b8a7364f
#: a66d16e5424a42a3a1309dfb8ffc33f9 b8ebce0a9e7e481da5f16214f955665d
#: f533b3f37e6f4594aec5e0f59f241683
msgid "4-bit"
msgstr "4-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 9eac7e866ebe45169c64a952c363ce43 aa56722db3014abd9022067ed5fc4f98
#: af4df898fb47471fbb487fcf6e2d40d6
msgid "8 GB"
msgstr "8 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 211aaf2e234d46108b5eee5006d5f4bb 40214b2f71ce452db3501ea9d81a0c8a
#: 72fcd5e0634e48d79813f1037e6acb45 7756b67568cc40c4b73079b26e79c85d
#: 8c21f8e90154407682c093a46b93939d ad937c14bbcd41ac92a3dbbdb8339eed
#: d1e7ee217dd64b15b934456c3a72c450
msgid "8-bit"
msgstr "8-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 4812504dfb9a4b25a5db773d9a08f34f 76ae2407ba4e4013953b9f243d9a5d92
#: 927054919de047fd8a83df67e1400622 9773e73eb89847f8a85a2dc55b562916
#: ce33d0c3792f43398fc7e2694653d8fc d3dc0d4cceb24d2b9dc5c7120fbed94e
msgid "12 GB"
msgstr "12 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 83e6d6ba1aa74946858f0162424752ab b6b99caeaeff44c488e3e819ed337074
msgid "vicuna-13b-v1.5"
msgstr "vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 492c5f0d560946fe879f6c339975ba37 970063dda21e4dd8be6f89a3c87832a5
#: a66bad6054b24dd99b370312bc8b6fa6
msgid "20 GB"
msgstr "20 GB"

#: ../../getting_started/install/deploy/deploy.md
#: a75f3405085441d8920db49f159588d2 cf635931c55846aea4cbccd92e4f0377
msgid "llama-2-7b"
msgstr "llama-2-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 61d632df8c5149b393d03ac802141125 bc98c895d457495ea26e3537de83b432
msgid "llama-2-13b"
msgstr "llama-2-13b"

#: ../../getting_started/install/deploy/deploy.md
#: 3ccb1f6d8a924aeeacb5373edc168103 9ecce68e159a4649a8d5e69157af17a1
msgid "llama-2-70b"
msgstr "llama-2-70b"

#: ../../getting_started/install/deploy/deploy.md
#: ca1da6ce08674b3daa0ab9ee0330203f
msgid "48 GB"
msgstr "48 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 34d4d20e57c1410fbdcabd09a5968cdd
msgid "80 GB"
msgstr "80 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 4ec2213171054c96ac9cd46e259ce7bf 68a1752f76a54287a73e82724723ea75
msgid "baichuan-7b"
msgstr "baichuan-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 103b020a575744ad964c60a367aa1651 c659a720a1024869b09d7cc161bcd8a2
msgid "baichuan-13b"
msgstr "baichuan-13b"

#: ../../getting_started/install/deploy/deploy.md:51
#: 2259a008d0e14f9e8d1e1d9234b97298
msgid "2. Install"
msgstr "2. Install"

#: ../../getting_started/install/deploy/deploy.md:56
#: 875c7d8e32574552a48199577c78ccdd
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration.  For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. [How to install "
"Miniconda](https://docs.conda.io/en/latest/miniconda.html)"
msgstr ""
"ç›®å‰ä½¿ç”¨Sqliteä½œä¸ºé»˜è®¤æ•°æ®åº“ï¼Œå› æ­¤DB-"
"GPTå¿«é€Ÿéƒ¨ç½²ä¸éœ€è¦éƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–æ•°æ®åº“ï¼Œéœ€è¦å…ˆéƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚æˆ‘ä»¬ç›®å‰ä½¿ç”¨Minicondaè¿›è¡Œpythonç¯å¢ƒå’ŒåŒ…ä¾èµ–ç®¡ç†[å®‰è£…"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy/deploy.md:67
#: c03e3290e1144320a138d015171ac596
msgid ""
"Once the environment is installed, we have to create a new folder "
"\"models\" in the DB-GPT project, and then we can put all the models "
"downloaded from huggingface in this directory"
msgstr "å¦‚æœä½ å·²ç»å®‰è£…å¥½äº†ç¯å¢ƒéœ€è¦åˆ›å»ºmodels, ç„¶ååˆ°huggingfaceå®˜ç½‘ä¸‹è½½æ¨¡å‹"

#: ../../getting_started/install/deploy/deploy.md:70
#: 933401ac909741ada4acf6bcd4142ed6
msgid "Notice make sure you have install git-lfs"
msgstr "æ³¨æ„ç¡®è®¤ä½ å·²ç»å®‰è£…äº†git-lfs"

#: ../../getting_started/install/deploy/deploy.md:72
#: e8e4886a83dd402c85fe3fa989322991
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:74
#: 5ead7e98bddf4fa4845c3d3955f18054
msgid "ubuntu:apt-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:76
#: 08acfaaaa2544182a59df54cdf61cd84
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:78
#: 312ad44170c34531865576067c58701a
msgid "Download LLM Model and Embedding Model"
msgstr "ä¸‹è½½LLMæ¨¡å‹å’ŒEmbeddingæ¨¡å‹"

#: ../../getting_started/install/deploy/deploy.md:80
#: de54793643434528a417011d2919b2c4
#, fuzzy
msgid ""
"If you use OpenAI llm service, see [LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"
msgstr ""
"å¦‚æœæƒ³ä½¿ç”¨openaiå¤§æ¨¡å‹æœåŠ¡, å¯ä»¥å‚è€ƒ[LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"

#: ../../getting_started/install/deploy/deploy.md:83
#: 50ec1eb7c56a46ac8fbf911c7adc9b0e
msgid ""
"If you use openai or Azure or tongyi llm api service, you don't need to "
"download llm model."
msgstr "å¦‚æœä½ æƒ³é€šè¿‡openai or Azure or tongyiç¬¬ä¸‰æ–¹apiè®¿é—®æ¨¡å‹æœåŠ¡ï¼Œä½ å¯ä»¥ä¸ç”¨ä¸‹è½½llmæ¨¡å‹"

#: ../../getting_started/install/deploy/deploy.md:103
#: 03950b2a480149388fb7b88f7d251ef5
msgid ""
"The model files are large and will take a long time to download. During "
"the download, let's configure the .env file, which needs to be copied and"
" created from the .env.template"
msgstr "æ¨¡å‹æ–‡ä»¶å¾ˆå¤§ï¼Œéœ€è¦å¾ˆé•¿æ—¶é—´æ‰èƒ½ä¸‹è½½ã€‚åœ¨ä¸‹è½½è¿‡ç¨‹ä¸­ï¼Œè®©æˆ‘ä»¬é…ç½®.envæ–‡ä»¶ï¼Œå®ƒéœ€è¦ä»ã€‚env.templateä¸­å¤åˆ¶å’Œåˆ›å»ºã€‚"

#: ../../getting_started/install/deploy/deploy.md:106
#: 441c4333216a402a84fd52f8e56fc81b
msgid "cp .env.template .env"
msgstr "cp .env.template .env"

#: ../../getting_started/install/deploy/deploy.md:109
#: 4eac3d98df6a4e788234ff0ec1ffd03e
msgid ""
"You can configure basic parameters in the .env file, for example setting "
"LLM_MODEL to the model to be used"
msgstr "æ‚¨å¯ä»¥åœ¨.envæ–‡ä»¶ä¸­é…ç½®åŸºæœ¬å‚æ•°ï¼Œä¾‹å¦‚å°†LLM_MODELè®¾ç½®ä¸ºè¦ä½¿ç”¨çš„æ¨¡å‹ã€‚"

#: ../../getting_started/install/deploy/deploy.md:111
#: a36bd6d6236b4c74b161a935ae792b91
msgid ""
"([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"æ‚¨å¯ä»¥åœ¨.envæ–‡ä»¶ä¸­é…ç½®åŸºæœ¬å‚æ•°ï¼Œä¾‹å¦‚å°†LLM_MODELè®¾ç½®ä¸ºè¦ä½¿ç”¨çš„æ¨¡å‹ã€‚([Vicuna-v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5)ï¼Œ "
"ç›®å‰Vicuna-v1.5æ¨¡å‹(åŸºäºllama2)å·²ç»å¼€æºäº†ï¼Œæˆ‘ä»¬æ¨èä½ ä½¿ç”¨è¿™ä¸ªæ¨¡å‹é€šè¿‡è®¾ç½®LLM_MODEL=vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md:113
#: 78334cbf0c364eb3bc41a2a6c55ebb0d
msgid "3. Run"
msgstr "3. Run"

#: ../../getting_started/install/deploy/deploy.md:115
#: 6d5ad6eb067d4e9fa1c574b7b706233f
msgid "**(Optional) load examples into SQLite**"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:120
#: 07219a4ed3c44e349314ae04ebdf58e1
msgid "On windows platform:"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:125
#: 819be2bb22044440ae00c2e7687ea249
#, fuzzy
msgid "Run db-gpt server"
msgstr "1.Run db-gpt server"

#: ../../getting_started/install/deploy/deploy.md:131
#: 5ba6d7c9bf9146c797036ab4b9b4f59e
msgid "Open http://localhost:5000 with your browser to see the product."
msgstr "æ‰“å¼€æµè§ˆå™¨è®¿é—®http://localhost:5000"

#: ../../getting_started/install/deploy/deploy.md:134
#: be3a2729ef3b4742a403017b31bda7e3
#, fuzzy
msgid "Multiple GPUs"
msgstr "4. Multiple GPUs"

#: ../../getting_started/install/deploy/deploy.md:136
#: 00ffa1cc145e4afa830c592a629246f9
msgid ""
"DB-GPT will use all available gpu by default. And you can modify the "
"setting `CUDA_VISIBLE_DEVICES=0,1` in `.env` file to use the specific gpu"
" IDs."
msgstr "DB-GPTé»˜è®¤åŠ è½½å¯åˆ©ç”¨çš„gpuï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡ä¿®æ”¹ åœ¨`.env`æ–‡ä»¶ `CUDA_VISIBLE_DEVICES=0,1`æ¥æŒ‡å®šgpu IDs"

#: ../../getting_started/install/deploy/deploy.md:138
#: bde32a5a8fea4350868be579e9ee6baa
msgid ""
"Optionally, you can also specify the gpu ID to use before the starting "
"command, as shown below:"
msgstr "ä½ ä¹Ÿå¯ä»¥æŒ‡å®šgpu IDå¯åŠ¨"

#: ../../getting_started/install/deploy/deploy.md:148
#: 791ed2db2cff44c48342a7828cbd4c45
msgid ""
"You can modify the setting `MAX_GPU_MEMORY=xxGib` in `.env` file to "
"configure the maximum memory used by each GPU."
msgstr "åŒæ—¶ä½ å¯ä»¥é€šè¿‡åœ¨.envæ–‡ä»¶è®¾ç½®`MAX_GPU_MEMORY=xxGib`ä¿®æ”¹æ¯ä¸ªGPUçš„æœ€å¤§ä½¿ç”¨å†…å­˜"

#: ../../getting_started/install/deploy/deploy.md:150
#: f86b37c8943e4f5595610706e75b4add
#, fuzzy
msgid "Not Enough Memory"
msgstr "5. Not Enough Memory"

#: ../../getting_started/install/deploy/deploy.md:152
#: 8a7bd02cbeca497aa8eecaaf1910a6ad
msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
msgstr "DB-GPT æ”¯æŒ 8-bit quantization å’Œ 4-bit quantization."

#: ../../getting_started/install/deploy/deploy.md:154
#: 5ad49b99fe774ba79c50de0cd694807c
msgid ""
"You can modify the setting `QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` "
"in `.env` file to use quantization(8-bit quantization is enabled by "
"default)."
msgstr "ä½ å¯ä»¥é€šè¿‡åœ¨.envæ–‡ä»¶è®¾ç½®`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`"

#: ../../getting_started/install/deploy/deploy.md:156
#: b9c80e92137447da91eb944443144c69
msgid ""
"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit"
" quantization can run with 48 GB of VRAM."
msgstr ""
"Llama-2-70b with 8-bit quantization å¯ä»¥è¿è¡Œåœ¨80GB VRAMæœºå™¨ï¼Œ 4-bit "
"quantizationå¯ä»¥è¿è¡Œåœ¨ 48 GB  VRAM"

#~ msgid ""
#~ "Notice make sure you have install "
#~ "git-lfs centos:yum install git-lfs "
#~ "ubuntu:app-get install git-lfs "
#~ "macos:brew install git-lfs"
#~ msgstr ""
#~ "æ³¨æ„ä¸‹è½½æ¨¡å‹ä¹‹å‰ç¡®ä¿git-lfså·²ç»å®‰ubuntu:app-get install "
#~ "git-lfs macos:brew install git-lfs"

#~ msgid ""
#~ "You can refer to this document to"
#~ " obtain the Vicuna weights: "
#~ "[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md"
#~ "#model-weights) ."
#~ msgstr ""
#~ "ä½ å¯ä»¥å‚è€ƒå¦‚ä½•è·å–Vicuna weightsæ–‡æ¡£[Vicuna](https://github.com/lm-"
#~ "sys/FastChat/blob/main/README.md#model-weights) ."

#~ msgid ""
#~ "If you have difficulty with this "
#~ "step, you can also directly use "
#~ "the model from [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) as "
#~ "a replacement."
#~ msgstr ""
#~ "å¦‚æœè§‰å¾—æ¨¡å‹å¤ªå¤§ä½ ä¹Ÿå¯ä»¥ä¸‹è½½vicuna-7b [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) "

#~ msgid ""
#~ "If you want to access an external"
#~ " LLM service, you need to 1.set "
#~ "the variables LLM_MODEL=YOUR_MODEL_NAME "
#~ "MODEL_SERVER=YOUR_MODEL_SERVERï¼ˆeg:http://localhost:5000ï¼‰ in "
#~ "the .env file. 2.execute dbgpt_server.py "
#~ "in light mode"
#~ msgstr ""
#~ "å¦‚æœä½ æƒ³è®¿é—®å¤–éƒ¨çš„å¤§æ¨¡å‹æœåŠ¡(æ˜¯é€šè¿‡DB-"
#~ "GPT/pilot/server/llmserver.pyå¯åŠ¨çš„æ¨¡å‹æœåŠ¡)ï¼Œ1.éœ€è¦åœ¨.envæ–‡ä»¶è®¾ç½®æ¨¡å‹åå’Œå¤–éƒ¨æ¨¡å‹æœåŠ¡åœ°å€ã€‚2.ä½¿ç”¨lightæ¨¡å¼å¯åŠ¨æœåŠ¡"

#~ msgid ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."
#~ msgstr ""
#~ "æ³¨æ„ï¼Œéœ€è¦å®‰è£…[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)æ¶‰åŠçš„æ‰€æœ‰çš„ä¾èµ–"

#~ msgid "ubuntu:app-get install git-lfs"
#~ msgstr ""

#~ msgid "Before use DB-GPT Knowledge"
#~ msgstr "åœ¨ä½¿ç”¨çŸ¥è¯†åº“ä¹‹å‰"

#~ msgid "**(Optional) load examples into SQLlite**"
#~ msgstr ""

#~ msgid "If you want to access an external LLM service, you need to"
#~ msgstr ""

#~ msgid ""
#~ "1.set the variables LLM_MODEL=YOUR_MODEL_NAME, "
#~ "MODEL_SERVER=YOUR_MODEL_SERVERï¼ˆeg:http://localhost:5000ï¼‰ in "
#~ "the .env file."
#~ msgstr ""

#~ msgid "2.execute dbgpt_server.py in light mode"
#~ msgstr ""

#~ msgid ""
#~ "If you want to learn about "
#~ "dbgpt-webui, read https://github./csunny/DB-"
#~ "GPT/tree/new-page-framework/datacenter"
#~ msgstr ""
#~ "å¦‚æœä½ æƒ³äº†è§£web-ui, è¯·è®¿é—®https://github./csunny/DB-GPT/tree"
#~ "/new-page-framework/datacenter"

