# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT ğŸ‘ğŸ‘ 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-29 20:30+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy/deploy.md:1
#: 963de2c147e4491085e40a367ede1cb3
msgid "Installation From Source"
msgstr "æºç å®‰è£…"

#: ../../getting_started/install/deploy/deploy.md:3
#: 3597fef8c1c24663ba9ddf0240dd8a1e
msgid ""
"This tutorial gives you a quick walkthrough about use DB-GPT with you "
"environment and data."
msgstr "æœ¬æ•™ç¨‹ä¸ºæ‚¨æä¾›äº†å…³äºå¦‚ä½•ä½¿ç”¨DB-GPTçš„ä½¿ç”¨æŒ‡å—ã€‚"

#: ../../getting_started/install/deploy/deploy.md:5
#: e9bd2165f24e41c2bebb4fed1672fd54
msgid "Installation"
msgstr "å®‰è£…"

#: ../../getting_started/install/deploy/deploy.md:7
#: 7a52cf49d60a4e76a43d8e534dcac6b8
msgid "To get started, install DB-GPT with the following steps."
msgstr "è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®‰è£…DB-GPT"

#: ../../getting_started/install/deploy/deploy.md:9
#: 4dbc9d0146bb4574b31222a14c45eb46
msgid "1. Hardware Requirements"
msgstr "1. ç¡¬ä»¶è¦æ±‚"

#: ../../getting_started/install/deploy/deploy.md:10
#: eee7223178704420a179781df476e855
msgid ""
"As our project has the ability to achieve ChatGPT performance of over "
"85%, there are certain hardware requirements. However, overall, the "
"project can be deployed and used on consumer-grade graphics cards. The "
"specific hardware requirements for deployment are as follows:"
msgstr "ç”±äºæˆ‘ä»¬çš„é¡¹ç›®æœ‰èƒ½åŠ›è¾¾åˆ°85%ä»¥ä¸Šçš„ChatGPTæ€§èƒ½ï¼Œæ‰€ä»¥å¯¹ç¡¬ä»¶æœ‰ä¸€å®šçš„è¦æ±‚ã€‚ä½†æ€»ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šå³å¯å®Œæˆé¡¹ç›®çš„éƒ¨ç½²ä½¿ç”¨ï¼Œå…·ä½“éƒ¨ç½²çš„ç¡¬ä»¶è¯´æ˜å¦‚ä¸‹:"

#: ../../getting_started/install/deploy/deploy.md
#: 21613161531642fb8d62d589b8d4feaa
msgid "GPU"
msgstr "GPU"

#: ../../getting_started/install/deploy/deploy.md
#: cf5a8b7a75034011b7305d8bd09cf69c e8b55944bb9d4b91976027b3a2ae09d0
msgid "VRAM Size"
msgstr "æ˜¾å­˜"

#: ../../getting_started/install/deploy/deploy.md
#: 36641aaf9b81420fae3c2a2d89816d8a
msgid "Performance"
msgstr "Performance"

#: ../../getting_started/install/deploy/deploy.md
#: ebc46f5c3f944ca6944774ac264ac801
msgid "RTX 4090"
msgstr "RTX 4090"

#: ../../getting_started/install/deploy/deploy.md
#: c387fa1e47ec4ae4bb145fbd4c18cd99 efbe071097b4421dbf06080d4ab1ec70
msgid "24 GB"
msgstr "24 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 48d5632d53e0413d937bd4c13716591a
msgid "Smooth conversation inference"
msgstr "Smooth conversation inference"

#: ../../getting_started/install/deploy/deploy.md
#: 5d7a590b885c4f128716da6347d91304
msgid "RTX 3090"
msgstr "RTX 3090"

#: ../../getting_started/install/deploy/deploy.md
#: 93a35c5359e9432b9fac149e67087b4b
msgid "Smooth conversation inference, better than V100"
msgstr "Smooth conversation inference, better than V100"

#: ../../getting_started/install/deploy/deploy.md
#: ec6d393791ae49eea9b2ff79f803e76c
msgid "V100"
msgstr "V100"

#: ../../getting_started/install/deploy/deploy.md
#: 7da6411bc1c642e9a1063e988044e9f2 852dac8854514958b45f7f125c11f625
msgid "16 GB"
msgstr "16 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 1a3c78ad04114d5ebf25edba7ebdfcb7 b8ed26b4da564660bd70f0a3af1282a8
msgid "Conversation inference possible, noticeable stutter"
msgstr "Conversation inference possible, noticeable stutter"

#: ../../getting_started/install/deploy/deploy.md
#: 36603d7af55742eebc2f023fdf798f43
msgid "T4"
msgstr "T4"

#: ../../getting_started/install/deploy/deploy.md:19
#: 0a4d0daf5d44421893db967825c0e158
msgid ""
"if your VRAM Size is not enough, DB-GPT supported 8-bit quantization and "
"4-bit quantization."
msgstr "å¦‚æœä½ çš„æ˜¾å­˜ä¸å¤Ÿï¼ŒDB-GPTæ”¯æŒ8-bitå’Œ4-bité‡åŒ–ç‰ˆæœ¬"

#: ../../getting_started/install/deploy/deploy.md:21
#: 3523d76dc2a8414495ad46f72c304226
msgid ""
"Here are some of the VRAM size usage of the models we tested in some "
"common scenarios."
msgstr "è¿™é‡Œæ˜¯é‡åŒ–ç‰ˆæœ¬çš„ç›¸å…³è¯´æ˜"

#: ../../getting_started/install/deploy/deploy.md
#: 8ff21b833bf64962901e2e7a52e1322a
msgid "Model"
msgstr "Model"

#: ../../getting_started/install/deploy/deploy.md
#: be429d17a47643c797d989148d62583a
msgid "Quantize"
msgstr "Quantize"

#: ../../getting_started/install/deploy/deploy.md
#: 2ec73c552480461dada4f44763c4623e c51b0f85b34845798d926db31eaf720c
msgid "vicuna-7b-v1.5"
msgstr "vicuna-7b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 2550147d36074525b23021a14dc55b24 5200cf5ca316459184cd247285adeb9c
#: 9698f16cfa3d4ac589e7741e2371766c a3c90ef3728240a9927da25688ca500b
#: c197d7bfe2be4f29995d6e6882a73c36 c8742a66fae049f9b2c6d333dd3fc209
#: ed4e5c0cc67c4cd3acbb45def0dbac67
msgid "4-bit"
msgstr "4-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 1985bf309e324843aa849c38c9819b03 998c11e1c9b846d596c4423a22a438c8
#: ffe2badbf75b4f119e22481b47866cc5
msgid "8 GB"
msgstr "8 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 23fed1f7361b46139294f979144f55b9 44b5316ad4be45acaca850ac350613dd
#: 5d2642a53e444757b176c8ff86504f45 5e9a0cf9bab54b6ea805dd4c9b209b51
#: 6a2ffb34d35a4d64bcb18d88b6ed3e83 b70e17fc9a184074ac4bc641312fc8e8
#: bb6bd13b7d09417ea26949cf385d037c
msgid "8-bit"
msgstr "8-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 0718757036b04690bd6fa98959ae41d3 2401f442291c4fd8a7fdf69d1745b604
#: 3c3af4923aa5498ca85d87610712f9ff 620ef43ffb5441a7ac0bfcd920729518
#: bf626548d1624794ad4e16f0acbdf59f e24c281f31b44b89bb1a9151b7e0ba9a
msgid "12 GB"
msgstr "12 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 685bc5ce8e4a411a8f59026674b5967d 8223632d5da5462b977c3bd9cb2731a2
msgid "vicuna-13b-v1.5"
msgstr "vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 09e37f82a825411191ab74a634706866 80138163ba034d819eeb165384392655
#: da076416fa8c4fb7b28939ca7c76d06b
msgid "20 GB"
msgstr "20 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 4551174e7f8449bd93fb14de039ed400 9879cd654383482784992f410ba478aa
msgid "llama-2-7b"
msgstr "llama-2-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 0a74ee29547445e4a1389386249528a4 af6fa6d9dec54179808820fda8b8210b
msgid "llama-2-13b"
msgstr "llama-2-13b"

#: ../../getting_started/install/deploy/deploy.md
#: 1aadc9ec7645467596e991f770e5bce5 32aa71dcf6d9497ba48af4c2c192e3ef
msgid "llama-2-70b"
msgstr "llama-2-70b"

#: ../../getting_started/install/deploy/deploy.md
#: 1e63e7f32ec74f4e8290254ce348c50d
msgid "48 GB"
msgstr "48 GB"

#: ../../getting_started/install/deploy/deploy.md
#: d5416570171e49a88a75161f4e5f0c77
msgid "80 GB"
msgstr "80 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 598393baadb7468ca649db8de42fd413 f6aaa22133ac4721b6740ab75c43819d
msgid "baichuan-7b"
msgstr "baichuan-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 454e6b05fc2f4f318afc7d7e5f4477b9 ebd9204f511949d695b20caa8d2b5701
msgid "baichuan-13b"
msgstr "baichuan-13b"

#: ../../getting_started/install/deploy/deploy.md:40
#: a74a45bb031c41a9a88fab4023791430
msgid "2. Install"
msgstr "2. Install"

#: ../../getting_started/install/deploy/deploy.md:45
#: 480a54d6f3b944b2a26414561ccd5cf4
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration.  For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. [How to install "
"Miniconda](https://docs.conda.io/en/latest/miniconda.html)"
msgstr ""
"ç›®å‰ä½¿ç”¨Sqliteä½œä¸ºé»˜è®¤æ•°æ®åº“ï¼Œå› æ­¤DB-"
"GPTå¿«é€Ÿéƒ¨ç½²ä¸éœ€è¦éƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–æ•°æ®åº“ï¼Œéœ€è¦å…ˆéƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚æˆ‘ä»¬ç›®å‰ä½¿ç”¨Minicondaè¿›è¡Œpythonç¯å¢ƒå’ŒåŒ…ä¾èµ–ç®¡ç†[å®‰è£…"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy/deploy.md:54
#: 8d9d99b9b3f6444f8e2b289ff0b3d1cf
msgid "Before use DB-GPT Knowledge"
msgstr "åœ¨ä½¿ç”¨çŸ¥è¯†åº“ä¹‹å‰"

#: ../../getting_started/install/deploy/deploy.md:60
#: 3ba68f5b12134958ad3495fc03e6261f
msgid ""
"Once the environment is installed, we have to create a new folder "
"\"models\" in the DB-GPT project, and then we can put all the models "
"downloaded from huggingface in this directory"
msgstr "å¦‚æœä½ å·²ç»å®‰è£…å¥½äº†ç¯å¢ƒéœ€è¦åˆ›å»ºmodels, ç„¶ååˆ°huggingfaceå®˜ç½‘ä¸‹è½½æ¨¡å‹"

#: ../../getting_started/install/deploy/deploy.md:63
#: ae6612ce4b6845cfa9396fcad5549bd0
msgid "Notice make sure you have install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:65
#: fbfa0caf0615484bbe861b327a67c847
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:67
#: ca10fedecab243fba8a0d11b14ccddde
msgid "ubuntu:app-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:69
#: a3026601a2304ce3a38e162cb7f44d2f
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:86
#: ffda61cebb534429bfa1d3a375647077
msgid ""
"The model files are large and will take a long time to download. During "
"the download, let's configure the .env file, which needs to be copied and"
" created from the .env.template"
msgstr "æ¨¡å‹æ–‡ä»¶å¾ˆå¤§ï¼Œéœ€è¦å¾ˆé•¿æ—¶é—´æ‰èƒ½ä¸‹è½½ã€‚åœ¨ä¸‹è½½è¿‡ç¨‹ä¸­ï¼Œè®©æˆ‘ä»¬é…ç½®.envæ–‡ä»¶ï¼Œå®ƒéœ€è¦ä»ã€‚env.templateä¸­å¤åˆ¶å’Œåˆ›å»ºã€‚"

#: ../../getting_started/install/deploy/deploy.md:88
#: f30593a4496c498d807bc1bf48fc333e
msgid ""
"if you want to use openai llm service, see [LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"
msgstr ""
"å¦‚æœæƒ³ä½¿ç”¨openaiå¤§æ¨¡å‹æœåŠ¡, å¯ä»¥å‚è€ƒ[LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"

#: ../../getting_started/install/deploy/deploy.md:91
#: adbf4130afb442e0b2ebe9ecadc27927
msgid "cp .env.template .env"
msgstr "cp .env.template .env"

#: ../../getting_started/install/deploy/deploy.md:94
#: 67a0efdf4138465b84f50946c50aef33
msgid ""
"You can configure basic parameters in the .env file, for example setting "
"LLM_MODEL to the model to be used"
msgstr "æ‚¨å¯ä»¥åœ¨.envæ–‡ä»¶ä¸­é…ç½®åŸºæœ¬å‚æ•°ï¼Œä¾‹å¦‚å°†LLM_MODELè®¾ç½®ä¸ºè¦ä½¿ç”¨çš„æ¨¡å‹ã€‚"

#: ../../getting_started/install/deploy/deploy.md:96
#: ec7328fea11344b8ac6b821a1cd10531
msgid ""
"([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"æ‚¨å¯ä»¥åœ¨.envæ–‡ä»¶ä¸­é…ç½®åŸºæœ¬å‚æ•°ï¼Œä¾‹å¦‚å°†LLM_MODELè®¾ç½®ä¸ºè¦ä½¿ç”¨çš„æ¨¡å‹ã€‚([Vicuna-v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5)ï¼Œ "
"ç›®å‰Vicuna-v1.5æ¨¡å‹(åŸºäºllama2)å·²ç»å¼€æºäº†ï¼Œæˆ‘ä»¬æ¨èä½ ä½¿ç”¨è¿™ä¸ªæ¨¡å‹é€šè¿‡è®¾ç½®LLM_MODEL=vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md:98
#: 386d91268dc54f3a92172758afe25d3e
msgid "3. Run"
msgstr "3. Run"

#: ../../getting_started/install/deploy/deploy.md:100
#: 4672d31b78dc44719ad41f05fe0bf103
msgid "**(Optional) load examples into SQLlite**"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:105
#: 174bb3d5c82c41ad92fa2147dbfe0bdc
msgid "On windows platform:"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:110
#: aa25742904354373a10b8aaa62fa6005
msgid "1.Run db-gpt server"
msgstr "1.Run db-gpt server"

#: ../../getting_started/install/deploy/deploy.md:116
#: 86a196e04cd1459ab83c0402b6c73c25
msgid "Open http://localhost:5000 with your browser to see the product."
msgstr "æ‰“å¼€æµè§ˆå™¨è®¿é—®http://localhost:5000"

#: ../../getting_started/install/deploy/deploy.md:119
#: e53c6021747e48298a9a94b33161aba2
msgid "If you want to access an external LLM service, you need to"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:121
#: a5d7891ca43a4da8a60e925cfdff829b
msgid ""
"1.set the variables LLM_MODEL=YOUR_MODEL_NAME, "
"MODEL_SERVER=YOUR_MODEL_SERVERï¼ˆeg:http://localhost:5000ï¼‰ in the .env "
"file."
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:123
#: adc8ccbdda9743aab58a3fdb8aa593fd
msgid "2.execute dbgpt_server.py in light mode"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:126
#: 29fb5c398fd94f5eaed6382ec7332097
msgid ""
"If you want to learn about dbgpt-webui, read https://github./csunny/DB-"
"GPT/tree/new-page-framework/datacenter"
msgstr ""
"å¦‚æœä½ æƒ³äº†è§£web-ui, è¯·è®¿é—®https://github./csunny/DB-GPT/tree/new-page-"
"framework/datacenter"

#: ../../getting_started/install/deploy/deploy.md:132
#: d5a6113dc2844430bde6d1e62ae70eb9
#, fuzzy
msgid "Multiple GPUs"
msgstr "4. Multiple GPUs"

#: ../../getting_started/install/deploy/deploy.md:134
#: 15d02416586344d19adfc39f6d05ee9f
msgid ""
"DB-GPT will use all available gpu by default. And you can modify the "
"setting `CUDA_VISIBLE_DEVICES=0,1` in `.env` file to use the specific gpu"
" IDs."
msgstr "DB-GPTé»˜è®¤åŠ è½½å¯åˆ©ç”¨çš„gpuï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡ä¿®æ”¹ åœ¨`.env`æ–‡ä»¶ `CUDA_VISIBLE_DEVICES=0,1`æ¥æŒ‡å®šgpu IDs"

#: ../../getting_started/install/deploy/deploy.md:136
#: d6875130c3ba4bbf80a046a77c815f26
msgid ""
"Optionally, you can also specify the gpu ID to use before the starting "
"command, as shown below:"
msgstr "ä½ ä¹Ÿå¯ä»¥æŒ‡å®šgpu IDå¯åŠ¨"

#: ../../getting_started/install/deploy/deploy.md:146
#: b2d5855d7cf946a1a00ec55e0328b1a6
msgid ""
"You can modify the setting `MAX_GPU_MEMORY=xxGib` in `.env` file to "
"configure the maximum memory used by each GPU."
msgstr "åŒæ—¶ä½ å¯ä»¥é€šè¿‡åœ¨.envæ–‡ä»¶è®¾ç½®`MAX_GPU_MEMORY=xxGib`ä¿®æ”¹æ¯ä¸ªGPUçš„æœ€å¤§ä½¿ç”¨å†…å­˜"

#: ../../getting_started/install/deploy/deploy.md:148
#: 385caf97c7d149b1bb1852198c8e04de
#, fuzzy
msgid "Not Enough Memory"
msgstr "5. Not Enough Memory"

#: ../../getting_started/install/deploy/deploy.md:150
#: 30ec48dfe9f44ef88e55f7218b2d3967
msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
msgstr "DB-GPT æ”¯æŒ 8-bit quantization å’Œ 4-bit quantization."

#: ../../getting_started/install/deploy/deploy.md:152
#: dad6fbec89104dd9b11ae6778942bf69
msgid ""
"You can modify the setting `QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` "
"in `.env` file to use quantization(8-bit quantization is enabled by "
"default)."
msgstr "ä½ å¯ä»¥é€šè¿‡åœ¨.envæ–‡ä»¶è®¾ç½®`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`"

#: ../../getting_started/install/deploy/deploy.md:154
#: d6eebf82de5b410a979e978e489d5317
msgid ""
"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit"
" quantization can run with 48 GB of VRAM."
msgstr ""
"Llama-2-70b with 8-bit quantization å¯ä»¥è¿è¡Œåœ¨80GB VRAMæœºå™¨ï¼Œ 4-bit "
"quantizationå¯ä»¥è¿è¡Œåœ¨ 48 GB  VRAM"

#~ msgid ""
#~ "Notice make sure you have install "
#~ "git-lfs centos:yum install git-lfs "
#~ "ubuntu:app-get install git-lfs "
#~ "macos:brew install git-lfs"
#~ msgstr ""
#~ "æ³¨æ„ä¸‹è½½æ¨¡å‹ä¹‹å‰ç¡®ä¿git-lfså·²ç»å®‰ubuntu:app-get install "
#~ "git-lfs macos:brew install git-lfs"

#~ msgid ""
#~ "You can refer to this document to"
#~ " obtain the Vicuna weights: "
#~ "[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md"
#~ "#model-weights) ."
#~ msgstr ""
#~ "ä½ å¯ä»¥å‚è€ƒå¦‚ä½•è·å–Vicuna weightsæ–‡æ¡£[Vicuna](https://github.com/lm-"
#~ "sys/FastChat/blob/main/README.md#model-weights) ."

#~ msgid ""
#~ "If you have difficulty with this "
#~ "step, you can also directly use "
#~ "the model from [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) as "
#~ "a replacement."
#~ msgstr ""
#~ "å¦‚æœè§‰å¾—æ¨¡å‹å¤ªå¤§ä½ ä¹Ÿå¯ä»¥ä¸‹è½½vicuna-7b [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) "

#~ msgid ""
#~ "If you want to access an external"
#~ " LLM service, you need to 1.set "
#~ "the variables LLM_MODEL=YOUR_MODEL_NAME "
#~ "MODEL_SERVER=YOUR_MODEL_SERVERï¼ˆeg:http://localhost:5000ï¼‰ in "
#~ "the .env file. 2.execute dbgpt_server.py "
#~ "in light mode"
#~ msgstr ""
#~ "å¦‚æœä½ æƒ³è®¿é—®å¤–éƒ¨çš„å¤§æ¨¡å‹æœåŠ¡(æ˜¯é€šè¿‡DB-"
#~ "GPT/pilot/server/llmserver.pyå¯åŠ¨çš„æ¨¡å‹æœåŠ¡)ï¼Œ1.éœ€è¦åœ¨.envæ–‡ä»¶è®¾ç½®æ¨¡å‹åå’Œå¤–éƒ¨æ¨¡å‹æœåŠ¡åœ°å€ã€‚2.ä½¿ç”¨lightæ¨¡å¼å¯åŠ¨æœåŠ¡"

#~ msgid ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."
#~ msgstr ""
#~ "æ³¨æ„ï¼Œéœ€è¦å®‰è£…[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)æ¶‰åŠçš„æ‰€æœ‰çš„ä¾èµ–"

