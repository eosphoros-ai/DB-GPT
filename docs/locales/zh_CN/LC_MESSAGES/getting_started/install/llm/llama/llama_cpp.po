# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-21 16:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/llm/llama/llama_cpp.md:1
#: 24d5c21cd8b44f1d8585ba5c83e34acc
msgid "llama.cpp"
msgstr "llama.cpp"

#: ../../getting_started/install/llm/llama/llama_cpp.md:5
#: 56969ff863d949aa8df55d3bdb6957e7
msgid ""
"DB-GPT already supports "
"[llama.cpp](https://github.com/ggerganov/llama.cpp) via [llama-cpp-"
"python](https://github.com/abetlen/llama-cpp-python)."
msgstr ""

#: ../../getting_started/install/llm/llama/llama_cpp.md:7
#: afe223eafcc641779e1580cac574c34a
msgid "Running llama.cpp"
msgstr "运行 llama.cpp"

#: ../../getting_started/install/llm/llama/llama_cpp.md:9
#: 0eaf98a036434eecb2af1fa89f045620
msgid "Preparing Model Files"
msgstr "准备模型文件"

#: ../../getting_started/install/llm/llama/llama_cpp.md:11
#: 4f45be5d9658451fb95f1d5d31dc8778
msgid ""
"To use llama.cpp, you need to prepare a ggml format model file, and there"
" are two common ways to obtain it, you can choose either:"
msgstr "使用llama.cpp, 你需要准备ggml格式的文件，你可以通过以下两种方法获取"

#: ../../getting_started/install/llm/llama/llama_cpp.md:13
#: 9934596e0f6e466aae63cefbb019e0ec
msgid "Download a pre-converted model file."
msgstr "Download a pre-converted model file."

#: ../../getting_started/install/llm/llama/llama_cpp.md:15
#: 33fef76961064a5ca4c86c57111c8bd3
msgid ""
"Suppose you want to use [Vicuna 7B v1.5](https://huggingface.co/lmsys"
"/vicuna-7b-v1.5), you can download the file already converted from "
"[TheBloke/vicuna-7B-v1.5-GGML](https://huggingface.co/TheBloke/vicuna-"
"7B-v1.5-GGML), only one file is needed. Download it to the `models` "
"directory and rename it to `ggml-model-q4_0.bin`."
msgstr ""
"假设您想使用[Vicuna 7B v1.5](https://huggingface.co/lmsys/vicuna-"
"7b-v1.5)您可以从[TheBloke/vicuna-"
"7B-v1.5-GGML](https://huggingface.co/TheBloke/vicuna-"
"7B-v1.5-GGML)下载已转换的文件，只需要一个文件。将其下载到models目录并将其重命名为ggml-model-q4_0.bin。"

#: ../../getting_started/install/llm/llama/llama_cpp.md:21
#: 65fed5b7e95b4205b2b94596a21b6fe8
msgid "Convert It Yourself"
msgstr "Convert It Yourself"

#: ../../getting_started/install/llm/llama/llama_cpp.md:23
#: 1421761d320046f79f725e64bd7d854c
msgid ""
"You can convert the model file yourself according to the instructions in "
"[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run), and put the converted file in the models directory "
"and rename it to `ggml-model-q4_0.bin`."
msgstr ""
"您可以根据[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run)中的说明自己转换模型文件，然后将转换后的文件放入models目录中，并将其重命名为ggml-"
"model-q4_0.bin。"

#: ../../getting_started/install/llm/llama/llama_cpp.md:25
#: 850b1f8ef6be49b192e01c1b7d8f1f26
msgid "Installing Dependencies"
msgstr "安装依赖"

#: ../../getting_started/install/llm/llama/llama_cpp.md:27
#: b323ee4799d745cc9c0a449bd37c371a
msgid ""
"llama.cpp is an optional dependency in DB-GPT, and you can manually "
"install it using the following command:"
msgstr "llama.cpp在DB-GPT中是可选安装项, 你可以通过一下命令进行安装"

#: ../../getting_started/install/llm/llama/llama_cpp.md:33
#: 75b75c84ffb7476d8501a28bb2719615
msgid "Modifying the Configuration File"
msgstr "修改配置文件"

#: ../../getting_started/install/llm/llama/llama_cpp.md:35
#: d1f8b3e1ad3441f2aafbfe2519113c2c
msgid "Next, you can directly modify your `.env` file to enable llama.cpp."
msgstr "修改`.env`文件使用llama.cpp"

#: ../../getting_started/install/llm/llama/llama_cpp.md:42
#: 2ddcab3834f646e58a8b3316abf6ce3a
msgid ""
"Then you can run it according to [Run](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html#run)."
msgstr ""
"然后你可以通过[Run](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html#run).来运行"

#: ../../getting_started/install/llm/llama/llama_cpp.md:45
#: bb9f222d22534827a9fa164b2126d192
msgid "More Configurations"
msgstr "更多配置文件"

#: ../../getting_started/install/llm/llama/llama_cpp.md:47
#: 14d016ad5bad451888d01e24f0ca86d9
msgid ""
"In DB-GPT, the model configuration can be done through  `{model "
"name}_{config key}`."
msgstr ""
"In DB-GPT, the model configuration can be done through  `{model "
"name}_{config key}`."

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: a1bf4c1f49bd4d97ac45d4f3aff442c6
msgid "Environment Variable Key"
msgstr "Environment Variable Key"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 92692a38219c432fadffb8b3825ce678
msgid "default"
msgstr "default"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 72b2d251aa2e4ca09c335b58e1a08de3
msgid "Prompt Template Name"
msgstr "Prompt Template Name"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 85a9f89eeb9a4b70b56913354e947329
msgid "llama_cpp_prompt_template"
msgstr "llama_cpp_prompt_template"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 17e9750fbb824dfdaaed5415f6406e35 602016763bb2470d8a8ef700e576407b
#: 790caafd5c4c4cecbb4c190745fb994c ceb6c41315ab4c5798ab3c64ee8693eb
#: cfafab69a2684e27bd55aadfdd4c1575
msgid "None"
msgstr "None"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 5d02f2d1d5834b1e9e5d6982247fd6c9
msgid ""
"Prompt template name, now support: `zero_shot, vicuna_v1.1, llama-2"
",baichuan-chat`, If None, the prompt template is automatically determined"
" from model path。"
msgstr ""
"Prompt template 现在可以支持`zero_shot, vicuna_v1.1, llama-2,baichuan-chat`, "
"如果是None, the prompt template可以自动选择模型路径"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 2a95bc11386f45498b3585b194f24c17
msgid "llama_cpp_model_path"
msgstr "llama_cpp_model_path"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: c02db8a50e7a4df0acb6b75798a3ad4b
msgid "Model path"
msgstr "Model path"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 6c92b2ec52634728bcc421670cdda70b
msgid "llama_cpp_n_gpu_layers"
msgstr "llama_cpp_n_gpu_layers"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 9f1e1b763a0b40d28efd734fe20e1ba7
msgid "1000000000"
msgstr "1000000000"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 0f511b7907594c1f9c9818638764f209
msgid ""
"Number of layers to offload to the GPU, Set this to 1000000000 to offload"
" all layers to the GPU. If your GPU VRAM is not enough, you can set a low"
" number, eg: `10`"
msgstr "要将层数转移到GPU上，将其设置为1000000000以将所有层转移到GPU上。如果您的GPU VRAM不足，可以设置较低的数字，例如：10。"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 1ffdfa4eb78d4127b302b6d703852692
msgid "llama_cpp_n_threads"
msgstr "llama_cpp_n_threads"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: f14379e7ea16476da403d5085b67db1c
msgid ""
"Number of threads to use. If None, the number of threads is automatically"
" determined"
msgstr "要使用的线程数量。如果为None，则线程数量将自动确定。"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 41cc1035f6e340e19848452d48a161db
msgid "llama_cpp_n_batch"
msgstr "llama_cpp_n_batch"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 993c3b9218ee4299beae53bd75a01001
msgid "512"
msgstr "512"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 0e11d38c9b58478cacdade34de146320
msgid "Maximum number of prompt tokens to batch together when calling llama_eval"
msgstr "在调用llama_eval时，批处理在一起的prompt tokens的最大数量"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 24f5381956d34569aabee4a5d832388b
msgid "llama_cpp_n_gqa"
msgstr "llama_cpp_n_gqa"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 07d05844541c452caaa8d5bf56c3f8a1
msgid "Grouped-query attention. Must be 8 for llama-2 70b."
msgstr "对于llama-2 70b模型，Grouped-query attention必须为8。"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 40a1b9750d854bb19dc18b7d530beccf
msgid "llama_cpp_rms_norm_eps"
msgstr "llama_cpp_rms_norm_eps"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 6018ee183b9548eabf91e9fc683e7c24
msgid "5e-06"
msgstr "5e-06"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: eb273c6bcf2c4c47808024008ce230dc
msgid "5e-6 is a good value for llama-2 models."
msgstr "对于llama-2模型来说，5e-6是一个不错的值。"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: f70f3e935b764b6f9544d201ba2aaa05
msgid "llama_cpp_cache_capacity"
msgstr "llama_cpp_cache_capacity"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 70035ec5be244eda9fe93be3df2c66df
msgid "Maximum cache capacity. Examples: 2000MiB, 2GiB"
msgstr "cache capacity最大值. Examples: 2000MiB, 2GiB"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 164c31b005ae4979938d9bc67e7f2759
msgid "llama_cpp_prefer_cpu"
msgstr "llama_cpp_prefer_cpu"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 28f890f6bee3412e94aeb1326367326e
msgid "False"
msgstr "False"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: f8f27b6323384431ba064a720f39f997
msgid ""
"If a GPU is available, it will be preferred by default, unless "
"prefer_cpu=False is configured."
msgstr "如果有可用的GPU，默认情况下会优先使用GPU，除非配置了prefer_cpu=False。"

#: ../../getting_started/install/llm/llama/llama_cpp.md:61
#: 0471e56c790047bab422aa47edad0a15
msgid "GPU Acceleration"
msgstr "GPU 加速"

#: ../../getting_started/install/llm/llama/llama_cpp.md:63
#: e95ad40d29004455bebeec8a1a7248c8
msgid ""
"GPU acceleration is supported by default. If you encounter any issues, "
"you can uninstall the dependent packages with the following command:"
msgstr "默认情况下支持GPU加速。如果遇到任何问题，您可以使用以下命令卸载相关的依赖包"

#: ../../getting_started/install/llm/llama/llama_cpp.md:68
#: c0caf1420e43437589693ddec96bd50f
msgid ""
"Then install `llama-cpp-python` according to the instructions in [llama-"
"cpp-python](https://github.com/abetlen/llama-cpp-"
"python/blob/main/README.md)."
msgstr ""
"然后通过指令[llama-cpp-python](https://github.com/abetlen/llama-cpp-"
"python/blob/main/README.md).安装`llama-cpp-python`"

#: ../../getting_started/install/llm/llama/llama_cpp.md:71
#: fe082f65b4e9416c97b18e5005bc0a59
msgid "Mac Usage"
msgstr "Mac Usage"

#: ../../getting_started/install/llm/llama/llama_cpp.md:73
#: 6f30d3fa399f434189fcb03d28a42d2d
msgid ""
"Special attention, if you are using Apple Silicon (M1) Mac, it is highly "
"recommended to install arm64 architecture python support, for example:"
msgstr "特别注意：如果您正在使用苹果芯片（M1）的Mac电脑，强烈建议安装arm64架构的Python支持，例如："

#: ../../getting_started/install/llm/llama/llama_cpp.md:80
#: 74602bede3c5472fbabc7de47eb2ff7a
msgid "Windows Usage"
msgstr "Windows使用"

#: ../../getting_started/install/llm/llama/llama_cpp.md:82
#: ae78332a348b44cb847723a998b98048
msgid ""
"The use under the Windows platform has not been rigorously tested and "
"verified, and you are welcome to use it. If you have any problems, you "
"can create an [issue](https://github.com/eosphoros-ai/DB-GPT/issues) or "
"[contact us](https://github.com/eosphoros-ai/DB-GPT/tree/main#contact-"
"information) directly."
msgstr ""
"在Windows平台上的使用尚未经过严格的测试和验证，欢迎您使用。如果您有任何问题，可以创建一个[issue](https://github.com"
"/eosphoros-ai/DB-GPT/issues)或者[contact us](https://github.com/eosphoros-"
"ai/DB-GPT/tree/main#contact-information) directly."

#~ msgid ""
#~ "DB-GPT is now supported by "
#~ "[llama-cpp-python](https://github.com/abetlen/llama-"
#~ "cpp-python) through "
#~ "[llama.cpp](https://github.com/ggerganov/llama.cpp)."
#~ msgstr ""
#~ "DB-GPT is now supported by "
#~ "[llama-cpp-python](https://github.com/abetlen/llama-"
#~ "cpp-python) through "
#~ "[llama.cpp](https://github.com/ggerganov/llama.cpp)."
