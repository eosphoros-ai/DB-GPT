# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT ğŸ‘ğŸ‘ 0.4.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-11-04 15:08+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy.rst:4 bb7e1a7b70624a3d91804c5ce2d5216b
msgid "Installation From Source"
msgstr "æºç å®‰è£…"

#: ../../getting_started/install/deploy.rst:6 7524477e8b8a456b8e2b942621a6b225
msgid "To get started, install DB-GPT with the following steps."
msgstr "æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œå®‰è£…"

#: ../../getting_started/install/deploy.rst:10 871cdb0a7e004668b518168461d76945
msgid "1.Preparation"
msgstr "1.å‡†å¤‡"

#: ../../getting_started/install/deploy.rst:11 d845f48ec05a4bb7a5b0f95a0644029d
msgid "**Download DB-GPT**"
msgstr "**ä¸‹è½½DB-GPTé¡¹ç›®**"

#: ../../getting_started/install/deploy.rst:17 85df7bb2b73141058ba0ca5a73e46701
msgid "**Install Miniconda**"
msgstr "**å®‰è£…Miniconda**"

#: ../../getting_started/install/deploy.rst:19 b7b3e120ded344d09fb2d7a3a7903497
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration. For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. `How to install Miniconda "
"<https://docs.conda.io/en/latest/miniconda.html>`_"
msgstr ""
"ç›®å‰ä½¿ç”¨Sqliteä½œä¸ºé»˜è®¤æ•°æ®åº“ï¼Œå› æ­¤DB-"
"GPTå¿«é€Ÿéƒ¨ç½²ä¸éœ€è¦éƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–æ•°æ®åº“ï¼Œéœ€è¦å…ˆéƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚æˆ‘ä»¬ç›®å‰ä½¿ç”¨Minicondaè¿›è¡Œpythonç¯å¢ƒå’ŒåŒ…ä¾èµ–ç®¡ç†[å®‰è£…"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy.rst:36 a5e34db6a6b8450c8a1f35377a1dc93b
msgid "2.Deploy LLM Service"
msgstr "2.éƒ¨ç½²LLMæœåŠ¡"

#: ../../getting_started/install/deploy.rst:37 5c49829b086e4d5fbcb5a971a34fa3b5
msgid ""
"DB-GPT can be deployed on servers with low hardware requirements or on "
"servers with high hardware requirements."
msgstr "DB-GPTå¯ä»¥éƒ¨ç½²åœ¨å¯¹ç¡¬ä»¶è¦æ±‚ä¸é«˜çš„æœåŠ¡å™¨ï¼Œä¹Ÿå¯ä»¥éƒ¨ç½²åœ¨å¯¹ç¡¬ä»¶è¦æ±‚é«˜çš„æœåŠ¡å™¨"

#: ../../getting_started/install/deploy.rst:39 7c2f7e2b49054d17850942bef9570c45
msgid ""
"If you are low hardware requirements you can install DB-GPT by Using "
"third-part LLM REST API Service OpenAI, Azure, tongyi."
msgstr "Low hardware requirementsæ¨¡å¼é€‚ç”¨äºå¯¹æ¥ç¬¬ä¸‰æ–¹æ¨¡å‹æœåŠ¡çš„api,æ¯”å¦‚OpenAI, é€šä¹‰åƒé—®, æ–‡å¿ƒ.cppã€‚"

#: ../../getting_started/install/deploy.rst:43 8c392e63073a47698e38456c5994ff25
msgid "As our project has the ability to achieve OpenAI performance of over 85%,"
msgstr "ä½¿ç”¨OpenAIæœåŠ¡å¯ä»¥è®©DB-GPTå‡†ç¡®ç‡è¾¾åˆ°85%"

#: ../../getting_started/install/deploy.rst:48 148f8d85c02345219ac535003fe05fca
msgid "Notice make sure you have install git-lfs"
msgstr "ç¡®è®¤æ˜¯å¦å·²ç»å®‰è£…git-lfs"

#: ../../getting_started/install/deploy.rst:50 7f719273d1ff4d888376030531a560c4
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:52 f5c2035bca214e3fbef2b70be0e76247
msgid "ubuntu:apt-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:54 cb1b52b9323e4e9fbccfd3c42067a568
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:58
#: ../../getting_started/install/deploy.rst:223
#: 9c601ee0c06646d58cbd2c2fff342cb5 d3fe6c712b4145d0856487c65057c2a0
msgid "OpenAI"
msgstr "OpenAI"

#: ../../getting_started/install/deploy.rst:60
#: ../../getting_started/install/deploy.rst:207
#: 5e0e84bd11084d158e18b702022aaa96 6a9c9830e5184eac8b6b7425780f4a3e
msgid "Download embedding model"
msgstr "ä¸‹è½½embedding model"

#: ../../getting_started/install/deploy.rst:72
#: ../../getting_started/install/deploy.rst:231
#: 953d6931627641b985588abf8b6ad9fd d52e287164ca44f79709f4645a1bb774
msgid "Configure LLM_MODEL and PROXY_API_URL and API_KEY in `.env` file"
msgstr "åœ¨`.env`æ–‡ä»¶è®¾ç½®LLM_MODEL and PROXY_API_URL and API_KEY"

#: ../../getting_started/install/deploy.rst:82
#: ../../getting_started/install/deploy.rst:282
#: 6dc89616c0bf46a898a49606d1349039 e30b7458fc0d46e8bcf0ec61a2be2e19
msgid "Make sure your .env configuration is not overwritten"
msgstr "è®¤.envæ–‡ä»¶ä¸ä¼šè¢«è¦†ç›–\""

#: ../../getting_started/install/deploy.rst:85 7adce7ac878d4b1ab9249d840d302520
msgid "Vicuna"
msgstr "Vicuna"

#: ../../getting_started/install/deploy.rst:86 44a32c4cd0b140838b49f739d68ff42d
msgid ""
"`Vicuna-v1.5 <https://huggingface.co/lmsys/vicuna-13b-v1.5>`_ based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""

#: ../../getting_started/install/deploy.rst:88 f4991de79c434961b0ee777cf42dc491
msgid "vicuna-v1.5 hardware requirements"
msgstr ""

#: ../../getting_started/install/deploy.rst:92
#: ../../getting_started/install/deploy.rst:137
#: 596405ba21d046c993c18d0bcaf95048 ca993a5f1b194707bbe3d5d6e9d670d8
msgid "Model"
msgstr ""

#: ../../getting_started/install/deploy.rst:93
#: ../../getting_started/install/deploy.rst:138
#: 8437bb47daa94ca8a43b0eadc1286842 d040d964198d4c95afa6dd0d86586cc2
msgid "Quantize"
msgstr ""

#: ../../getting_started/install/deploy.rst:94
#: ../../getting_started/install/deploy.rst:139
#: 453bc4416d89421699d90d940498b9ac 5de996bb955f4c6ca8d5651985ced255
msgid "VRAM Size"
msgstr ""

#: ../../getting_started/install/deploy.rst:95
#: ../../getting_started/install/deploy.rst:98 077d52b8588e4ddd956acb7ff86bc458
#: fcf0ca75422343d19b12502fb7746f08
msgid "vicuna-7b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:96
#: ../../getting_started/install/deploy.rst:102
#: ../../getting_started/install/deploy.rst:141
#: ../../getting_started/install/deploy.rst:147
#: 799b310b76454f79a261300253a9bb75 bd98df81ec8941b69c7d1de107154b1b
#: ca71731754f545d589e506cb53e6f042 d4023553e4384cc4955ef5bf4dc4d05b
msgid "4-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:97
#: ../../getting_started/install/deploy.rst:142
#: adfc22d9c6394046bb632831a4a3b066 e4e7762c714a49828da630a5f9d641dc
msgid "8 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:99
#: ../../getting_started/install/deploy.rst:105
#: ../../getting_started/install/deploy.rst:144
#: ../../getting_started/install/deploy.rst:150
#: 0d6bd1c4a6464df38723de978c09e8bc 106e20f7727f43d280b7560dc0f19bbc
#: 5a95691d1d134638bffa478daf2577a0 f8add03506244306b1ac6e10c5a2bc1f
msgid "8-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:100
#: ../../getting_started/install/deploy.rst:103
#: ../../getting_started/install/deploy.rst:145
#: ../../getting_started/install/deploy.rst:148
#: 03222e6a682e406bb56c21025412fbe5 2eb88af3efb74fb79899d7818a79d177
#: 9705b9d7d2084f0b8867f6aeec66251b bca6e5151e244a318f4c2078c3f82118
msgid "12 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:101
#: ../../getting_started/install/deploy.rst:104
#: 1596cbb77c954d1db00e2fa283d66a9a c6e8dc1600d14ee5a47f45f75ec709f4
msgid "vicuna-13b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:106
#: ../../getting_started/install/deploy.rst:151
#: 00ee98593b7f4b29b5d97fa5a323bfa1 0169214dcf8d46c791b3482c81283fa0
msgid "20 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:122
#: ../../getting_started/install/deploy.rst:169
#: ../../getting_started/install/deploy.rst:195
#: 852f800e380c44d3b1b5bdaa881406b7 89ddd6c4a4874f4f8b7f051d797e364a
#: a2b4436f8fcb44ada3b5bf5c1099e646
msgid "The model files are large and will take a long time to download."
msgstr ""

#: ../../getting_started/install/deploy.rst:124
#: ../../getting_started/install/deploy.rst:171
#: ../../getting_started/install/deploy.rst:197
#: 18ccf7acdf3045588a19866c0161f44b 3d3d93649ec943c8b2b725807e969cbf
#: 6205975466f54d8fbf8e30b58df6b9c2
msgid "**Configure LLM_MODEL in `.env` file**"
msgstr ""

#: ../../getting_started/install/deploy.rst:131
#: ../../getting_started/install/deploy.rst:228
#: 022addaea4ea432584b8a27ea83a9dc4 7a95d25c58d2491e855dff42f6d3a686
msgid "Baichuan"
msgstr ""

#: ../../getting_started/install/deploy.rst:133
#: b5374f6d0ca44ee9bd984c0198515861
msgid "Baichuan hardware requirements"
msgstr ""

#: ../../getting_started/install/deploy.rst:140
#: ../../getting_started/install/deploy.rst:143
#: 6a5e6034c185455d81e8ba5349dbc191 9d0109817c524747909417924bb0bd31
msgid "baichuan-7b"
msgstr ""

#: ../../getting_started/install/deploy.rst:146
#: ../../getting_started/install/deploy.rst:149
#: b3300ed57f0c4ed98816652ea4a5c3c8 d57e98b46fb14f649d6ac8df27edcfd8
msgid "baichuan-13b"
msgstr ""

#: ../../getting_started/install/deploy.rst:173
#: ac684ccbea7d4f5380c43ad02a1b7231
msgid "please rename Baichuan path to \"baichuan2-13b\" or \"baichuan2-7b\""
msgstr "å°†Baichuanæ¨¡å‹ç›®å½•ä¿®æ”¹ä¸º\"baichuan2-13b\" æˆ– \"baichuan2-7b\""

#: ../../getting_started/install/deploy.rst:179
#: d2e283112ed1440b93510c331922b37c
msgid "ChatGLM"
msgstr ""

#: ../../getting_started/install/deploy.rst:199
#: ae30d065461e446e86b02d3903108a02
msgid "please rename chatglm model path to \"chatglm2-6b\""
msgstr "å°†chatglmæ¨¡å‹ç›®å½•ä¿®æ”¹ä¸º\"chatglm2-6b\""

#: ../../getting_started/install/deploy.rst:205
#: 81a8983e89c4400abc20080c86865c02
msgid "Other LLM API"
msgstr ""

#: ../../getting_started/install/deploy.rst:219
#: 3df3d5e0e4f64813a7ca2adccd056c4a
msgid "Now DB-GPT support LLM REST API TYPE:"
msgstr "ç›®å‰DB-GPTæ”¯æŒçš„å¤§æ¨¡å‹REST APIç±»å‹:"

#: ../../getting_started/install/deploy.rst:224
#: 4f7cb7a3f324410d8a34ba96d7c9536a
msgid "Azure"
msgstr ""

#: ../../getting_started/install/deploy.rst:225
#: ee6cbe48a8094637b6b8a049446042c6
msgid "Aliyun tongyi"
msgstr ""

#: ../../getting_started/install/deploy.rst:226
#: a3d9c061b0f440258be88cc3143b1887
msgid "Baidu wenxin"
msgstr ""

#: ../../getting_started/install/deploy.rst:227
#: 176001fb6eaf448f9eca1abd256b3ef4
msgid "Zhipu"
msgstr ""

#: ../../getting_started/install/deploy.rst:229
#: 17fecc140be2474ea1566d44641a27c0
msgid "Bard"
msgstr ""

#: ../../getting_started/install/deploy.rst:284
#: bc34832cec754984a40c4d01526f0f83
msgid "llama.cpp"
msgstr ""

#: ../../getting_started/install/deploy.rst:286
#: aa4b14e726dc447f92dec48f38a3d770
msgid ""
"DB-GPT already supports `llama.cpp "
"<https://github.com/ggerganov/llama.cpp>`_ via `llama-cpp-python "
"<https://github.com/abetlen/llama-cpp-python>`_ ."
msgstr ""
"DB-GPT å·²ç»æ”¯æŒäº† `llama.cpp <https://github.com/ggerganov/llama.cpp>`_ via "
"`llama-cpp-python <https://github.com/abetlen/llama-cpp-python>`_ ."

#: ../../getting_started/install/deploy.rst:288
#: e97a868ad05b48ee8d25017457c1b7ee
msgid "**Preparing Model Files**"
msgstr "**å‡†å¤‡Modelæ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:290
#: 8efc7824082545a4a98ebbd6b12fa00f
msgid ""
"To use llama.cpp, you need to prepare a gguf format model file, and there"
" are two common ways to obtain it, you can choose either:"
msgstr "ä½¿ç”¨ llama.cppï¼Œä½ éœ€è¦å‡†å¤‡ gguf æ ¼å¼çš„æ–‡ä»¶ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹æ³•è·å–"

#: ../../getting_started/install/deploy.rst:292
#: 5b9f3a35779c4f89a4b9051904c5f7d8
msgid "**1. Download a pre-converted model file.**"
msgstr "**1.ä¸‹è½½å·²è½¬æ¢çš„æ¨¡å‹æ–‡ä»¶.**"

#: ../../getting_started/install/deploy.rst:294
#: e15f33a2e5fc4caaa9ad2e10cabf9d32
msgid ""
"Suppose you want to use [Vicuna 13B v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5), you can download the file already converted from "
"[TheBloke/vicuna-13B-v1.5-GGUF](https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF), only one file is needed. Download it to the `models` "
"directory and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"å‡è®¾æ‚¨æƒ³ä½¿ç”¨[Vicuna 13B v1.5](https://huggingface.co/lmsys/vicuna-"
"13b-v1.5)æ‚¨å¯ä»¥ä»[TheBloke/vicuna-"
"13B-v1.5-GGUF](https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF)ä¸‹è½½å·²è½¬æ¢çš„æ–‡ä»¶ï¼Œåªéœ€è¦ä¸€ä¸ªæ–‡ä»¶ã€‚å°†å…¶ä¸‹è½½åˆ°modelsç›®å½•å¹¶å°†å…¶é‡å‘½åä¸º `ggml-"
"model-q4_0.gguf`ã€‚"

#: ../../getting_started/install/deploy.rst:300
#: 81ab603cd9b94cabb48add9541590b8b
msgid "**2. Convert It Yourself**"
msgstr "**2. è‡ªè¡Œè½¬æ¢**"

#: ../../getting_started/install/deploy.rst:302
#: f9b46aa70a0842d6acc7e837c8b27778
msgid ""
"You can convert the model file yourself according to the instructions in "
"[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run), and put the converted file in the models directory "
"and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"æ‚¨å¯ä»¥æ ¹æ®[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run)ä¸­çš„è¯´æ˜è‡ªè¡Œè½¬æ¢æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶æŠŠè½¬æ¢åçš„æ–‡ä»¶æ”¾åœ¨modelsç›®å½•ä¸­ï¼Œå¹¶é‡å‘½åä¸º`ggml-"
"model-q4_0.gguf`ã€‚"

#: ../../getting_started/install/deploy.rst:304
#: 5278c05568dd4ad688bcc181e45caa18
msgid "**Installing Dependencies**"
msgstr "**å®‰è£…ä¾èµ–**"

#: ../../getting_started/install/deploy.rst:306
#: bd67bf651a9a4096a37e73851b5fad98
msgid ""
"llama.cpp is an optional dependency in DB-GPT, and you can manually "
"install it using the following command:"
msgstr "llama.cppåœ¨DB-GPTä¸­æ˜¯å¯é€‰å®‰è£…é¡¹, ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…"

#: ../../getting_started/install/deploy.rst:313
#: 71ed6ae4d0aa4d3ca27ebfffc42a5095
msgid "**3.Modifying the Configuration File**"
msgstr "**3.ä¿®æ”¹é…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:315
#: a0a2fdb799524ef4938df2c306536e2a
msgid "Next, you can directly modify your `.env` file to enable llama.cpp."
msgstr "ä¿®æ”¹`.env`æ–‡ä»¶ä½¿ç”¨llama.cpp"

#: ../../getting_started/install/deploy.rst:322
#: ../../getting_started/install/deploy.rst:390
#: 03694cbb7dfa432cb32116925357f008
msgid ""
"Then you can run it according to `Run <https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html#run>`_"
msgstr ""
"ç„¶åä½ å¯ä»¥æ ¹æ®[è¿è¡Œ](https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-"
"cn/zh_CN/latest/getting_started/install/deploy/deploy.html#run)æ¥è¿è¡Œ"

#: ../../getting_started/install/deploy.rst:325
#: c9bbc15ea2fc4562bdf87a4e099600a0
msgid "**More Configurations**"
msgstr "**æ›´å¤šé…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:327
#: 9beff887d8554d1dac169de93625c1b2
msgid ""
"In DB-GPT, the model configuration can be done through  `{model "
"name}_{config key}`."
msgstr "åœ¨DB-GPTä¸­ï¼Œæ¨¡å‹é…ç½®å¯ä»¥é€šè¿‡`{æ¨¡å‹åç§°}_{é…ç½®å}` æ¥é…ç½®ã€‚"

#: ../../getting_started/install/deploy.rst:329
#: 6cad64bcc9f141de85479203b21f7f9e
msgid "More Configurations"
msgstr "**æ›´å¤šé…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:333
#: de97b0d5b59c4bfabbd47b7e1766cc70
msgid "Environment Variable Key"
msgstr "ç¯å¢ƒå˜é‡Key"

#: ../../getting_started/install/deploy.rst:334
#: 76afa3655f644576b54b4422bad3fcad
msgid "Default"
msgstr "é»˜è®¤å€¼"

#: ../../getting_started/install/deploy.rst:335
#: 0dcabfeeb4874e08ba53d4b4aa6a1d73
msgid "Description"
msgstr "æè¿°"

#: ../../getting_started/install/deploy.rst:336
#: c1670aec27cf4c068d825456817d7667
msgid "llama_cpp_prompt_template"
msgstr ""

#: ../../getting_started/install/deploy.rst:337
#: ../../getting_started/install/deploy.rst:340
#: ../../getting_started/install/deploy.rst:346
#: ../../getting_started/install/deploy.rst:352
#: ../../getting_started/install/deploy.rst:358
#: 49c8f875066f49c6aac195805f88e4a9
msgid "None"
msgstr ""

#: ../../getting_started/install/deploy.rst:338
#: bb31e19533e5434b832d5975bf579dc3
msgid ""
"Prompt template name, now support: zero_shot, vicuna_v1.1,alpaca,llama-2"
",baichuan-chat,internlm-chat, If None, the prompt template is "
"automatically determined from model pathã€‚"
msgstr ""
"Prompt template ç°åœ¨å¯ä»¥æ”¯æŒ`zero_shot, vicuna_v1.1,alpaca,llama-2,baichuan-"
"chat,internlm-chat`, å¦‚æœæ˜¯None, å¯ä»¥æ ¹æ®æ¨¡å‹è·¯å¾„æ¥è‡ªåŠ¨è·å–æ¨¡å‹ Prompt template"

#: ../../getting_started/install/deploy.rst:339
#: 38ed23e19e2a47cf8fbacb035cfe1292
msgid "llama_cpp_model_path"
msgstr ""

#: ../../getting_started/install/deploy.rst:341
#: eb069fdfdfeb4b17b723ee6733ba50c2
msgid "Model path"
msgstr "æ¨¡å‹è·¯å¾„"

#: ../../getting_started/install/deploy.rst:342
#: 348d3ff3e4f44ceb99aadead11f5cca5
msgid "llama_cpp_n_gpu_layers"
msgstr ""

#: ../../getting_started/install/deploy.rst:343
#: 007deecf593b4553b6ca8df3e9240a28
msgid "1000000000"
msgstr ""

#: ../../getting_started/install/deploy.rst:344
#: 4da435c417f0482581895bf4d052a6a1
msgid ""
"Number of layers to offload to the GPU, Set this to 1000000000 to offload"
" all layers to the GPU. If your GPU VRAM is not enough, you can set a low"
" number, eg: 10"
msgstr "è¦å°†å¤šå°‘ç½‘ç»œå±‚è½¬ç§»åˆ°GPUä¸Šï¼Œå°†å…¶è®¾ç½®ä¸º1000000000ä»¥å°†æ‰€æœ‰å±‚è½¬ç§»åˆ°GPUä¸Šã€‚å¦‚æœæ‚¨çš„ GPU å†…å­˜ä¸è¶³ï¼Œå¯ä»¥è®¾ç½®è¾ƒä½çš„æ•°å­—ï¼Œä¾‹å¦‚ï¼š10ã€‚"

#: ../../getting_started/install/deploy.rst:345
#: 4f186505ea81467590cf817d116e6879
msgid "llama_cpp_n_threads"
msgstr ""

#: ../../getting_started/install/deploy.rst:347
#: b94dc2fb2cd14a4a8f38ba95b05fbb5b
msgid ""
"Number of threads to use. If None, the number of threads is automatically"
" determined"
msgstr "è¦ä½¿ç”¨çš„çº¿ç¨‹æ•°é‡ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™çº¿ç¨‹æ•°é‡å°†è‡ªåŠ¨ç¡®å®šã€‚"

#: ../../getting_started/install/deploy.rst:348
#: 867f2357430440eba4e749a8a39bff18
msgid "llama_cpp_n_batch"
msgstr ""

#: ../../getting_started/install/deploy.rst:349
#: 78516f7b23264147bae11e13426097eb
msgid "512"
msgstr ""

#: ../../getting_started/install/deploy.rst:350
#: c2ec97ffef3e4a70afef6634e78801a2
msgid "Maximum number of prompt tokens to batch together when calling llama_eval"
msgstr "åœ¨è°ƒç”¨llama_evalæ—¶ï¼Œæ‰¹å¤„ç†åœ¨ä¸€èµ·çš„prompt tokensçš„æœ€å¤§æ•°é‡"

#: ../../getting_started/install/deploy.rst:351
#: 29ad3c5ab7134da49d6fca3b42d734d6
msgid "llama_cpp_n_gqa"
msgstr ""

#: ../../getting_started/install/deploy.rst:353
#: e5bc322b0c824465b1adbd162838a3b7
msgid "Grouped-query attention. Must be 8 for llama-2 70b."
msgstr "å¯¹äº llama-2 70B æ¨¡å‹ï¼ŒGrouped-query attention å¿…é¡»ä¸º8ã€‚"

#: ../../getting_started/install/deploy.rst:354
#: 43e7d412238a4f0ba8227938d9fa4172
msgid "llama_cpp_rms_norm_eps"
msgstr ""

#: ../../getting_started/install/deploy.rst:355
#: 6328db04645b4b089593291e2ca13f79
msgid "5e-06"
msgstr ""

#: ../../getting_started/install/deploy.rst:356
#: 6c8bf631cece42fa86954f5cf2d75503
msgid "5e-6 is a good value for llama-2 models."
msgstr "å¯¹äºllama-2æ¨¡å‹æ¥è¯´ï¼Œ5e-6æ˜¯ä¸€ä¸ªä¸é”™çš„å€¼ã€‚"

#: ../../getting_started/install/deploy.rst:357
#: 22ebee39fe8b4cc18378447cac67e631
msgid "llama_cpp_cache_capacity"
msgstr ""

#: ../../getting_started/install/deploy.rst:359
#: 4dc98bede90f4237829f65513e4adf61
msgid "Maximum cache capacity. Examples: 2000MiB, 2GiB"
msgstr "æ¨¡å‹ç¼“å­˜æœ€å¤§å€¼. ä¾‹å¦‚: 2000MiB, 2GiB"

#: ../../getting_started/install/deploy.rst:360
#: bead617b5af943dab0bc1209823d3c22
msgid "llama_cpp_prefer_cpu"
msgstr ""

#: ../../getting_started/install/deploy.rst:361
#: 8efb44f1cfc54b1a8b3ca8ea138113ee
msgid "False"
msgstr ""

#: ../../getting_started/install/deploy.rst:362
#: b854e6a44c8348ceb72ac382b73ceec5
msgid ""
"If a GPU is available, it will be preferred by default, unless "
"prefer_cpu=False is configured."
msgstr "å¦‚æœæœ‰å¯ç”¨çš„GPUï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šä¼˜å…ˆä½¿ç”¨GPUï¼Œé™¤éé…ç½®äº† prefer_cpu=Falseã€‚"

#: ../../getting_started/install/deploy.rst:365
#: 41b0440eb0074e5fa8c87dd404efe0ce
msgid "vllm"
msgstr ""

#: ../../getting_started/install/deploy.rst:367
#: 6fc445675bf74b2ea2fe0c7f2a64db69
msgid "vLLM is a fast and easy-to-use library for LLM inference and serving."
msgstr "\"vLLM æ˜¯ä¸€ä¸ªå¿«é€Ÿä¸”æ˜“äºä½¿ç”¨çš„ LLM æ¨ç†å’ŒæœåŠ¡çš„åº“ã€‚"

#: ../../getting_started/install/deploy.rst:369
#: aab5f913d29445c4b345180b8793ebc7
msgid "**Running vLLM**"
msgstr "**è¿è¡ŒvLLM**"

#: ../../getting_started/install/deploy.rst:371
#: a9560d697821404cb0abdabf9f479645
msgid "**1.Installing Dependencies**"
msgstr "**1.å®‰è£…ä¾èµ–**"

#: ../../getting_started/install/deploy.rst:373
#: b008224d4f314e5c988335262c95a42e
msgid ""
"vLLM is an optional dependency in DB-GPT, and you can manually install it"
" using the following command:"
msgstr "vLLM åœ¨ DB-GPT æ˜¯ä¸€ä¸ªå¯é€‰ä¾èµ–, ä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æ‰‹åŠ¨å®‰è£…å®ƒï¼š"

#: ../../getting_started/install/deploy.rst:379
#: 7dc0c8e996124177935a4e0d9ef19837
msgid "**2.Modifying the Configuration File**"
msgstr "**2.ä¿®æ”¹é…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:381
#: 49667576b44c46bf87d5bf4d207dd63a
msgid "Next, you can directly modify your .env file to enable vllm."
msgstr "ä½ å¯ä»¥ç›´æ¥ä¿®æ”¹ä½ çš„ `.env` æ–‡ä»¶"

#: ../../getting_started/install/deploy.rst:388
#: f16a78f0ee6545babab0d66f12654c0a
msgid ""
"You can view the models supported by vLLM `here "
"<https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models>`_"
msgstr ""
"ä½ å¯ä»¥åœ¨ "
"[è¿™é‡Œ](https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models) æŸ¥çœ‹ vLLM æ”¯æŒçš„æ¨¡å‹ã€‚"

#: ../../getting_started/install/deploy.rst:397
#: f7ae366723e9494d8177eeb963ba0ed9
msgid "3.Prepare sql example(Optional)"
msgstr "3.å‡†å¤‡ sql example(å¯é€‰)"

#: ../../getting_started/install/deploy.rst:398
#: d302aac8ddb346598fc9d73e0f6c2cbc
msgid "**(Optional) load examples into SQLite**"
msgstr "**(å¯é€‰) load examples into SQLite**"

#: ../../getting_started/install/deploy.rst:405
#: ae2a315e40854f27a074f2c0f2506014
msgid "On windows platform:"
msgstr ""

#: ../../getting_started/install/deploy.rst:412
#: 36b4b7a813844af3a4ec8062b39059a3
msgid "4.Run db-gpt server"
msgstr "4.è¿è¡Œdb-gpt server"

#: ../../getting_started/install/deploy.rst:418
#: e56f15e563484027b7efb2147c9b71a7
msgid "**Open http://localhost:5000 with your browser to see the product.**"
msgstr "æ‰“å¼€æµè§ˆå™¨è®¿é—®http://localhost:5000"

#~ msgid ""
#~ "DB-GPT can be deployed on servers"
#~ " with low hardware requirements or on"
#~ " servers with high hardware requirements."
#~ " You can install DB-GPT by "
#~ "Using third-part LLM REST API "
#~ "Service OpenAI, Azure."
#~ msgstr ""

#~ msgid ""
#~ "And you can also install DB-GPT"
#~ " by deploy LLM Service by download"
#~ " LLM model."
#~ msgstr ""

