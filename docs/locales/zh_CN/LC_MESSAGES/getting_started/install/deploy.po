# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT ğŸ‘ğŸ‘ 0.4.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-11-06 19:38+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy.rst:4 f3ea3305f122460aaa11999edc4b5de6
msgid "Installation From Source"
msgstr "æºç å®‰è£…"

#: ../../getting_started/install/deploy.rst:6 bb941f2bd56d4eb48f7c4f75ebd74176
msgid "To get started, install DB-GPT with the following steps."
msgstr "æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œå®‰è£…"

#: ../../getting_started/install/deploy.rst:10 27a1e092c1f945ceb9946ebdaf89b600
msgid "1.Preparation"
msgstr "1.å‡†å¤‡"

#: ../../getting_started/install/deploy.rst:11 5c5bfbdc74a14c3b9b1f1ed66617cac8
msgid "**Download DB-GPT**"
msgstr "**ä¸‹è½½DB-GPTé¡¹ç›®**"

#: ../../getting_started/install/deploy.rst:17 3065ee2f34f9417598a37fd699a4863e
msgid "**Install Miniconda**"
msgstr "**å®‰è£…Miniconda**"

#: ../../getting_started/install/deploy.rst:19 f9f3a653ffb8447284686aa37a7bb79a
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration. For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. `How to install Miniconda "
"<https://docs.conda.io/en/latest/miniconda.html>`_"
msgstr ""
"ç›®å‰ä½¿ç”¨Sqliteä½œä¸ºé»˜è®¤æ•°æ®åº“ï¼Œå› æ­¤DB-"
"GPTå¿«é€Ÿéƒ¨ç½²ä¸éœ€è¦éƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–æ•°æ®åº“ï¼Œéœ€è¦å…ˆéƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚æˆ‘ä»¬ç›®å‰ä½¿ç”¨Minicondaè¿›è¡Œpythonç¯å¢ƒå’ŒåŒ…ä¾èµ–ç®¡ç†ã€‚`å¦‚ä½•å®‰è£…"
" Miniconda <https://docs.conda.io/en/latest/miniconda.html>`_ ã€‚"

#: ../../getting_started/install/deploy.rst:36 a2cd2fdd1d16421f9cbe341040b153b6
msgid "2.Deploy LLM Service"
msgstr "2.éƒ¨ç½²LLMæœåŠ¡"

#: ../../getting_started/install/deploy.rst:37 180a121e3c994a92a917ace80bf12386
msgid ""
"DB-GPT can be deployed on servers with low hardware requirements or on "
"servers with high hardware requirements."
msgstr "DB-GPTå¯ä»¥éƒ¨ç½²åœ¨å¯¹ç¡¬ä»¶è¦æ±‚ä¸é«˜çš„æœåŠ¡å™¨ï¼Œä¹Ÿå¯ä»¥éƒ¨ç½²åœ¨å¯¹ç¡¬ä»¶è¦æ±‚é«˜çš„æœåŠ¡å™¨"

#: ../../getting_started/install/deploy.rst:39 395608515c0348d5849030b58da7b659
msgid ""
"If you are low hardware requirements you can install DB-GPT by Using "
"third-part LLM REST API Service OpenAI, Azure, tongyi."
msgstr "ä½ç¡¬ä»¶è¦æ±‚æ¨¡å¼é€‚ç”¨äºå¯¹æ¥ç¬¬ä¸‰æ–¹æ¨¡å‹æœåŠ¡çš„ APIï¼Œæ¯”å¦‚ OpenAIã€é€šä¹‰åƒé—®ã€ æ–‡å¿ƒä¸€è¨€ç­‰ã€‚"

#: ../../getting_started/install/deploy.rst:43 e29297e61e2e4d05ba88f0e1c2b1f365
msgid "As our project has the ability to achieve OpenAI performance of over 85%,"
msgstr "ä½¿ç”¨OpenAIæœåŠ¡å¯ä»¥è®©DB-GPTå‡†ç¡®ç‡è¾¾åˆ°85%"

#: ../../getting_started/install/deploy.rst:48 d0d70d51e8684c2891c58a6da4941a52
msgid "Notice make sure you have install git-lfs"
msgstr "ç¡®è®¤æ˜¯å¦å·²ç»å®‰è£…git-lfs"

#: ../../getting_started/install/deploy.rst:50 0d2781fd38eb467ebad2a3c310a344e6
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:52 1574ea24ad6443409070aa3a1f7abe87
msgid "ubuntu:apt-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:54 ad86473d5c87447091c713f45cbfed0e
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:58
#: ../../getting_started/install/deploy.rst:229
#: 3dd1e40f33924faab63634907a7f6511 dce32420face4ab2b99caf7f3900ede9
msgid "OpenAI"
msgstr "OpenAI"

#: ../../getting_started/install/deploy.rst:60 1f66400540114de2820761ef80137805
msgid "Installing Dependencies"
msgstr "å®‰è£…ä¾èµ–"

#: ../../getting_started/install/deploy.rst:66
#: ../../getting_started/install/deploy.rst:213
#: 31b856a6fc094334a37914c046cb1bb1 42b2f6d36ca4487f8e31d59bba123fca
msgid "Download embedding model"
msgstr "ä¸‹è½½ embedding æ¨¡å‹"

#: ../../getting_started/install/deploy.rst:78 f970fb69e47c40d7bda381ec6f045829
msgid "Configure LLM_MODEL, PROXY_API_URL and API_KEY in `.env` file"
msgstr "åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½® LLM_MODELã€PROXY_API_URL å’Œ API_KEY"

#: ../../getting_started/install/deploy.rst:88
#: ../../getting_started/install/deploy.rst:288
#: 6ca04c88fc60480db2ebdc9b234a0bbb 709cfe74c45c4eff83a7d77bb30b4a2b
msgid "Make sure your .env configuration is not overwritten"
msgstr "ç¡®ä¿ä½ çš„ .env æ–‡ä»¶ä¸ä¼šè¢«è¦†ç›–"

#: ../../getting_started/install/deploy.rst:91 147aea0d753f44588f4a0c56002334ab
msgid "Vicuna"
msgstr "Vicuna"

#: ../../getting_started/install/deploy.rst:92 6a0bd60c4ca2478cb0f3d85aff70cd3b
msgid ""
"`Vicuna-v1.5 <https://huggingface.co/lmsys/vicuna-13b-v1.5>`_ based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"åŸºäº llama-2 çš„æ¨¡å‹ `Vicuna-v1.5 <https://huggingface.co/lmsys/vicuna-"
"13b-v1.5>`_ å·²ç»å‘å¸ƒï¼Œæˆ‘ä»¬æ¨èä½ é€šè¿‡é…ç½® `LLM_MODEL=vicuna-13b-v1.5` æ¥å°è¯•è¿™ä¸ªæ¨¡å‹"

#: ../../getting_started/install/deploy.rst:94 6a111c2ef31f41d4b737cf8b6f36fb16
msgid "vicuna-v1.5 hardware requirements"
msgstr "vicuna-v1.5 çš„ç¡¬ä»¶è¦æ±‚"

#: ../../getting_started/install/deploy.rst:98
#: ../../getting_started/install/deploy.rst:143
#: dc24c0238ce141df8bdce26cc0e2ddbb e04f1ea4b36940f3a28b66cdff7b702e
msgid "Model"
msgstr "æ¨¡å‹"

#: ../../getting_started/install/deploy.rst:99
#: ../../getting_started/install/deploy.rst:144
#: b6473e65ca1a437a84226531be4da26d e0a2f7580685480aa13ca462418764d3
msgid "Quantize"
msgstr "é‡åŒ–"

#: ../../getting_started/install/deploy.rst:100
#: ../../getting_started/install/deploy.rst:145
#: 56471c3b174d4adf9e8cb5bebaa300a6 d82297b8b9c148c3906d8ee4ed10d8a0
msgid "VRAM Size"
msgstr "æ˜¾å­˜"

#: ../../getting_started/install/deploy.rst:101
#: ../../getting_started/install/deploy.rst:104
#: 1214432602fe47a28479ce3e21a7d88b 51838e72e42248f199653f1bf08c8155
msgid "vicuna-7b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:102
#: ../../getting_started/install/deploy.rst:108
#: ../../getting_started/install/deploy.rst:147
#: ../../getting_started/install/deploy.rst:153
#: a64439f4e6f64c42bb76fbb819556784 ed95f498641e4a0f976318df608a1d67
#: fc400814509048b4a1cbe1e07c539285 ff7a8cb2cce8438cb6cb0d80dabfc2b5
msgid "4-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:103
#: ../../getting_started/install/deploy.rst:148
#: 2726e8a278c34e6db59147e9f66f2436 5feab5755a41403c9d641da697de4651
msgid "8 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:105
#: ../../getting_started/install/deploy.rst:111
#: ../../getting_started/install/deploy.rst:150
#: ../../getting_started/install/deploy.rst:156
#: 1984406682da4da3ad7b275e44085d07 2f027d838d0c46409e54c066d7983aae
#: 5c5878fe64944872b6769f075fedca05 e2507408a9c5423988e17b7029b487e4
msgid "8-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:106
#: ../../getting_started/install/deploy.rst:109
#: ../../getting_started/install/deploy.rst:151
#: ../../getting_started/install/deploy.rst:154
#: 332f50702c7b46e79ea0af5cbf86c6d5 381d23253cfd40109bacefca6a179f91
#: aafe2423c25546e789e4804e3fd91d1d cc56990a58e941d6ba023cbd4dca0357
msgid "12 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:107
#: ../../getting_started/install/deploy.rst:110
#: 1f14e2fa6d41493cb208f55eddff9773 6457f6307d8546beb5f2fb69c30922d8
msgid "vicuna-13b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:112
#: ../../getting_started/install/deploy.rst:157
#: e24d3a36b5ce4cfe861dce2d1c4db592 f2e66b2da7954aaab0ee526b25a371f5
msgid "20 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:128
#: ../../getting_started/install/deploy.rst:175
#: ../../getting_started/install/deploy.rst:201
#: 1719c11f92874c47a87c00c634b9fad8 4596fcbe415d42fdbb29b92964fae070
#: e639ae6076a64b7b9de08527966e4550
msgid "The model files are large and will take a long time to download."
msgstr "è¿™ä¸ªæ¨¡å‹æƒé‡æ–‡ä»¶æ¯”è¾ƒå¤§ï¼Œéœ€è¦èŠ±è´¹è¾ƒé•¿æ—¶é—´æ¥ä¸‹è½½ã€‚"

#: ../../getting_started/install/deploy.rst:130
#: ../../getting_started/install/deploy.rst:177
#: ../../getting_started/install/deploy.rst:203
#: 4ec1492d389f403ebd9dd805fcaac68e ac6c68e2bf9b47c694ea8e0506014b10
#: e39be72282e64760903aaba45f8effb8
msgid "**Configure LLM_MODEL in `.env` file**"
msgstr "**åœ¨ `.env` æ–‡ä»¶ä¸­é…ç½® LLM_MODEL**"

#: ../../getting_started/install/deploy.rst:137
#: ../../getting_started/install/deploy.rst:234
#: 7ce4e2253ef24a7ea890ade04ce36682 b9d5bf4fa09649c4a098503132ce7c0c
msgid "Baichuan"
msgstr "ç™¾å·"

#: ../../getting_started/install/deploy.rst:139
#: ffdad6a70558457fa825bad4d811100d
msgid "Baichuan hardware requirements"
msgstr "ç™¾å· çš„ç¡¬ä»¶è¦æ±‚"

#: ../../getting_started/install/deploy.rst:146
#: ../../getting_started/install/deploy.rst:149
#: 59d9b64f54d34971a68e93e3101def06 a66ce354d8f143ce920303241cd8947e
msgid "baichuan-7b"
msgstr ""

#: ../../getting_started/install/deploy.rst:152
#: ../../getting_started/install/deploy.rst:155
#: c530662259ca4ec5b03a18e4b690e17a fa3af65ecca54daab961f55729bbc40e
msgid "baichuan-13b"
msgstr ""

#: ../../getting_started/install/deploy.rst:179
#: efd73637994a4b7c97ef3557e1f3161c
msgid "please rename Baichuan path to \"baichuan2-13b\" or \"baichuan2-7b\""
msgstr "å°†Baichuanæ¨¡å‹ç›®å½•ä¿®æ”¹ä¸º\"baichuan2-13b\" æˆ– \"baichuan2-7b\""

#: ../../getting_started/install/deploy.rst:185
#: 435a3f0d0fe84b49a7305e2c0f51a5df
msgid "ChatGLM"
msgstr ""

#: ../../getting_started/install/deploy.rst:205
#: 165e23d3d40d4756b5a6a2580d015213
msgid "please rename chatglm model path to \"chatglm2-6b\""
msgstr "å°† chatglm æ¨¡å‹ç›®å½•ä¿®æ”¹ä¸º\"chatglm2-6b\""

#: ../../getting_started/install/deploy.rst:211
#: b651ebb5e0424b8992bc8b49d2280bee
msgid "Other LLM API"
msgstr "å…¶å®ƒæ¨¡å‹ API"

#: ../../getting_started/install/deploy.rst:225
#: 4eabdc25f4a34676b3ece620c88d866f
msgid "Now DB-GPT support LLM REST API TYPE:"
msgstr "ç›®å‰DB-GPTæ”¯æŒçš„å¤§æ¨¡å‹ REST API ç±»å‹:"

#: ../../getting_started/install/deploy.rst:230
#: d361963cc3404e5ca55a823f1f1f545c
msgid "Azure"
msgstr ""

#: ../../getting_started/install/deploy.rst:231
#: 3b0f17c74aaa4bbd9db935973fa1c36b
msgid "Aliyun tongyi"
msgstr ""

#: ../../getting_started/install/deploy.rst:232
#: 7c4c457a499943b8804e31046551006d
msgid "Baidu wenxin"
msgstr ""

#: ../../getting_started/install/deploy.rst:233
#: ac1880a995184295acf07fff987d7c56
msgid "Zhipu"
msgstr ""

#: ../../getting_started/install/deploy.rst:235
#: 6927500d7d3445b7b1981da1df4e1666
msgid "Bard"
msgstr ""

#: ../../getting_started/install/deploy.rst:237
#: 419d564de18c485780d9336b852735b6
msgid "Configure LLM_MODEL and PROXY_API_URL and API_KEY in `.env` file"
msgstr "åœ¨`.env`æ–‡ä»¶è®¾ç½® LLM_MODELã€PROXY_API_URLå’Œ API_KEY"

#: ../../getting_started/install/deploy.rst:290
#: 71d5203682e24e2e896e4b9913471f78
msgid "llama.cpp"
msgstr ""

#: ../../getting_started/install/deploy.rst:292
#: 36a2b82f711a4c0f9491aca9c84d3c91
msgid ""
"DB-GPT already supports `llama.cpp "
"<https://github.com/ggerganov/llama.cpp>`_ via `llama-cpp-python "
"<https://github.com/abetlen/llama-cpp-python>`_ ."
msgstr ""
"DB-GPT å·²ç»é€šè¿‡ `llama-cpp-python <https://github.com/abetlen/llama-cpp-"
"python>`_ æ”¯æŒäº† `llama.cpp <https://github.com/ggerganov/llama.cpp>`_ ã€‚"

#: ../../getting_started/install/deploy.rst:294
#: 439064115dca4ae08d8e60041f2ffe17
msgid "**Preparing Model Files**"
msgstr "**å‡†å¤‡æ¨¡å‹æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:296
#: 7291d6fa20b34942926e7765c01f25c9
msgid ""
"To use llama.cpp, you need to prepare a gguf format model file, and there"
" are two common ways to obtain it, you can choose either:"
msgstr "ä¸ºäº†ä½¿ç”¨ llama.cppï¼Œä½ éœ€è¦å‡†å¤‡ gguf æ ¼å¼çš„æ–‡ä»¶ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹æ³•è·å–"

#: ../../getting_started/install/deploy.rst:298
#: 45752f3f5dd847469da0c5edddc530fa
msgid "**1. Download a pre-converted model file.**"
msgstr "**1.ä¸‹è½½å·²è½¬æ¢çš„æ¨¡å‹æ–‡ä»¶.**"

#: ../../getting_started/install/deploy.rst:300
#: c451db2157ff49b2b4992aed9907ddfa
msgid ""
"Suppose you want to use `Vicuna 13B v1.5 <https://huggingface.co/lmsys"
"/vicuna-13b-v1.5>`_ , you can download the file already converted from "
"`TheBloke/vicuna-13B-v1.5-GGUF <https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF>`_ , only one file is needed. Download it to the `models` "
"directory and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"å‡è®¾æ‚¨æƒ³ä½¿ç”¨ `Vicuna 13B v1.5 <https://huggingface.co/lmsys/vicuna-"
"13b-v1.5>`_ æ‚¨å¯ä»¥ä» `TheBloke/vicuna-"
"13B-v1.5-GGUF <https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF>`_ ä¸‹è½½å·²è½¬æ¢çš„æ–‡ä»¶ï¼Œåªéœ€è¦ä¸€ä¸ªæ–‡ä»¶ã€‚å°†å…¶ä¸‹è½½åˆ°modelsç›®å½•å¹¶å°†å…¶é‡å‘½åä¸º `ggml-"
"model-q4_0.gguf`ã€‚"

#: ../../getting_started/install/deploy.rst:306
#: f5b92b51622b43d398b3dc13a5892c29
msgid "**2. Convert It Yourself**"
msgstr "**2. è‡ªè¡Œè½¬æ¢**"

#: ../../getting_started/install/deploy.rst:308
#: 8838ae6dcecf44ecad3fd963980c8eb3
msgid ""
"You can convert the model file yourself according to the instructions in "
"`llama.cpp#prepare-data--run <https://github.com/ggerganov/llama.cpp"
"#prepare-data--run>`_ , and put the converted file in the models "
"directory and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"æ‚¨å¯ä»¥æ ¹æ® `llama.cpp#prepare-data--run <https://github.com/ggerganov/llama.cpp"
"#prepare-data--run>`_ ä¸­çš„è¯´æ˜è‡ªè¡Œè½¬æ¢æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶æŠŠè½¬æ¢åçš„æ–‡ä»¶æ”¾åœ¨modelsç›®å½•ä¸­ï¼Œå¹¶é‡å‘½åä¸º`ggml-"
"model-q4_0.gguf`ã€‚"

#: ../../getting_started/install/deploy.rst:310
#: 3fe28d6e5eaa4bdf9c5c44a914c3577c
msgid "**Installing Dependencies**"
msgstr "**å®‰è£…ä¾èµ–**"

#: ../../getting_started/install/deploy.rst:312
#: bdc10d2e88cc4c3f84a8c4a8dc2037a9
msgid ""
"llama.cpp is an optional dependency in DB-GPT, and you can manually "
"install it using the following command:"
msgstr "llama.cppåœ¨DB-GPTä¸­æ˜¯å¯é€‰å®‰è£…é¡¹, ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…"

#: ../../getting_started/install/deploy.rst:319
#: 9c136493448b43b5b27f66af74ff721e
msgid "**3.Modifying the Configuration File**"
msgstr "**3.ä¿®æ”¹é…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:321
#: c835a7dee1dd409fb861e7b886c6dc5b
msgid "Next, you can directly modify your `.env` file to enable llama.cpp."
msgstr "ä¿®æ”¹`.env`æ–‡ä»¶ä½¿ç”¨llama.cpp"

#: ../../getting_started/install/deploy.rst:328
#: ../../getting_started/install/deploy.rst:396
#: 296e6d08409544918fee0c31b1bf195c a81e5d882faf4722b0e10d53f635f53c
msgid ""
"Then you can run it according to `Run <https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html#run>`_"
msgstr ""
"ç„¶åä½ å¯ä»¥æ ¹æ® `è¿è¡Œ <https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-"
"cn/zh_CN/latest/getting_started/install/deploy/deploy.html#run>`_ æ¥è¿è¡Œã€‚"

#: ../../getting_started/install/deploy.rst:331
#: 0f7f487ee11a4e01a95f7c504f0469ba
msgid "**More Configurations**"
msgstr "**æ›´å¤šé…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:333
#: b0f9964497f64fb5b3740099232cd72b
msgid ""
"In DB-GPT, the model configuration can be done through  `{model "
"name}_{config key}`."
msgstr "åœ¨DB-GPTä¸­ï¼Œæ¨¡å‹é…ç½®å¯ä»¥é€šè¿‡`{æ¨¡å‹åç§°}_{é…ç½®å}` æ¥é…ç½®ã€‚"

#: ../../getting_started/install/deploy.rst:335
#: 7c225de4fe9d4dd3a3c2b2a33802e656
msgid "More Configurations"
msgstr "**æ›´å¤šé…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:339
#: 5cc1671910314796a9ce0b5107d3c9fe
msgid "Environment Variable Key"
msgstr "ç¯å¢ƒå˜é‡Key"

#: ../../getting_started/install/deploy.rst:340
#: 4359ed4e11bb47ad89a605cbf9016cd5
msgid "Default"
msgstr "é»˜è®¤å€¼"

#: ../../getting_started/install/deploy.rst:341
#: 5cf0efc6d1014665bb9dbdae96bf2726
msgid "Description"
msgstr "æè¿°"

#: ../../getting_started/install/deploy.rst:342
#: e7c291f80a9a40fa90d642901eca02c6
msgid "llama_cpp_prompt_template"
msgstr ""

#: ../../getting_started/install/deploy.rst:343
#: ../../getting_started/install/deploy.rst:346
#: ../../getting_started/install/deploy.rst:352
#: ../../getting_started/install/deploy.rst:358
#: ../../getting_started/install/deploy.rst:364
#: 07dc7fc4e51e4d9faf8e5221bcf03ee0 549f3c57a2e9427880e457e653ce1182
#: 7ad961957f7b49d08e4aff347749b78d c1eab368175c4fa88fe0b471919523b2
#: e2e0bf9903484972b6d20e6837010029
msgid "None"
msgstr ""

#: ../../getting_started/install/deploy.rst:344
#: 6b5044a2009f432c92fcd65db42506d8
msgid ""
"Prompt template name, now support: zero_shot, vicuna_v1.1,alpaca,llama-2"
",baichuan-chat,internlm-chat, If None, the prompt template is "
"automatically determined from model pathã€‚"
msgstr ""
"Prompt template ç°åœ¨å¯ä»¥æ”¯æŒ`zero_shot, vicuna_v1.1,alpaca,llama-2,baichuan-"
"chat,internlm-chat`, å¦‚æœæ˜¯None, å¯ä»¥æ ¹æ®æ¨¡å‹è·¯å¾„æ¥è‡ªåŠ¨è·å–æ¨¡å‹ Prompt template"

#: ../../getting_started/install/deploy.rst:345
#: e01c860441ad43b88c0a8d012f97d2d8
msgid "llama_cpp_model_path"
msgstr ""

#: ../../getting_started/install/deploy.rst:347
#: 1cb68d772e454812a1a0c6de4950b8ce
msgid "Model path"
msgstr "æ¨¡å‹è·¯å¾„"

#: ../../getting_started/install/deploy.rst:348
#: 6dac03820edb4fbd8a0856405e84c5bc
msgid "llama_cpp_n_gpu_layers"
msgstr ""

#: ../../getting_started/install/deploy.rst:349
#: 8cd5607b7941427f9a342ca7a00e5778
msgid "1000000000"
msgstr ""

#: ../../getting_started/install/deploy.rst:350
#: 61c9297656da434aa7ac2b49cf61ea9d
msgid ""
"Number of layers to offload to the GPU, Set this to 1000000000 to offload"
" all layers to the GPU. If your GPU VRAM is not enough, you can set a low"
" number, eg: 10"
msgstr "è¦å°†å¤šå°‘ç½‘ç»œå±‚è½¬ç§»åˆ°GPUä¸Šï¼Œå°†å…¶è®¾ç½®ä¸º1000000000ä»¥å°†æ‰€æœ‰å±‚è½¬ç§»åˆ°GPUä¸Šã€‚å¦‚æœæ‚¨çš„ GPU å†…å­˜ä¸è¶³ï¼Œå¯ä»¥è®¾ç½®è¾ƒä½çš„æ•°å­—ï¼Œä¾‹å¦‚ï¼š10ã€‚"

#: ../../getting_started/install/deploy.rst:351
#: 8c2d2182557a483aa2fda590c24faaf3
msgid "llama_cpp_n_threads"
msgstr ""

#: ../../getting_started/install/deploy.rst:353
#: cc442f61ffc442ecbd98c1e7f5598e1a
msgid ""
"Number of threads to use. If None, the number of threads is automatically"
" determined"
msgstr "è¦ä½¿ç”¨çš„çº¿ç¨‹æ•°é‡ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™çº¿ç¨‹æ•°é‡å°†è‡ªåŠ¨ç¡®å®šã€‚"

#: ../../getting_started/install/deploy.rst:354
#: 8d5e917d86f048348106e6923638a0c2
msgid "llama_cpp_n_batch"
msgstr ""

#: ../../getting_started/install/deploy.rst:355
#: ee2719a0a8cd4a77846cffd8e675638f
msgid "512"
msgstr ""

#: ../../getting_started/install/deploy.rst:356
#: 845b354315384762a611ad2daa539d57
msgid "Maximum number of prompt tokens to batch together when calling llama_eval"
msgstr "åœ¨è°ƒç”¨llama_evalæ—¶ï¼Œæ‰¹å¤„ç†åœ¨ä¸€èµ·çš„prompt tokensçš„æœ€å¤§æ•°é‡"

#: ../../getting_started/install/deploy.rst:357
#: a95e788bfa5f46f3bcd6356dfd9f87eb
msgid "llama_cpp_n_gqa"
msgstr ""

#: ../../getting_started/install/deploy.rst:359
#: 23ad9b5f34b5440bb90b2b21bab25763
msgid "Grouped-query attention. Must be 8 for llama-2 70b."
msgstr "å¯¹äº llama-2 70B æ¨¡å‹ï¼ŒGrouped-query attention å¿…é¡»ä¸º8ã€‚"

#: ../../getting_started/install/deploy.rst:360
#: 9ce25b7966fc40ec8be47ecfaf5f9994
msgid "llama_cpp_rms_norm_eps"
msgstr ""

#: ../../getting_started/install/deploy.rst:361
#: 58365f0d36af447ba976213646018431
msgid "5e-06"
msgstr ""

#: ../../getting_started/install/deploy.rst:362
#: d00b742a759140b795ba5949f1ce9a36
msgid "5e-6 is a good value for llama-2 models."
msgstr "å¯¹äºllama-2æ¨¡å‹æ¥è¯´ï¼Œ5e-6æ˜¯ä¸€ä¸ªä¸é”™çš„å€¼ã€‚"

#: ../../getting_started/install/deploy.rst:363
#: b9972e9b19354f55a5e6d9c50513a620
msgid "llama_cpp_cache_capacity"
msgstr ""

#: ../../getting_started/install/deploy.rst:365
#: 3c98c5396dd74db8b6d70fc50fa0754f
msgid "Maximum cache capacity. Examples: 2000MiB, 2GiB"
msgstr "æ¨¡å‹ç¼“å­˜æœ€å¤§å€¼. ä¾‹å¦‚: 2000MiB, 2GiB"

#: ../../getting_started/install/deploy.rst:366
#: 4277e155992c4442b69d665d6269bed6
msgid "llama_cpp_prefer_cpu"
msgstr ""

#: ../../getting_started/install/deploy.rst:367
#: 049169c1210a4ecabb25702ed813ea0a
msgid "False"
msgstr ""

#: ../../getting_started/install/deploy.rst:368
#: 60a39e93e7874491a93893de78b7d37e
msgid ""
"If a GPU is available, it will be preferred by default, unless "
"prefer_cpu=False is configured."
msgstr "å¦‚æœæœ‰å¯ç”¨çš„GPUï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šä¼˜å…ˆä½¿ç”¨GPUï¼Œé™¤éé…ç½®äº† prefer_cpu=Falseã€‚"

#: ../../getting_started/install/deploy.rst:371
#: 7c86780fbf634de8873afd439389cf89
msgid "vllm"
msgstr ""

#: ../../getting_started/install/deploy.rst:373
#: e2827892e43d420c85b8b83c4855d197
msgid "vLLM is a fast and easy-to-use library for LLM inference and serving."
msgstr "vLLM æ˜¯ä¸€ä¸ªå¿«é€Ÿä¸”æ˜“äºä½¿ç”¨çš„ LLM æ¨ç†å’ŒæœåŠ¡çš„åº“ã€‚"

#: ../../getting_started/install/deploy.rst:375
#: 81bbfa3876a74244acc82d295803fdd4
msgid "**Running vLLM**"
msgstr "**è¿è¡ŒvLLM**"

#: ../../getting_started/install/deploy.rst:377
#: 75bc518b444c417ba4d9c15246549327
msgid "**1.Installing Dependencies**"
msgstr "**1.å®‰è£…ä¾èµ–**"

#: ../../getting_started/install/deploy.rst:379
#: 725c620b0a5045c1a64a3b2a2e9b48f3
msgid ""
"vLLM is an optional dependency in DB-GPT, and you can manually install it"
" using the following command:"
msgstr "vLLM åœ¨ DB-GPT æ˜¯ä¸€ä¸ªå¯é€‰ä¾èµ–, ä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æ‰‹åŠ¨å®‰è£…å®ƒï¼š"

#: ../../getting_started/install/deploy.rst:385
#: 6f4b540107764f3592cc07cf170e4911
msgid "**2.Modifying the Configuration File**"
msgstr "**2.ä¿®æ”¹é…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:387
#: b8576a1572674c4890e09b73e02cf0e8
msgid "Next, you can directly modify your .env file to enable vllm."
msgstr "ä½ å¯ä»¥ç›´æ¥ä¿®æ”¹ä½ çš„ `.env` æ–‡ä»¶"

#: ../../getting_started/install/deploy.rst:394
#: b006745f3aee4651aaa0cf79081b5d7f
msgid ""
"You can view the models supported by vLLM `here "
"<https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models>`_"
msgstr ""
"ä½ å¯ä»¥åœ¨ `è¿™é‡Œ "
"<https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models>`_ æŸ¥çœ‹ vLLM æ”¯æŒçš„æ¨¡å‹ã€‚"

#: ../../getting_started/install/deploy.rst:403
#: bc8057ee75e14737bf8fca3ceb555dac
msgid "3.Prepare sql example(Optional)"
msgstr "3.å‡†å¤‡ sql example(å¯é€‰)"

#: ../../getting_started/install/deploy.rst:404
#: 9b0b9112237c4b3aaa1dd5d704ea32e6
msgid "**(Optional) load examples into SQLite**"
msgstr "**(å¯é€‰) åŠ è½½æ ·ä¾‹æ•°æ®åˆ° SQLite æ•°æ®åº“ä¸­**"

#: ../../getting_started/install/deploy.rst:411
#: 0815e13b96264ffcba1526c82ba2e7c8
msgid "On windows platform:"
msgstr "åœ¨ Windows å¹³å°ï¼š"

#: ../../getting_started/install/deploy.rst:418
#: 577a4167ecac4fa88586961f225f0487
msgid "4.Run db-gpt server"
msgstr "4.è¿è¡Œdb-gpt server"

#: ../../getting_started/install/deploy.rst:424
#: a9f96b064b674f80824257b4b0a18e2a
msgid "**Open http://localhost:5000 with your browser to see the product.**"
msgstr "æ‰“å¼€æµè§ˆå™¨è®¿é—®http://localhost:5000"

#~ msgid ""
#~ "DB-GPT can be deployed on servers"
#~ " with low hardware requirements or on"
#~ " servers with high hardware requirements."
#~ " You can install DB-GPT by "
#~ "Using third-part LLM REST API "
#~ "Service OpenAI, Azure."
#~ msgstr ""

#~ msgid ""
#~ "And you can also install DB-GPT"
#~ " by deploy LLM Service by download"
#~ " LLM model."
#~ msgstr ""

#~ msgid "ç™¾å·"
#~ msgstr ""

#~ msgid "ç™¾å· ç¡¬ä»¶è¦æ±‚"
#~ msgstr ""

