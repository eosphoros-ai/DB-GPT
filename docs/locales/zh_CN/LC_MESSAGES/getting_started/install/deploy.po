# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT ğŸ‘ğŸ‘ 0.4.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-11-03 13:00+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy.rst:4 7a1ee708aa40431981178ebd1d34b9aa
msgid "Installation From Source"
msgstr "æºç å®‰è£…"

#: ../../getting_started/install/deploy.rst:6 dd4c542c563b4d4ca5b710dc7326ff8b
msgid "To get started, install DB-GPT with the following steps."
msgstr "æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œå®‰è£…"

#: ../../getting_started/install/deploy.rst:8 f352a8d93da744aaab775e8290c74704
msgid ""
"DB-GPT can be deployed on servers with low hardware requirements or on "
"servers with high hardware requirements. You can install DB-GPT by Using "
"third-part LLM REST API Service OpenAI, Azure."
msgstr ""

#: ../../getting_started/install/deploy.rst:11 b89bfaa679d5448791d315b9ffebe7c5
msgid ""
"And you can also install DB-GPT by deploy LLM Service by download LLM "
"model."
msgstr ""

#: ../../getting_started/install/deploy.rst:15 942b2999b3b5432e956c44e2a51b5269
msgid "1.Preparation"
msgstr "1.å‡†å¤‡"

#: ../../getting_started/install/deploy.rst:16 d27c8698bd4a45d7a7ebdbba470318d6
msgid "**Download DB-GPT**"
msgstr "**ä¸‹è½½DB-GPTé¡¹ç›®**"

#: ../../getting_started/install/deploy.rst:22 ce3e61a03ca945b4a3c3b264a063442c
msgid "**Install Miniconda**"
msgstr "**å®‰è£…Miniconda**"

#: ../../getting_started/install/deploy.rst:24 1fd868a2e84c4752b242b401ac64d0e4
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration. For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. `How to install Miniconda "
"<https://docs.conda.io/en/latest/miniconda.html>`_"
msgstr ""
"ç›®å‰ä½¿ç”¨Sqliteä½œä¸ºé»˜è®¤æ•°æ®åº“ï¼Œå› æ­¤DB-"
"GPTå¿«é€Ÿéƒ¨ç½²ä¸éœ€è¦éƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–æ•°æ®åº“ï¼Œéœ€è¦å…ˆéƒ¨ç½²ç›¸å…³æ•°æ®åº“æœåŠ¡ã€‚æˆ‘ä»¬ç›®å‰ä½¿ç”¨Minicondaè¿›è¡Œpythonç¯å¢ƒå’ŒåŒ…ä¾èµ–ç®¡ç†[å®‰è£…"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy.rst:41 59dc27ad4237444d8eb7229fe29c975d
msgid "2.Deploy LLM Service"
msgstr "2.éƒ¨ç½²LLMæœåŠ¡"

#: ../../getting_started/install/deploy.rst:42 12ed83127fb744bcb60c5c7c16359a0e
msgid ""
"DB-GPT can be deployed on servers with low hardware requirements or on "
"servers with high hardware requirements."
msgstr "DB-GPTå¯ä»¥éƒ¨ç½²åœ¨å¯¹ç¡¬ä»¶è¦æ±‚ä¸é«˜çš„æœåŠ¡å™¨ï¼Œä¹Ÿå¯ä»¥éƒ¨ç½²åœ¨å¯¹ç¡¬ä»¶è¦æ±‚é«˜çš„æœåŠ¡å™¨"

#: ../../getting_started/install/deploy.rst:44 1674c59c24804200ab53bd31847be19a
msgid ""
"If you are low hardware requirements you can install DB-GPT by Using "
"third-part LLM REST API Service OpenAI, Azure, tongyi."
msgstr "Low hardware requirementsæ¨¡å¼é€‚ç”¨äºå¯¹æ¥ç¬¬ä¸‰æ–¹æ¨¡å‹æœåŠ¡çš„api,æ¯”å¦‚OpenAI, é€šä¹‰åƒé—®, æ–‡å¿ƒ.cppã€‚"

#: ../../getting_started/install/deploy.rst:48 e9c37648778540fe982c26d4104931ae
msgid "As our project has the ability to achieve OpenAI performance of over 85%,"
msgstr "ä½¿ç”¨OpenAIæœåŠ¡å¯ä»¥è®©DB-GPTå‡†ç¡®ç‡è¾¾åˆ°85%"

#: ../../getting_started/install/deploy.rst:53 201b7af45c0046faada4b81e110e7745
msgid "Notice make sure you have install git-lfs"
msgstr "ç¡®è®¤æ˜¯å¦å·²ç»å®‰è£…git-lfs"

#: ../../getting_started/install/deploy.rst:55 f7c9535c0eb546f7b1389a181b08c5c0
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:57 8fb3efededcb42c592fa27b03c4e9a65
msgid "ubuntu:apt-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:59 b3df472ae3ae470d94112f7327787e13
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:63
#: ../../getting_started/install/deploy.rst:226
#: 070a6d653f6740cf852ddaf036ac2538 635662e449d34d5b9f6316898d14e0a6
msgid "OpenAI"
msgstr "OpenAI"

#: ../../getting_started/install/deploy.rst:65
#: ../../getting_started/install/deploy.rst:212
#: 1ff3d5d9f9814e638f118925bedb7800 59cda8abcdc7471eb0a488610121a533
msgid "Download embedding model"
msgstr "ä¸‹è½½embedding model"

#: ../../getting_started/install/deploy.rst:77
#: ../../getting_started/install/deploy.rst:234
#: 0c68e075ab7840bd9d75ced89deaea86 b9e25c8690da4d9c94c012a04ddc8f0d
msgid "Configure LLM_MODEL and PROXY_API_URL and API_KEY in `.env` file"
msgstr "åœ¨`.env`æ–‡ä»¶è®¾ç½®LLM_MODEL and PROXY_API_URL and API_KEY"

#: ../../getting_started/install/deploy.rst:87
#: ../../getting_started/install/deploy.rst:285
#: 5eb0e6ffb66a48dfa05cfea8414b21c5 61b75bb879d64726976c131f0f7cea83
msgid "Make sure your .env configuration is not overwritten"
msgstr "è®¤.envæ–‡ä»¶ä¸ä¼šè¢«è¦†ç›–\""

#: ../../getting_started/install/deploy.rst:90 d14c2a247eee401bae9b1711cdcc0712
msgid "Vicuna"
msgstr "Vicuna"

#: ../../getting_started/install/deploy.rst:91 d59e03c9d3c3405ea04d814bffc59ef8
msgid ""
"`Vicuna-v1.5 <https://huggingface.co/lmsys/vicuna-13b-v1.5>`_ based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""

#: ../../getting_started/install/deploy.rst:93 c6d22fb4c35c40378159e7845c87bb51
msgid "vicuna-v1.5 hardware requirements"
msgstr ""

#: ../../getting_started/install/deploy.rst:97
#: ../../getting_started/install/deploy.rst:142
#: 215382b66c944bef8ba8d081792cb3c5 eadfd6f7230f49a4a9d5552b51d766c0
msgid "Model"
msgstr ""

#: ../../getting_started/install/deploy.rst:98
#: ../../getting_started/install/deploy.rst:143
#: 5eca1dbe065b4e648ee00ca118e84214 abec181550804601ad54500469b332f9
msgid "Quantize"
msgstr ""

#: ../../getting_started/install/deploy.rst:99
#: ../../getting_started/install/deploy.rst:144
#: 24a907ffe6e4449abb632d80ada8733c 472a99174d68460a84a72d65d9fdcd07
msgid "VRAM Size"
msgstr ""

#: ../../getting_started/install/deploy.rst:100
#: ../../getting_started/install/deploy.rst:103
#: 87bba38be8f24c0688431a5985622d33 e8c8ef23e4964d059d9615f41719d05f
msgid "vicuna-7b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:101
#: ../../getting_started/install/deploy.rst:107
#: ../../getting_started/install/deploy.rst:146
#: ../../getting_started/install/deploy.rst:152
#: 7695fa31995b4e038ac4359df76a0a2f a8c361af629c4a179c811ac39ef16c3c
#: defc11c71d45471da982eb7c96039450 f9362395c5ff4213b5fb2f4d1e430496
msgid "4-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:102
#: ../../getting_started/install/deploy.rst:147
#: 7f2ebb804fb042c1b24cf6274c2cb7fc 9604200fbd974a6e9e3d66a38f9e5895
msgid "8 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:104
#: ../../getting_started/install/deploy.rst:110
#: ../../getting_started/install/deploy.rst:149
#: ../../getting_started/install/deploy.rst:155
#: 4936725d2b0a47f39286453db766027c 879daa7be9e243b7a8a51f0859459096
#: c5322eaace39472282941c4bcae87232 e79778e8505e490a912aa80c28e37b0c
msgid "8-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:105
#: ../../getting_started/install/deploy.rst:108
#: ../../getting_started/install/deploy.rst:150
#: ../../getting_started/install/deploy.rst:153
#: 09cd46eb89234d1cbbaf9fccd2d7f206 531282962fd148108a5d4582022b9d11
#: c2249464c63b4808a13be66c6a04653d c54d26deb5e14bbc9ebeefe751ee32a1
msgid "12 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:106
#: ../../getting_started/install/deploy.rst:109
#: 40d56aeeeeae4ad5a08d432da14c91f6 a4aae2c08b35462699a074c20436b583
msgid "vicuna-13b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:111
#: ../../getting_started/install/deploy.rst:156
#: 4e58bb5b42d043b8b8105ff1424b3dda 670a8f2e7d9a4f56b338a890a0e6179c
msgid "20 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:127
#: ../../getting_started/install/deploy.rst:174
#: ../../getting_started/install/deploy.rst:200
#: 7aec36decef845a4979e4d72b5556166 edb0520c46de41e79a56453901f5dbda
#: f726ecdc3a3b40eeb527464e705d262c
msgid "The model files are large and will take a long time to download."
msgstr ""

#: ../../getting_started/install/deploy.rst:129
#: ../../getting_started/install/deploy.rst:176
#: ../../getting_started/install/deploy.rst:202
#: 1ced4f4cd3ca4b4a8a545ba4625e5888 1f3fc32e037846ecba6e9d99acaa341e
#: 2e2a3b69997043858a21ce88c349fd87
msgid "**Configure LLM_MODEL in `.env` file**"
msgstr ""

#: ../../getting_started/install/deploy.rst:136
#: ../../getting_started/install/deploy.rst:231
#: 0ae3d2ff947545fb8ae3a3221ada4fca 908164bbe3f745cf994b65c7cc0d4f42
msgid "Baichuan"
msgstr ""

#: ../../getting_started/install/deploy.rst:138
#: 4249581eb6eb4c90ac467c3b23f9cf47
msgid "Baichuan hardware requirements"
msgstr ""

#: ../../getting_started/install/deploy.rst:145
#: ../../getting_started/install/deploy.rst:148
#: c01aea1eaf0c4d0ca14c10e51003fa2e f1081e1ecc6b42b6a7d227cf4a3b9aa9
msgid "baichuan-7b"
msgstr ""

#: ../../getting_started/install/deploy.rst:151
#: ../../getting_started/install/deploy.rst:154
#: 1cf37d7196814348a177341d80d9748a edf669023f5647c89c52475f385dd91f
msgid "baichuan-13b"
msgstr ""

#: ../../getting_started/install/deploy.rst:178
#: 25e45bdc7e6b475080f8b39cd555746d
msgid "please rename Baichuan path to \"baichuan2-13b\" or \"baichuan2-7b\""
msgstr "å°†Baichuanæ¨¡å‹ç›®å½•ä¿®æ”¹ä¸º\"baichuan2-13b\" æˆ– \"baichuan2-7b\""

#: ../../getting_started/install/deploy.rst:184
#: 80c36809333c4e2490166571222963cc
msgid "ChatGLM"
msgstr ""

#: ../../getting_started/install/deploy.rst:204
#: f1e3c93f447b40039a57d278d49ff32d
msgid "please rename chatglm model path to \"chatglm2-6b\""
msgstr "å°†chatglmæ¨¡å‹ç›®å½•ä¿®æ”¹ä¸º\"chatglm2-6b\""

#: ../../getting_started/install/deploy.rst:210
#: 62570cd6084a4b3c8ceff5fdd5234aa0
msgid "Other LLM API"
msgstr ""

#: ../../getting_started/install/deploy.rst:227
#: fb0c2d274f3740a2af7ece03fdb81d22
msgid "Azure"
msgstr ""

#: ../../getting_started/install/deploy.rst:228
#: 13e28550c95e4838b9e35a064448f9ca
msgid "Aliyun tongyi"
msgstr ""

#: ../../getting_started/install/deploy.rst:229
#: 32d7289e7e514db5b3f49ebad9eb0e46
msgid "Baidu wenxin"
msgstr ""

#: ../../getting_started/install/deploy.rst:230
#: b8033cd9b67d4b18823fc2053e23114b
msgid "Zhipu"
msgstr ""

#: ../../getting_started/install/deploy.rst:232
#: 9cc479d40a3f435a81e8e2aa8c015bbe
msgid "Bard"
msgstr ""

#: ../../getting_started/install/deploy.rst:287
#: abd4caa347e3405f86319825c14b3b4a
msgid "llama.cpp"
msgstr ""

#: ../../getting_started/install/deploy.rst:289
#: b3b53e55206345e8b332af19851f5a5f
msgid ""
"DB-GPT already supports `llama.cpp "
"<https://github.com/ggerganov/llama.cpp>`_ via `llama-cpp-python "
"<https://github.com/abetlen/llama-cpp-python>`_ ."
msgstr ""
"DB-GPT å·²ç»æ”¯æŒäº† `llama.cpp <https://github.com/ggerganov/llama.cpp>`_ via "
"`llama-cpp-python <https://github.com/abetlen/llama-cpp-python>`_ ."

#: ../../getting_started/install/deploy.rst:291
#: 54fc351de03445e7b4b7a5408215c0cf
msgid "**Preparing Model Files**"
msgstr "**å‡†å¤‡Modelæ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:293
#: 41e7c90a891d47c1ae04883454cf999f
msgid ""
"To use llama.cpp, you need to prepare a gguf format model file, and there"
" are two common ways to obtain it, you can choose either:"
msgstr "ä½¿ç”¨ llama.cppï¼Œä½ éœ€è¦å‡†å¤‡ gguf æ ¼å¼çš„æ–‡ä»¶ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹æ³•è·å–"

#: ../../getting_started/install/deploy.rst:295
#: 8aefa2f2e0ab47469717aff4af20668e
msgid "**1. Download a pre-converted model file.**"
msgstr "**1.ä¸‹è½½å·²è½¬æ¢çš„æ¨¡å‹æ–‡ä»¶.**"

#: ../../getting_started/install/deploy.rst:297
#: 9849e3b8c0824cfa9add58a97fb583c4
msgid ""
"Suppose you want to use [Vicuna 13B v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5), you can download the file already converted from "
"[TheBloke/vicuna-13B-v1.5-GGUF](https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF), only one file is needed. Download it to the `models` "
"directory and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"å‡è®¾æ‚¨æƒ³ä½¿ç”¨[Vicuna 13B v1.5](https://huggingface.co/lmsys/vicuna-"
"13b-v1.5)æ‚¨å¯ä»¥ä»[TheBloke/vicuna-"
"13B-v1.5-GGUF](https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF)ä¸‹è½½å·²è½¬æ¢çš„æ–‡ä»¶ï¼Œåªéœ€è¦ä¸€ä¸ªæ–‡ä»¶ã€‚å°†å…¶ä¸‹è½½åˆ°modelsç›®å½•å¹¶å°†å…¶é‡å‘½åä¸º `ggml-"
"model-q4_0.gguf`ã€‚"

#: ../../getting_started/install/deploy.rst:303
#: b79538a3af8a4946b24d8ebdb343e8aa
msgid "**2. Convert It Yourself**"
msgstr "**2. è‡ªè¡Œè½¬æ¢**"

#: ../../getting_started/install/deploy.rst:305
#: 12f31f96202f4789af86c254feaac717
msgid ""
"You can convert the model file yourself according to the instructions in "
"[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run), and put the converted file in the models directory "
"and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"æ‚¨å¯ä»¥æ ¹æ®[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run)ä¸­çš„è¯´æ˜è‡ªè¡Œè½¬æ¢æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶æŠŠè½¬æ¢åçš„æ–‡ä»¶æ”¾åœ¨modelsç›®å½•ä¸­ï¼Œå¹¶é‡å‘½åä¸º`ggml-"
"model-q4_0.gguf`ã€‚"

#: ../../getting_started/install/deploy.rst:307
#: c45612d904e64b3a8c0d50c564447853
msgid "**Installing Dependencies**"
msgstr "**å®‰è£…ä¾èµ–**"

#: ../../getting_started/install/deploy.rst:309
#: c3c04c3e560a46bba53719472a25ce11
msgid ""
"llama.cpp is an optional dependency in DB-GPT, and you can manually "
"install it using the following command:"
msgstr "llama.cppåœ¨DB-GPTä¸­æ˜¯å¯é€‰å®‰è£…é¡¹, ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…"

#: ../../getting_started/install/deploy.rst:316
#: 25f8ceb4fca941cdbd7c4dcc49818d79
msgid "**3.Modifying the Configuration File**"
msgstr "**3.ä¿®æ”¹é…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:318
#: 5a63a04a43a1487eac56f2560dbc2275
msgid "Next, you can directly modify your `.env` file to enable llama.cpp."
msgstr "ä¿®æ”¹`.env`æ–‡ä»¶ä½¿ç”¨llama.cpp"

#: ../../getting_started/install/deploy.rst:325
#: ../../getting_started/install/deploy.rst:393
#: e8b1498ad8c44201ba2e050db454c61c ffcb505780884dddaf3559990405a081
msgid ""
"Then you can run it according to `Run <https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html#run>`_"
msgstr ""
"ç„¶åä½ å¯ä»¥æ ¹æ®[è¿è¡Œ](https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-"
"cn/zh_CN/latest/getting_started/install/deploy/deploy.html#run)æ¥è¿è¡Œ"

#: ../../getting_started/install/deploy.rst:328
#: 0d8d0f68e43b465a9537fbc741a9d37f
msgid "**More Configurations**"
msgstr "**æ›´å¤šé…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:330
#: 6fdef4b280e5456faf11521700b7fe04
msgid ""
"In DB-GPT, the model configuration can be done through  `{model "
"name}_{config key}`."
msgstr "åœ¨DB-GPTä¸­ï¼Œæ¨¡å‹é…ç½®å¯ä»¥é€šè¿‡`{æ¨¡å‹åç§°}_{é…ç½®å}` æ¥é…ç½®ã€‚"

#: ../../getting_started/install/deploy.rst:332
#: 256b25378f3d4bfd902f155b3a4346ad
msgid "More Configurations"
msgstr "**æ›´å¤šé…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:336
#: ebe4390d2de44fd288379a439d99bccd
msgid "Environment Variable Key"
msgstr "ç¯å¢ƒå˜é‡Key"

#: ../../getting_started/install/deploy.rst:337
#: ee1c68623cc24446b4036870e0c7f21c
msgid "Default"
msgstr "é»˜è®¤å€¼"

#: ../../getting_started/install/deploy.rst:338
#: ed6c358d27fb4022a70de1e733ad99cd
msgid "Description"
msgstr "æè¿°"

#: ../../getting_started/install/deploy.rst:339
#: 4ad9e1b85c294cf084b8ebf858cc4075
msgid "llama_cpp_prompt_template"
msgstr ""

#: ../../getting_started/install/deploy.rst:340
#: ../../getting_started/install/deploy.rst:343
#: ../../getting_started/install/deploy.rst:349
#: ../../getting_started/install/deploy.rst:355
#: ../../getting_started/install/deploy.rst:361
#: 0cfccd9b8f7043ec8ccfe5159dfcf2c3 155d01bf0ad94016a32a2ec18a0fd881
#: 3a194e635268499a939990b3503da72e 8909fd33e33b4f5a9208b8338b82b20f
#: e32e3219d2384884b398054cecceec8a
msgid "None"
msgstr ""

#: ../../getting_started/install/deploy.rst:341
#: c395eb1a7b044536a7ed300320e230f5
msgid ""
"Prompt template name, now support: zero_shot, vicuna_v1.1,alpaca,llama-2"
",baichuan-chat,internlm-chat, If None, the prompt template is "
"automatically determined from model pathã€‚"
msgstr ""
"Prompt template ç°åœ¨å¯ä»¥æ”¯æŒ`zero_shot, vicuna_v1.1,alpaca,llama-2,baichuan-"
"chat,internlm-chat`, å¦‚æœæ˜¯None, å¯ä»¥æ ¹æ®æ¨¡å‹è·¯å¾„æ¥è‡ªåŠ¨è·å–æ¨¡å‹ Prompt template"

#: ../../getting_started/install/deploy.rst:342
#: 8fb84e002d1f4fc79bd4465b161e018a
msgid "llama_cpp_model_path"
msgstr ""

#: ../../getting_started/install/deploy.rst:344
#: 81df474169634fbf9da20fe239d903a0
msgid "Model path"
msgstr "æ¨¡å‹è·¯å¾„"

#: ../../getting_started/install/deploy.rst:345
#: a94ac450e72940f78b8ba6f6c9854248
msgid "llama_cpp_n_gpu_layers"
msgstr ""

#: ../../getting_started/install/deploy.rst:346
#: 06a24fb226f24ed4a93cde7034a3b67f
msgid "1000000000"
msgstr ""

#: ../../getting_started/install/deploy.rst:347
#: d29a8733688a4c62a3ea88e4793b82ef
msgid ""
"Number of layers to offload to the GPU, Set this to 1000000000 to offload"
" all layers to the GPU. If your GPU VRAM is not enough, you can set a low"
" number, eg: 10"
msgstr "è¦å°†å¤šå°‘ç½‘ç»œå±‚è½¬ç§»åˆ°GPUä¸Šï¼Œå°†å…¶è®¾ç½®ä¸º1000000000ä»¥å°†æ‰€æœ‰å±‚è½¬ç§»åˆ°GPUä¸Šã€‚å¦‚æœæ‚¨çš„ GPU å†…å­˜ä¸è¶³ï¼Œå¯ä»¥è®¾ç½®è¾ƒä½çš„æ•°å­—ï¼Œä¾‹å¦‚ï¼š10ã€‚"

#: ../../getting_started/install/deploy.rst:348
#: 8531ecd5690e46f2a9d64823405054b1
msgid "llama_cpp_n_threads"
msgstr ""

#: ../../getting_started/install/deploy.rst:350
#: d22c11631f5b4bc3b9dabad2175f26d5
msgid ""
"Number of threads to use. If None, the number of threads is automatically"
" determined"
msgstr "è¦ä½¿ç”¨çš„çº¿ç¨‹æ•°é‡ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™çº¿ç¨‹æ•°é‡å°†è‡ªåŠ¨ç¡®å®šã€‚"

#: ../../getting_started/install/deploy.rst:351
#: 06994945569849a9a07ca0fbb56509e4
msgid "llama_cpp_n_batch"
msgstr ""

#: ../../getting_started/install/deploy.rst:352
#: a509c6b3800941aa8aaed1c2c3a89937
msgid "512"
msgstr ""

#: ../../getting_started/install/deploy.rst:353
#: 02f71c7f7ddc49f6b61b163237f4c4fe
msgid "Maximum number of prompt tokens to batch together when calling llama_eval"
msgstr "åœ¨è°ƒç”¨llama_evalæ—¶ï¼Œæ‰¹å¤„ç†åœ¨ä¸€èµ·çš„prompt tokensçš„æœ€å¤§æ•°é‡"

#: ../../getting_started/install/deploy.rst:354
#: 4d0df2430c22464fac021d816b4fa0c3
msgid "llama_cpp_n_gqa"
msgstr ""

#: ../../getting_started/install/deploy.rst:356
#: 24304fe7c331423ebcc9eb407d6f6f46
msgid "Grouped-query attention. Must be 8 for llama-2 70b."
msgstr "å¯¹äº llama-2 70B æ¨¡å‹ï¼ŒGrouped-query attention å¿…é¡»ä¸º8ã€‚"

#: ../../getting_started/install/deploy.rst:357
#: 8c6b332cdcc845dc9c218e3de7050e3e
msgid "llama_cpp_rms_norm_eps"
msgstr ""

#: ../../getting_started/install/deploy.rst:358
#: 2c2ac24d7d264be3aa6960e776fa5e56
msgid "5e-06"
msgstr ""

#: ../../getting_started/install/deploy.rst:359
#: 2d7170d63e364aa3bb3040d96ba31316
msgid "5e-6 is a good value for llama-2 models."
msgstr "å¯¹äºllama-2æ¨¡å‹æ¥è¯´ï¼Œ5e-6æ˜¯ä¸€ä¸ªä¸é”™çš„å€¼ã€‚"

#: ../../getting_started/install/deploy.rst:360
#: 402b942db0b149a8985685dbfe6f31c7
msgid "llama_cpp_cache_capacity"
msgstr ""

#: ../../getting_started/install/deploy.rst:362
#: c990275acf6741398ac147a8ddce1ce3
msgid "Maximum cache capacity. Examples: 2000MiB, 2GiB"
msgstr "æ¨¡å‹ç¼“å­˜æœ€å¤§å€¼. ä¾‹å¦‚: 2000MiB, 2GiB"

#: ../../getting_started/install/deploy.rst:363
#: 5f579fac7df54a338dc3c8a348bc90ea
msgid "llama_cpp_prefer_cpu"
msgstr ""

#: ../../getting_started/install/deploy.rst:364
#: 8413a349be174d50991653a5554933c5
msgid "False"
msgstr ""

#: ../../getting_started/install/deploy.rst:365
#: 78af815bd0724301870ef934d27b208d
msgid ""
"If a GPU is available, it will be preferred by default, unless "
"prefer_cpu=False is configured."
msgstr "å¦‚æœæœ‰å¯ç”¨çš„GPUï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šä¼˜å…ˆä½¿ç”¨GPUï¼Œé™¤éé…ç½®äº† prefer_cpu=Falseã€‚"

#: ../../getting_started/install/deploy.rst:368
#: f193ef95a45f42d2a5e156a79ed09685
msgid "vllm"
msgstr ""

#: ../../getting_started/install/deploy.rst:370
#: e28e1d550515498e8e41a2e1187f9956
msgid "vLLM is a fast and easy-to-use library for LLM inference and serving."
msgstr "\"vLLM æ˜¯ä¸€ä¸ªå¿«é€Ÿä¸”æ˜“äºä½¿ç”¨çš„ LLM æ¨ç†å’ŒæœåŠ¡çš„åº“ã€‚"

#: ../../getting_started/install/deploy.rst:372
#: adbcb8083c274d5eae4209b5b4fb8048
msgid "**Running vLLM**"
msgstr "**è¿è¡ŒvLLM**"

#: ../../getting_started/install/deploy.rst:374
#: 188e0fb1d0d34fccb15a796707ca95dd
msgid "**1.Installing Dependencies**"
msgstr "**1.å®‰è£…ä¾èµ–**"

#: ../../getting_started/install/deploy.rst:376
#: dbfcdd5d0e2a4cb29692022e63766116
msgid ""
"vLLM is an optional dependency in DB-GPT, and you can manually install it"
" using the following command:"
msgstr "vLLM åœ¨ DB-GPT æ˜¯ä¸€ä¸ªå¯é€‰ä¾èµ–, ä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æ‰‹åŠ¨å®‰è£…å®ƒï¼š"

#: ../../getting_started/install/deploy.rst:382
#: 4401138d25b34b028c4bf43ddfb89aa3
msgid "**2.Modifying the Configuration File**"
msgstr "**2.ä¿®æ”¹é…ç½®æ–‡ä»¶**"

#: ../../getting_started/install/deploy.rst:384
#: ee0ad8661dce4b909b07df4773150ee6
msgid "Next, you can directly modify your .env file to enable vllm."
msgstr "ä½ å¯ä»¥ç›´æ¥ä¿®æ”¹ä½ çš„ `.env` æ–‡ä»¶"

#: ../../getting_started/install/deploy.rst:391
#: 1be59f77d774429b9959bf834871a414
msgid ""
"You can view the models supported by vLLM `here "
"<https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models>`_"
msgstr "ä½ å¯ä»¥åœ¨ "
"[è¿™é‡Œ](https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models) æŸ¥çœ‹ vLLM æ”¯æŒçš„æ¨¡å‹ã€‚"

#: ../../getting_started/install/deploy.rst:400
#: def3d7d7cd03407d9dfeff2e1eee951e
msgid "3.Prepare sql example(Optional)"
msgstr "3.å‡†å¤‡ sql example(å¯é€‰)"

#: ../../getting_started/install/deploy.rst:401
#: b6038f79dac74fda8f6db08325bf0686
msgid "**(Optional) load examples into SQLite**"
msgstr "**(å¯é€‰) load examples into SQLite**"

#: ../../getting_started/install/deploy.rst:408
#: 37c84b22186b48baba5aca60f8a70f49
msgid "On windows platform:"
msgstr ""

#: ../../getting_started/install/deploy.rst:415
#: 09c9508bac8a4330931d45d02c33762f
msgid "4.Run db-gpt server"
msgstr "4.è¿è¡Œdb-gpt server"

#: ../../getting_started/install/deploy.rst:421
#: 04d68abe85a646388956cbd1b47f3232
msgid "**Open http://localhost:5000 with your browser to see the product.**"
msgstr "æ‰“å¼€æµè§ˆå™¨è®¿é—®http://localhost:5000"

