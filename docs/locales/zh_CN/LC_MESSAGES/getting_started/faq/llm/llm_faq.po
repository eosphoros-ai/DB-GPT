# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT ğŸ‘ğŸ‘ 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-30 11:37+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/faq/llm/llm_faq.md:1 98e23f85313c45169ff2ba7f80193356
msgid "LLM USE FAQ"
msgstr "LLMæ¨¡å‹ä½¿ç”¨FAQ"

#: ../../getting_started/faq/llm/llm_faq.md:3 0d49acfb4af947cb969b249346b00d33
#, fuzzy
msgid "Q1: how to use openai chatgpt service"
msgstr "æˆ‘æ€ä¹ˆä½¿ç”¨OPENAIæœåŠ¡"

#: ../../getting_started/faq/llm/llm_faq.md:4 7010fec33e264987a29de86c54da93e8
#, fuzzy
msgid "change your LLM_MODEL in `.env`"
msgstr "é€šè¿‡åœ¨.envæ–‡ä»¶è®¾ç½®LLM_MODEL"

#: ../../getting_started/faq/llm/llm_faq.md:9 0982d6d5d0b3434fb00698aaf675f3f3
msgid "set your OPENAPI KEY"
msgstr "set your OPENAPI KEY"

#: ../../getting_started/faq/llm/llm_faq.md:16 63650494c1574de09c007e1d470dd53d
msgid "make sure your openapi API_KEY is available"
msgstr "ç¡®è®¤openapi API_KEYæ˜¯å¦å¯ç”¨"

#: ../../getting_started/faq/llm/llm_faq.md:18 5721ec71e344499d96c55b7e531d7c08
#, fuzzy
msgid ""
"Q2: What difference between `python dbgpt_server --light` and `python "
"dbgpt_server`"
msgstr "Q2: `python dbgpt_server --light` å’Œ `python dbgpt_server`çš„åŒºåˆ«æ˜¯ä»€ä¹ˆ?"

#: ../../getting_started/faq/llm/llm_faq.md:20 76a650f195dd40b6a3a3564030cdc040
msgid ""
"`python dbgpt_server --light` dbgpt_server does not start the llm "
"service. Users can deploy the llm service separately by using `python "
"llmserver`, and dbgpt_server accesses the llm service through set the "
"LLM_SERVER environment variable in .env. The purpose is to allow for the "
"separate deployment of dbgpt's backend service and llm service."
msgstr ""
"`python dbgpt_server --light` dbgpt_serveråœ¨å¯åŠ¨åå°æœåŠ¡çš„æ—¶å€™ä¸å¯åŠ¨æ¨¡å‹æœåŠ¡, "
"ç”¨æˆ·å¯ä»¥é€šè¿‡`python "
"llmserver`å•ç‹¬éƒ¨ç½²æ¨¡å‹æœåŠ¡ï¼Œdbgpt_serveré€šè¿‡LLM_SERVERç¯å¢ƒå˜é‡æ¥è®¿é—®æ¨¡å‹æœåŠ¡ã€‚ç›®çš„æ˜¯ä¸ºäº†å¯ä»¥å°†dbgptåå°æœåŠ¡å’Œå¤§æ¨¡å‹æœåŠ¡åˆ†ç¦»éƒ¨ç½²ã€‚"

#: ../../getting_started/faq/llm/llm_faq.md:22 8cd87e3504784d9e891e1fb96c79e143
msgid ""
"`python dbgpt_server` dbgpt_server service and the llm service are "
"deployed on the same instance. when dbgpt_server starts the service, it "
"also starts the llm service at the same time."
msgstr "`python dbgpt_server` æ˜¯å°†åå°æœåŠ¡å’Œæ¨¡å‹æœåŠ¡éƒ¨ç½²åœ¨åŒä¸€å°å®ä¾‹ä¸Š.dbgpt_serveråœ¨å¯åŠ¨æœåŠ¡çš„æ—¶å€™åŒæ—¶å¼€å¯æ¨¡å‹æœåŠ¡."

#: ../../getting_started/faq/llm/llm_faq.md:27 58a6eaf57e6d425685f67058b1a642d4
msgid ""
"If you want to access an external LLM service(deployed by DB-GPT), you "
"need to"
msgstr "å¦‚æœæ¨¡å‹æœåŠ¡éƒ¨ç½²(é€šè¿‡DB-GPTéƒ¨ç½²)åœ¨åˆ«çš„æœºå™¨ï¼Œæƒ³é€šè¿‡dbgptæœåŠ¡è®¿é—®æ¨¡å‹æœåŠ¡"

#: ../../getting_started/faq/llm/llm_faq.md:29 67ac8823ca2e49ba9c833368e2cfb53c
msgid ""
"1.set the variables LLM_MODEL=YOUR_MODEL_NAME, "
"MODEL_SERVER=YOUR_MODEL_SERVERï¼ˆeg:http://localhost:5000ï¼‰ in the .env "
"file."
msgstr ""

#: ../../getting_started/faq/llm/llm_faq.md:31 e5c066bcdf0649a1b33bbfc7fd3b1a66
msgid "2.execute dbgpt_server.py in light mode"
msgstr "2.execute dbgpt_server.py light æ¨¡å¼"

#: ../../getting_started/faq/llm/llm_faq.md:33 402ff01d7ee94d97be4a0eb964e39b97
msgid "python pilot/server/dbgpt_server.py --light"
msgstr ""

#: ../../getting_started/faq/llm/llm_faq.md:38 86190c689d8f4d9a9b58d904e0b5867b
#, fuzzy
msgid "Q3: How to use MultiGPUs"
msgstr "Q3: æ€ä¹ˆä½¿ç”¨ MultiGPUs"

#: ../../getting_started/faq/llm/llm_faq.md:40 6b08cff88750440b98956203d8b8a084
msgid ""
"DB-GPT will use all available gpu by default. And you can modify the "
"setting `CUDA_VISIBLE_DEVICES=0,1` in `.env` file to use the specific gpu"
" IDs."
msgstr "DB-GPTé»˜è®¤åŠ è½½å¯åˆ©ç”¨çš„gpuï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡ä¿®æ”¹ åœ¨`.env`æ–‡ä»¶ `CUDA_VISIBLE_DEVICES=0,1`æ¥æŒ‡å®šgpu IDs"

#: ../../getting_started/faq/llm/llm_faq.md:43 93b39089e5be4475b9e90e7813f5a7d9
msgid ""
"Optionally, you can also specify the gpu ID to use before the starting "
"command, as shown below:"
msgstr "ä½ ä¹Ÿå¯ä»¥æŒ‡å®šgpu IDå¯åŠ¨"

#: ../../getting_started/faq/llm/llm_faq.md:53 62e3074c109d401fa4bf1ddbdc6c7be1
msgid ""
"You can modify the setting `MAX_GPU_MEMORY=xxGib` in `.env` file to "
"configure the maximum memory used by each GPU."
msgstr "åŒæ—¶ä½ å¯ä»¥é€šè¿‡åœ¨.envæ–‡ä»¶è®¾ç½®`MAX_GPU_MEMORY=xxGib`ä¿®æ”¹æ¯ä¸ªGPUçš„æœ€å¤§ä½¿ç”¨å†…å­˜"

#: ../../getting_started/faq/llm/llm_faq.md:55 d235bd83545c476f8e12572658d1c723
#, fuzzy
msgid "Q4: Not Enough Memory"
msgstr "Q4: æœºå™¨æ˜¾å­˜ä¸å¤Ÿ "

#: ../../getting_started/faq/llm/llm_faq.md:57 b3243ed9147f42bba987d7f9b778e66f
msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
msgstr "DB-GPT æ”¯æŒ 8-bit quantization å’Œ 4-bit quantization."

#: ../../getting_started/faq/llm/llm_faq.md:59 1ddb9f94ab994bfebfee46d1c19888d4
msgid ""
"You can modify the setting `QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` "
"in `.env` file to use quantization(8-bit quantization is enabled by "
"default)."
msgstr "ä½ å¯ä»¥é€šè¿‡åœ¨.envæ–‡ä»¶è®¾ç½®`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`"

#: ../../getting_started/faq/llm/llm_faq.md:61 54b85daa3fb24b17b67a6da31d2be8b0
msgid ""
"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit"
" quantization can run with 48 GB of VRAM."
msgstr ""
"Llama-2-70b with 8-bit quantization å¯ä»¥è¿è¡Œåœ¨ 80 GB VRAMæœºå™¨ï¼Œ 4-bit "
"quantizationå¯ä»¥è¿è¡Œåœ¨ 48 GB VRAM"

#: ../../getting_started/faq/llm/llm_faq.md:63 097d680aed184fee9eceebee55a47ac1
msgid ""
"Note: you need to install the quantization dependencies with `pip install"
" -e \".[quantization]\"`"
msgstr ""

#: ../../getting_started/faq/llm/llm_faq.md:65 f3a51056043c49eb84471040f2b364aa
#, fuzzy
msgid "Q5: How to Add LLM Service dynamic local mode"
msgstr "Q5: æ€æ ·åŠ¨æ€æ–°å¢æ¨¡å‹æœåŠ¡"

#: ../../getting_started/faq/llm/llm_faq.md:67 43ee6b0f23814c94a4ddb2429801a5e1
msgid ""
"Now DB-GPT through multi-llm service switch, so how to add llm service "
"dynamic,"
msgstr "DB-GPTæ”¯æŒå¤šä¸ªæ¨¡å‹æœåŠ¡åˆ‡æ¢, æ€æ ·æ·»åŠ ä¸€ä¸ªæ¨¡å‹æœåŠ¡å‘¢"

#: ../../getting_started/faq/llm/llm_faq.md:78 c217bbf0d2b6425fa7a1c691b7704a8d
#, fuzzy
msgid "Q6: How to Add LLM Service dynamic in remote mode"
msgstr "Q6: æ€æ ·åŠ¨æ€æ–°å¢æ¨¡å‹æœåŠ¡"

#: ../../getting_started/faq/llm/llm_faq.md:79 195bdaa937a94c7aa0d8c6e1a5430d6e
msgid ""
"If you  deploy llm service in remote machine instance, and you want to "
"add model service to dbgpt server to manage"
msgstr "å¦‚æœä½ æƒ³åœ¨è¿œç¨‹æœºå™¨å®ä¾‹éƒ¨ç½²å¤§æ¨¡å‹æœåŠ¡å¹¶æ·»åŠ åˆ°æœ¬åœ°dbgpt_serverè¿›è¡Œç®¡ç†"

#: ../../getting_started/faq/llm/llm_faq.md:81 c64098b838a94821963a1d16e56497ff
msgid "use dbgpt start worker and set --controller_addr."
msgstr "ä½¿ç”¨1`dbgpt start worker`å‘½ä»¤å¹¶è®¾ç½®æ³¨å†Œåœ°å€--controller_addr"

#: ../../getting_started/faq/llm/llm_faq.md:91 cb12d5e9d9d24f14abc3ebea877a4b24
#, fuzzy
msgid "Q7: dbgpt command not found"
msgstr "Q7: dbgpt command not found"

#: ../../getting_started/faq/llm/llm_faq.md:97 f95cdccfa82d4b3eb2a23dd297131faa
#, fuzzy
msgid ""
"Q8: When starting the worker_manager on a cloud server and registering it"
" with the controller, it is noticed that the worker's exposed IP is a "
"private IP instead of a public IP, which leads to the inability to access"
" the service."
msgstr ""
"Q8: äº‘æœåŠ¡å™¨å¯åŠ¨worker_manageræ³¨å†Œåˆ°controlleræ—¶ï¼Œå‘ç°workeræš´éœ²çš„ipæ˜¯ç§ç½‘ip, "
"æ²¡æœ‰ä»¥å…¬ç½‘ipæš´éœ²ï¼Œå¯¼è‡´æœåŠ¡è®¿é—®ä¸åˆ°"

#: ../../getting_started/faq/llm/llm_faq.md:106
#: 739a2983f3484acf98e877dc12f4ccda
msgid "Q9: How to customize model path and prompt template"
msgstr "Q9: å¦‚ä½•è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„å’Œ prompt æ¨¡æ¿"

#: ../../getting_started/faq/llm/llm_faq.md:108
#: 8b82a33a311649c7850c30c00c987c72
#, fuzzy
msgid ""
"DB-GPT will read the model path from "
"`pilot.configs.model_config.LLM_MODEL_CONFIG` based on the `LLM_MODEL`. "
"Of course, you can use the environment variable `LLM_MODEL_PATH` to "
"specify the model path and `LLM_PROMPT_TEMPLATE` to specify your model "
"prompt template."
msgstr ""
"DB-GPT ä¼šæ ¹æ® `LLM_MODEL` ä» `pilot.configs.model_config.LLM_MODEL_CONFIG` "
"ä¸­è¯»å–æ¨¡å‹è·¯å¾„ã€‚å½“ç„¶ï¼Œä½ å¯ä»¥ä½¿ç”¨ç¯å¢ƒ `LLM_MODEL_PATH` æ¥æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œä»¥åŠä½¿ç”¨ `LLM_PROMPT_TEMPLATE` "
"æ¥æŒ‡å®šæ¨¡å‹çš„ prompt æ¨¡æ¿ã€‚"

#~ msgid ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."
#~ msgstr ""

