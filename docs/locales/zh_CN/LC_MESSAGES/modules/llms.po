# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 0.3.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-10 16:38+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../modules/llms.md:1 50913b0297da4485ae3fbdb6a90ba6ba
msgid "LLMs"
msgstr "大语言模型"

#: ../../modules/llms.md:3 608344bb1c6b49dc94487e400a92e1f9
#, python-format
msgid ""
"In the underlying large model integration, we have designed an open "
"interface that supports integration with various large models. At the "
"same time, we have a very strict control and evaluation mechanism for the"
" effectiveness of the integrated models. In terms of accuracy, the "
"integrated models need to align with the capability of ChatGPT at a level"
" of 85% or higher. We use higher standards to select models, hoping to "
"save users the cumbersome testing and evaluation process in the process "
"of use."
msgstr "在底层大模型接入中，我们设计了开放的接口，支持对接多种大模型。同时对于接入模型的效果，我们有非常严格的把控与评审机制。对大模型能力上与ChatGPT对比，在准确率上需要满足85%以上的能力对齐。我们用更高的标准筛选模型，是期望在用户使用过程中，可以省去前面繁琐的测试评估环节。"

#: ../../modules/llms.md:5 138db7b30c624665930a7d532b557ea8
msgid "Multi LLMs Usage"
msgstr "多模型使用"

#: ../../modules/llms.md:6 1c82e93d4b334b209d7f3e63c82efb0a
msgid ""
"To use multiple models, modify the LLM_MODEL parameter in the .env "
"configuration file to switch between the models."
msgstr "如果要使用不同的模型，请修改.env配置文件中的LLM MODEL参数以在模型之间切换。"

#: ../../modules/llms.md:8 5b1846c29dbe42c4b95a2e26dc85a358
msgid ""
"Notice: you can create .env file from .env.template, just use command "
"like this:"
msgstr "注意:你可以从 .env.template 创建 .env 文件。只需使用如下命令:"

#: ../../modules/llms.md:14 95ecc02369ab4075bd245662809e4cc9
#, fuzzy
msgid ""
"now we support models vicuna-13b, vicuna-7b, chatglm-6b, flan-t5-base, "
"guanaco-33b-merged, falcon-40b, gorilla-7b, llama-2-7b, llama-2-13b, "
"baichuan-7b, baichuan-13b"
msgstr ""
"现在我们支持的模型有vicuna-13b, vicuna-7b, chatglm-6b, flan-t5-base, guanaco-33b-"
"merged, falcon-40b, gorilla-7b."

#: ../../modules/llms.md:16 9397c2959c244f5595c74b417c2b7edb
msgid ""
"if you want use other model, such as chatglm-6b, you just need update "
".env config file."
msgstr "如果你想使用其他模型，比如chatglm-6b, 仅仅需要修改.env 配置文件"

#: ../../modules/llms.md:20 fe143c38ed9f48adb9042b648529e64c
msgid ""
"or chatglm2-6b, which  is the second-generation version of the open-"
"source bilingual (Chinese-English) chat model ChatGLM-6B."
msgstr ""

#: ../../modules/llms.md:27 d43eb7fe0a4949a1a4772cf8e91b028c
msgid "Run Model with cpu."
msgstr "用CPU运行模型"

#: ../../modules/llms.md:28 e6caaf6efa774131b8bef036c440db19
msgid ""
"we alse support smaller models, like gpt4all.  you can use it with "
"cpu/mps(M1/M2), Download from [gpt4all model](https://gpt4all.io/models"
"/ggml-gpt4all-j-v1.3-groovy.bin)"
msgstr ""
"我们也支持一些小模型，你可以通过CPU/MPS(M1、M2)运行, 模型下载[gpt4all](https://gpt4all.io/models"
"/ggml-gpt4all-j-v1.3-groovy.bin)"

#: ../../modules/llms.md:30 8fc07b5d2d79496083ee3a279a131eb8
msgid "put it in the models path, then change .env config."
msgstr "将模型放在models路径, 修改.env 配置文件"

#: ../../modules/llms.md:35 ddd90d901eaf4867934ea58fdb2398ab
msgid ""
"DB-GPT provides a model load adapter and chat adapter. load adapter which"
" allows you to easily adapt load different LLM models by inheriting the "
"BaseLLMAdapter. You just implement match() and loader() method."
msgstr ""
"DB-GPT提供了多模型适配器load adapter和chat adapter.load adapter通过继承BaseLLMAdapter类,"
" 实现match和loader方法允许你适配不同的LLM."

#: ../../modules/llms.md:37 a1c446ab4dbe43d9b2558c5fad542069
msgid "vicuna llm load adapter"
msgstr "vicuna llm load adapter"

#: ../../modules/llms.md:54 fb8655afefc4411ea1ef97230098e23c
msgid "chatglm load adapter"
msgstr "chatglm load adapter"

#: ../../modules/llms.md:81 5c8c2b73419049e2b88e5e9d40149187
msgid ""
"chat adapter which allows you to easily adapt chat different LLM models "
"by inheriting the BaseChatAdpter.you just implement match() and "
"get_generate_stream_func() method"
msgstr ""
"chat "
"adapter通过继承BaseChatAdpter允许你通过实现match和get_generate_stream_func方法允许你适配不同的LLM."

#: ../../modules/llms.md:83 2d6be71d447846e4921f2e7fde678ae5
msgid "vicuna llm chat adapter"
msgstr "vicuna llm chat adapter"

#: ../../modules/llms.md:95 e50e6ef380e74402a131c253f5ef1552
msgid "chatglm llm chat adapter"
msgstr "chatglm llm chat adapter"

#: ../../modules/llms.md:108 e1825c1aa2384d4fbd40934ec32a3cf4
msgid ""
"if you want to integrate your own model, just need to inheriting "
"BaseLLMAdaper and BaseChatAdpter and implement the methods"
msgstr "如果你想集成自己的模型，只需要继承BaseLLMAdaper和BaseChatAdpter类，然后实现里面的方法即可"

#: ../../modules/llms.md:110 cfc357047ec14638934cc0514514718d
#, fuzzy
msgid "Multi Proxy LLMs"
msgstr "多模型使用"

#: ../../modules/llms.md:111 5a08dd4f88d4482dbac19403954f60bf
msgid "1. Openai proxy"
msgstr "1. Openai proxy"

#: ../../modules/llms.md:112 4984400803484dcc8de78252e3f74f9b
msgid ""
"If you haven't deployed a private infrastructure for a large model, or if"
" you want to use DB-GPT in a low-cost and high-efficiency way, you can "
"also use OpenAI's large model as your underlying model."
msgstr ""

#: ../../modules/llms.md:114 1696bedec29d4385b547a2d977720b76
msgid ""
"If your environment deploying DB-GPT has access to OpenAI, then modify "
"the .env configuration file as below will work."
msgstr "如果本地能够访问OpenAI，修改.env文件即可"

#: ../../modules/llms.md:122 908d72c6bdfa40eab24c8966e20f55df
msgid ""
"If you can't access OpenAI locally but have an OpenAI proxy service, you "
"can configure as follows."
msgstr "如果不能访问openapi服务,你可以如下配置"

#: ../../modules/llms.md:130 141d43ea2f4d49778d5d35ec644ee7f6
msgid "2. Bard Proxy"
msgstr "2. Bard Proxy"

#: ../../modules/llms.md:131 47471243adc94f95807e9c0c2a112c20
msgid ""
"If your environment deploying DB-GPT has access to <a "
"href=\"https://bard.google.com/\">Bard</a> (F12-> application-> __Secure-"
"1PSID), then modify the .env configuration file as below will work."
msgstr ""

#: ../../modules/llms.md:139 c6a139d829514d299d92abaa0695b681
msgid ""
"If you want to use your own bard proxy server like <a "
"href=\"https://github.com/eosphoros-ai/Bard-Proxy\">Bard-Proxy</a>, so "
"that you can deploy DB-GPT on your PC easily."
msgstr "如果你想使用 bard proxy server <a "
"href=\"https://github.com/eosphoros-ai/Bard-Proxy\">Bard-Proxy</a> 你可以轻松部署 bard proxy server"

#~ msgid "Multi Proxy LLMs"
#~ msgstr "多模型使用"

#~ msgid "1. Openai proxy"
#~ msgstr "Openai代理"

#~ msgid ""
#~ "If you haven't deployed a private "
#~ "infrastructure for a large model, or "
#~ "if you want to use DB-GPT in"
#~ " a low-cost and high-efficiency "
#~ "way, you can also use OpenAI's "
#~ "large model as your underlying model."
#~ msgstr "如果你没有部署私有大模型的资源，或者你想使用低成本启动DB-GPT,你可以使用openai的大模型作为你的底层模型"

#~ msgid ""
#~ "If your environment deploying DB-GPT "
#~ "has access to OpenAI, then modify "
#~ "the .env configuration file as below "
#~ "will work."
#~ msgstr "如果你的环境能够访问openai，你只需要参考如下修改.env配置文件即可"

#~ msgid ""
#~ "If you can't access OpenAI locally "
#~ "but have an OpenAI proxy service, "
#~ "you can configure as follows."
#~ msgstr "如果你本地无法访问openai，但是你有一个openai的代理服务，你可以参考如下配置"

#~ msgid ""
#~ "If your environment deploying DB-GPT "
#~ "has access to https://bard.google.com/ (F12->"
#~ " application-> __Secure-1PSID), then modify"
#~ " the .env configuration file as below"
#~ " will work."
#~ msgstr ""
