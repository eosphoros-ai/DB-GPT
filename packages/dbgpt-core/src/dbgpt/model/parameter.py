"""Models Configuration Parameters."""

from dataclasses import dataclass, field
from enum import Enum
from typing import List, Optional, Tuple, Union

from dbgpt.core.interface.parameter import (
    EmbeddingDeployModelParameters,
    LLMDeployModelParameters,
    RerankerDeployModelParameters,
)
from dbgpt.model.base import ModelType
from dbgpt.util.i18n_utils import _
from dbgpt.util.parameter_utils import BaseParameters, BaseServerParameters


class WorkerType(str, Enum):
    LLM = "llm"
    TEXT2VEC = "text2vec"
    RERANKER = "reranker"

    @staticmethod
    def values():
        return [item.value for item in WorkerType]

    @staticmethod
    def to_worker_key(worker_name, worker_type: Union[str, "WorkerType"]) -> str:
        """Generate worker key from worker name and worker type

        Args:
            worker_name (str): Worker name(eg., chatglm2-6b)
            worker_type (Union[str, "WorkerType"]):
                Worker type(eg., 'llm', or [`WorkerType.LLM`])

        Returns:
            str: Generated worker key
        """
        if "@" in worker_name:
            raise ValueError(f"Invaild symbol '@' in your worker name {worker_name}")
        if isinstance(worker_type, WorkerType):
            worker_type = worker_type.value
        return f"{worker_name}@{worker_type}"

    @staticmethod
    def parse_worker_key(worker_key: str) -> Tuple[str, str]:
        """Parse worker name and worker type from worker key

        Args:
            worker_key (str): Worker key generated by [`WorkerType.to_worker_key`]

        Returns:
            Tuple[str, str]: Worker name and worker type
        """
        return tuple(worker_key.split("@"))

    @classmethod
    def from_str(cls, value: str) -> "WorkerType":
        """Convert a string to an Enum value."""
        try:
            return cls(value)
        except ValueError:
            raise ValueError(
                f"Invalid value '{value}' for {cls.__name__}. "
                f"Valid values are {cls.values()}"
            )


@dataclass
class ModelControllerParameters(BaseServerParameters):
    port: Optional[int] = field(
        default=8000, metadata={"help": "Model Controller deploy port"}
    )
    registry_type: Optional[str] = field(
        default="embedded",
        metadata={
            "help": "Registry type: embedded, database...",
            "valid_values": ["embedded", "database"],
        },
    )
    registry_db_type: Optional[str] = field(
        default="mysql",
        metadata={
            "help": "Registry database type, now only support sqlite and mysql, it is "
            "valid when registry_type is database",
            "valid_values": ["mysql", "sqlite"],
        },
    )
    registry_db_name: Optional[str] = field(
        default="dbgpt",
        metadata={
            "help": "Registry database name, just for database, it is valid when "
            "registry_type is database, please set to full database path for sqlite"
        },
    )
    registry_db_host: Optional[str] = field(
        default=None,
        metadata={
            "help": "Registry database host, just for database, it is valid when "
            "registry_type is database"
        },
    )
    registry_db_port: Optional[int] = field(
        default=None,
        metadata={
            "help": "Registry database port, just for database, it is valid when "
            "registry_type is database"
        },
    )
    registry_db_user: Optional[str] = field(
        default=None,
        metadata={
            "help": "Registry database user, just for database, it is valid when "
            "registry_type is database"
        },
    )
    registry_db_password: Optional[str] = field(
        default=None,
        metadata={
            "help": "Registry database password, just for database, it is valid when "
            "registry_type is database. We recommend to use environment variable to "
            "store password, you can set it in your environment variable like "
            "export CONTROLLER_REGISTRY_DB_PASSWORD='your_password'"
        },
    )
    registry_db_pool_size: Optional[int] = field(
        default=5,
        metadata={
            "help": "Registry database pool size, just for database, it is valid when "
            "registry_type is database"
        },
    )
    registry_db_max_overflow: Optional[int] = field(
        default=10,
        metadata={
            "help": "Registry database max overflow, just for database, it is valid "
            "when registry_type is database"
        },
    )

    heartbeat_interval_secs: Optional[int] = field(
        default=20, metadata={"help": "The interval for checking heartbeats (seconds)"}
    )
    heartbeat_timeout_secs: Optional[int] = field(
        default=60,
        metadata={
            "help": "The timeout for checking heartbeats (seconds), it will be set "
            "unhealthy if the worker is not responding in this time"
        },
    )

    log_file: Optional[str] = field(
        default="dbgpt_model_controller.log",
        metadata={
            "help": "The filename to store log",
        },
    )
    tracer_file: Optional[str] = field(
        default="dbgpt_model_controller_tracer.jsonl",
        metadata={
            "help": "The filename to store tracer span records",
        },
    )
    tracer_storage_cls: Optional[str] = field(
        default=None,
        metadata={
            "help": "The storage class to storage tracer span records",
        },
    )


@dataclass
class ModelAPIServerParameters(BaseServerParameters):
    port: Optional[int] = field(
        default=8100, metadata={"help": "Model API server deploy port"}
    )
    controller_addr: Optional[str] = field(
        default="http://127.0.0.1:8000",
        metadata={"help": "The Model controller address to connect"},
    )

    api_keys: Optional[str] = field(
        default=None,
        metadata={"help": "Optional list of comma separated API keys"},
    )
    embedding_batch_size: Optional[int] = field(
        default=None, metadata={"help": "Embedding batch size"}
    )
    ignore_stop_exceeds_error: Optional[bool] = field(
        default=False, metadata={"help": "Ignore exceeds stop words error"}
    )

    log_file: Optional[str] = field(
        default="dbgpt_model_apiserver.log",
        metadata={
            "help": "The filename to store log",
        },
    )
    tracer_file: Optional[str] = field(
        default="dbgpt_model_apiserver_tracer.jsonl",
        metadata={
            "help": "The filename to store tracer span records",
        },
    )
    tracer_storage_cls: Optional[str] = field(
        default=None,
        metadata={
            "help": "The storage class to storage tracer span records",
        },
    )


@dataclass
class BaseModelParameters(BaseParameters):
    model_name: str = field(metadata={"help": "Model name", "tags": "fixed"})
    model_path: str = field(metadata={"help": "Model path", "tags": "fixed"})


@dataclass
class ModelWorkerParameters(BaseServerParameters, BaseModelParameters):
    worker_type: Optional[str] = field(
        default=None,
        metadata={"valid_values": WorkerType.values(), "help": "Worker type"},
    )
    model_alias: Optional[str] = field(
        default=None,
        metadata={"help": "model alias"},
    )
    worker_class: Optional[str] = field(
        default=None,
        metadata={"help": "Model worker class, dbgpt.model.cluster.DefaultModelWorker"},
    )
    model_type: Optional[str] = field(
        default="huggingface",
        metadata={
            "help": "Model type: huggingface, llama.cpp, proxy and vllm",
            "tags": "fixed",
        },
    )

    port: Optional[int] = field(
        default=8001, metadata={"help": "Model worker deploy port"}
    )
    limit_model_concurrency: Optional[int] = field(
        default=5, metadata={"help": "Model concurrency limit"}
    )
    standalone: Optional[bool] = field(
        default=False,
        metadata={"help": "Standalone mode. If True, embedded Run ModelController"},
    )
    register: Optional[bool] = field(
        default=True, metadata={"help": "Register current worker to model controller"}
    )
    worker_register_host: Optional[str] = field(
        default=None,
        metadata={
            "help": "The ip address of current worker to register to ModelController. "
            "If None, the address is automatically determined"
        },
    )
    controller_addr: Optional[str] = field(
        default=None, metadata={"help": "The Model controller address to register"}
    )
    send_heartbeat: Optional[bool] = field(
        default=True, metadata={"help": "Send heartbeat to model controller"}
    )
    heartbeat_interval: Optional[int] = field(
        default=20, metadata={"help": "The interval for sending heartbeats (seconds)"}
    )

    log_file: Optional[str] = field(
        default="dbgpt_model_worker_manager.log",
        metadata={
            "help": "The filename to store log",
        },
    )
    tracer_file: Optional[str] = field(
        default="dbgpt_model_worker_manager_tracer.jsonl",
        metadata={
            "help": "The filename to store tracer span records",
        },
    )
    tracer_storage_cls: Optional[str] = field(
        default=None,
        metadata={
            "help": "The storage class to storage tracer span records",
        },
    )


@dataclass
class ModelParameters(BaseModelParameters):
    device: Optional[str] = field(
        default=None,
        metadata={
            "help": "Device to run model. If None, the device is automatically "
            "determined"
        },
    )
    model_type: Optional[str] = field(
        default="huggingface",
        metadata={
            "help": "Model type: huggingface, llama.cpp, proxy and vllm",
            "tags": "fixed",
        },
    )
    prompt_template: Optional[str] = field(
        default=None,
        metadata={
            "help": "Prompt template. If None, the prompt template is automatically "
            "determined from model path"
        },
    )
    max_context_size: Optional[int] = field(
        default=4096, metadata={"help": "Maximum context size"}
    )

    num_gpus: Optional[int] = field(
        default=None,
        metadata={
            "help": "The number of gpus you expect to use, if it is empty, use all of "
            "them as much as possible"
        },
    )
    max_gpu_memory: Optional[str] = field(
        default=None,
        metadata={
            "help": "The maximum memory limit of each GPU, only valid in multi-GPU "
            "configuration"
        },
    )
    cpu_offloading: Optional[bool] = field(
        default=False, metadata={"help": "CPU offloading"}
    )
    load_8bit: Optional[bool] = field(
        default=False, metadata={"help": "8-bit quantization"}
    )
    load_4bit: Optional[bool] = field(
        default=False, metadata={"help": "4-bit quantization"}
    )
    quant_type: Optional[str] = field(
        default="nf4",
        metadata={
            "valid_values": ["nf4", "fp4"],
            "help": "Quantization datatypes, `fp4` (four bit float) and `nf4` "
            "(normal four bit float), only valid when load_4bit=True",
        },
    )
    use_double_quant: Optional[bool] = field(
        default=True,
        metadata={"help": "Nested quantization, only valid when load_4bit=True"},
    )
    compute_dtype: Optional[str] = field(
        default=None,
        metadata={
            "valid_values": ["bfloat16", "float16", "float32"],
            "help": "Model compute type",
        },
    )
    trust_remote_code: Optional[bool] = field(
        default=True, metadata={"help": "Trust remote code"}
    )
    verbose: Optional[bool] = field(
        default=False, metadata={"help": "Show verbose output."}
    )


@dataclass
class LlamaCppModelParameters(LLMDeployModelParameters):
    provider: str = ModelType.LLAMA_CPP
    path: Optional[str] = field(
        default=None,
        metadata={
            "help": _("The path of the model, if you want to deploy a local model."),
        },
    )
    device: Optional[str] = field(
        default=None,
        metadata={
            "help": _(
                "Device to run model. If None, the device is automatically determined"
            )
        },
    )
    seed: Optional[int] = field(
        default=-1, metadata={"help": "Random seed for llama-cpp models. -1 for random"}
    )
    n_threads: Optional[int] = field(
        default=None,
        metadata={
            "help": "Number of threads to use. If None, the number of threads is "
            "automatically determined"
        },
    )
    n_batch: Optional[int] = field(
        default=512,
        metadata={
            "help": "Maximum number of prompt tokens to batch together when calling "
            "llama_eval"
        },
    )
    n_gpu_layers: Optional[int] = field(
        default=1000000000,
        metadata={
            "help": "Number of layers to offload to the GPU, Set this to 1000000000 to "
            "offload all layers to the GPU."
        },
    )
    n_gqa: Optional[int] = field(
        default=None,
        metadata={"help": "Grouped-query attention. Must be 8 for llama-2 70b."},
    )
    rms_norm_eps: Optional[float] = field(
        default=5e-06, metadata={"help": "5e-6 is a good value for llama-2 models."}
    )
    cache_capacity: Optional[str] = field(
        default=None,
        metadata={
            "help": "Maximum cache capacity. Examples: 2000MiB, 2GiB. When provided "
            "without units, bytes will be assumed. "
        },
    )
    prefer_cpu: Optional[bool] = field(
        default=False,
        metadata={
            "help": "If a GPU is available, it will be preferred by default, unless "
            "prefer_cpu=False is configured."
        },
    )

    @property
    def real_model_path(self) -> Optional[str]:
        """Get the real model path.

        If deploy model is not local, return None.
        """
        return self._resolve_root_path(self.path)

    @property
    def real_device(self) -> Optional[str]:
        """Get the real device."""
        return self.device or super().real_device


@dataclass
class DeployModelWorkerParameters(BaseParameters):
    host: Optional[str] = field(
        default="0.0.0.0", metadata={"help": "The host IP address to bind to."}
    )
    port: Optional[int] = field(
        default=None, metadata={"help": "The port number to bind to."}
    )


@dataclass
class ModelServiceConfig(BaseParameters):
    """Model service configuration."""

    worker: DeployModelWorkerParameters = field(
        default_factory=DeployModelWorkerParameters,
        metadata={"help": _("Model worker")},
    )
    api: ModelAPIServerParameters = field(
        default_factory=ModelControllerParameters, metadata={"help": _("Model API")}
    )
    controller: ModelControllerParameters = field(
        default_factory=ModelControllerParameters,
        metadata={"help": _("Model controller")},
    )


@dataclass
class ModelsDeployParameters(BaseParameters):
    default_llm: Optional[str] = field(
        default=None,
        metadata={
            "help": _(
                "Default LLM model name, used to specify which model to use when you "
                "have multiple LLMs"
            ),
        },
    )
    default_embedding: Optional[str] = field(
        default=None,
        metadata={
            "help": _(
                "Default embedding model name, used to specify which model to use when "
                "you have multiple embedding models"
            ),
        },
    )
    default_reranker: Optional[str] = field(
        default=None,
        metadata={
            "help": _(
                "Default reranker model name, used to specify which model to use when "
                "you have multiple reranker models"
            ),
        },
    )
    llms: List[LLMDeployModelParameters] = field(
        default_factory=list,
        metadata={
            "help": "LLM model deploy configuration",
        },
    )
    embeddings: List[EmbeddingDeployModelParameters] = field(
        default_factory=list,
        metadata={
            "help": "Embedding model deploy configuration",
        },
    )
    rerankers: List[RerankerDeployModelParameters] = field(
        default_factory=list,
        metadata={
            "help": "Reranker model deploy configuration",
        },
    )

    def __post_init__(self):
        """Post init method."""
        llms, embeds, rerankers = [], [], []
        for llm in self.llms:
            llms.append(llm.name)
        for embedding in self.embeddings:
            embeds.append(embedding.name)
        for reranker in self.rerankers:
            rerankers.append(reranker.name)
        if not self.default_llm and llms:
            self.default_llm = llms[0]
        if not self.default_embedding and embeds:
            self.default_embedding = embeds[0]
        if not self.default_reranker and rerankers:
            self.default_reranker = rerankers[0]
