# Korean translations for PACKAGE package.
# Copyright (C) 2025 THE PACKAGE'S COPYRIGHT HOLDER
# This file is distributed under the same license as the PACKAGE package.
# Automatically generated, 2025.
#
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-02-23 13:40+0800\n"
"PO-Revision-Date: 2025-02-23 13:40+0800\n"
"Last-Translator: Automatically generated\n"
"Language-Team: none\n"
"Language: ko\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/siliconflow.py:35
msgid "The base url of the SiliconFlow API."
msgstr "SiliconFlow API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/siliconflow.py:42
msgid "The API key of the SiliconFlow API."
msgstr "SiliconFlow API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:38
msgid "The base url of the OpenAI API."
msgstr "OpenAI API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:45
msgid "The API key of the OpenAI API."
msgstr "OpenAI API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:52
msgid "The type of the OpenAI API, if you use Azure, it can be: azure"
msgstr "OpenAI API의 유형입니다. Azure를 사용하는 경우 'azure'로 설정할 수 있습니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:58
msgid "The version of the OpenAI API."
msgstr "OpenAI API의 버전입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:65
#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/wenxin.py:65
msgid ""
"The context length of the OpenAI API. If None, it is determined by the model."
msgstr "OpenAI API의 컨텍스트 길이입니다. None으로 설정하면 모델에 따라 결정됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:73
msgid "The http or https proxy to use openai"
msgstr "OpenAI를 사용할 때의 HTTP 또는 HTTPS 프록시입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:77
#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/wenxin.py:72
#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:278
#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:43
msgid "Model concurrency limit"
msgstr "모델 동시성 제한"

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:91
msgid "OpenAI LLM Client"
msgstr "OpenAI LLM 클라이언트"

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:96
msgid "OpenAI API Key"
msgstr "OpenAI API 키"

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:102
msgid ""
"OpenAI API Key, not required if you have set OPENAI_API_KEY environment "
"variable."
msgstr "OpenAI API 키입니다. OPENAI_API_KEY 환경 변수를 설정한 경우 필요하지 않습니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:107
msgid "OpenAI API Base"
msgstr "OpenAI API 기본 주소"

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/chatgpt.py:113
msgid ""
"OpenAI API Base, not required if you have set OPENAI_API_BASE environment "
"variable."
msgstr "OpenAI API 기본 주소입니다. OPENAI_API_BASE 환경 변수를 설정한 경우 필요하지 않습니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/zhipu.py:34
msgid "The base url of the Zhipu API."
msgstr "지푸 API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/zhipu.py:41
msgid "The API key of the Zhipu API."
msgstr "지푸 API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/moonshot.py:34
msgid "The base url of the Moonshot API."
msgstr "문숏 API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/moonshot.py:41
msgid "The API key of the Moonshot API."
msgstr "문숏 API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/gitee.py:35
msgid "The base url of the Gitee API."
msgstr "지티 API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/gitee.py:42
msgid "The API key of the Gitee API."
msgstr "지티 API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/deepseek.py:35
msgid "The base url of the DeepSeek API."
msgstr "딥시크 API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/deepseek.py:42
msgid "The API key of the DeepSeek API."
msgstr "딥시크 API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/ollama.py:29
msgid "The base url of the Ollama API."
msgstr "Ollama API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/yi.py:34
msgid "The base url of the Yi API."
msgstr "Yi API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/yi.py:41
msgid "The API key of the Yi API."
msgstr "Yi API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/spark.py:29
msgid "The base url of the Spark API."
msgstr "Spark API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/spark.py:36
msgid "The API key of the Spark API."
msgstr "Spark API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/baichuan.py:34
msgid "The base url of the Baichuan API."
msgstr "Baichuan API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/baichuan.py:41
msgid "The API key of the Baichuan API."
msgstr "Baichuan API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/gemini.py:53
msgid "The base url of the gemini API."
msgstr "Gemini API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/gemini.py:60
msgid "The API key of the gemini API."
msgstr "Gemini API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/tongyi.py:39
msgid "The base url of the tongyi API."
msgstr "Tongyi API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/tongyi.py:46
msgid "The API key of the tongyi API."
msgstr "Tongyi API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/volcengine.py:34
msgid "The base url of the Volcengine API."
msgstr "Volcengine API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/volcengine.py:41
msgid "The API key of the Volcengine API."
msgstr "Volcengine API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/wenxin.py:49
msgid "The API key of the Wenxin API."
msgstr "Wenxin API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/wenxin.py:56
msgid "The API secret key of the Wenxin API."
msgstr "Wenxin API의 API 비밀 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/claude.py:45
msgid "The base url of the claude API."
msgstr "Claude API의 기본 URL입니다."

#:../packages/dbgpt-core/src/dbgpt/model/proxy/llms/claude.py:52
msgid "The API key of the claude API."
msgstr "Claude API의 API 키입니다."

#:../packages/dbgpt-core/src/dbgpt/model/cli.py:312
msgid "The name of model"
msgstr "모델의 이름"

#:../packages/dbgpt-core/src/dbgpt/model/cli.py:319
msgid "System prompt"
msgstr "시스템 프롬프트"

#:../packages/dbgpt-core/src/dbgpt/model/cli.py:443
msgid "The config file to start server"
msgstr "서버를 시작하는 데 사용되는 구성 파일"

#:../packages/dbgpt-core/src/dbgpt/model/cli.py:451
msgid ""
"Run in daemon mode. It will run in the background. If you want to stop it, "
"use `dbgpt stop` command"
msgstr "데몬 모드로 실행합니다. 백그라운드에서 실행됩니다. 중지하려면 `dbgpt stop` 명령을 사용하세요."

#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:20
msgid "Default LLM Client"
msgstr "기본 LLM 클라이언트"

#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:23
msgid "Default LLM client(Connect to your DB-GPT model serving)"
msgstr "기본 LLM 클라이언트(DB-GPT 모델 서비스에 연결)"

#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:26
#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:128
msgid "Auto Convert Message"
msgstr "메시지 자동 변환"

#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:32
#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:134
msgid ""
"Whether to auto convert the messages that are not supported by the LLM to a "
"compatible format"
msgstr "LLM에서 지원하지 않는 메시지를 호환 가능한 형식으로 자동 변환할지 여부"

#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:114
msgid "Remote LLM Client"
msgstr "원격 LLM 클라이언트"

#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:117
msgid "Remote LLM client(Connect to the remote DB-GPT model serving)"
msgstr "원격 LLM 클라이언트(원격 DB-GPT 모델 서비스에 연결)"

#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:120
msgid "Controller Address"
msgstr "컨트롤러 주소"

#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:124
msgid "http://127.0.0.1:8000"
msgstr "http://127.0.0.1:8000"

#:../packages/dbgpt-core/src/dbgpt/model/cluster/client.py:125
msgid "Model controller address"
msgstr "모델 컨트롤러 주소"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/hf_adapter.py:33
#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:29
#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:25
msgid "The path of the model, if you want to deploy a local model."
msgstr "로컬 모델을 배포하려는 경우 모델의 경로입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/hf_adapter.py:41
#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:64
#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:37
#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:33
msgid "Device to run model. If None, the device is automatically determined"
msgstr "모델을 실행할 장치입니다. None인 경우 장치가 자동으로 결정됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/hf_adapter.py:47
#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:96
msgid "Trust remote code or not."
msgstr "원격 코드를 신뢰할지 여부입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/hf_adapter.py:52
msgid "The quantization parameters."
msgstr "양자화 매개변수입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/hf_adapter.py:59
msgid ""
"Whether to use low CPU memory usage mode. It can reduce the memory when "
"loading the model, if you load your model with quantization, it will be True "
"by default. You must install `accelerate` to make it work."
msgstr "낮은 CPU 메모리 사용 모드를 사용할지 여부입니다. 모델을 로드할 때 메모리를 줄일 수 있습니다. 양자화를 사용하여 모델을 로드하는 경우 기본적으로 True입니다. 이 기능을 사용하려면 `accelerate`를 설치해야 합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/hf_adapter.py:70
msgid ""
"The number of gpus you expect to use, if it is empty, use all of them as "
"much as possible"
msgstr "사용하려는 GPU의 수입니다. 비어 있으면 가능한 한 모든 GPU를 사용합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/hf_adapter.py:79
msgid ""
"The maximum memory limit of each GPU, only valid in multi-GPU configuration, "
"eg: 10GiB, 24GiB"
msgstr "각 GPU의 최대 메모리 제한입니다. 다중 GPU 구성에서만 유효합니다. 예: 10GiB, 24GiB"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/hf_adapter.py:87
msgid "The dtype of the model, default is None."
msgstr "모델의 데이터 유형입니다. 기본값은 None입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:45
msgid "Local model file path"
msgstr "로컬 모델 파일 경로"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:51
msgid "Hugging Face repository for model download"
msgstr "모델 다운로드를 위한 Hugging Face 저장소"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:56
msgid "Model file name in the Hugging Face repository"
msgstr "Hugging Face 저장소의 모델 파일 이름"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:71
msgid "Path to the server binary executable"
msgstr "서버 바이너리 실행 파일의 경로"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:75
msgid "Host address to bind the server"
msgstr "서버를 바인딩할 호스트 주소"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:80
msgid "Port to bind the server. 0 for random available port"
msgstr "서버를 바인딩할 포트입니다. 0은 임의의 사용 가능한 포트를 의미합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:85
msgid "Sampling temperature for text generation"
msgstr "텍스트 생성을 위한 샘플링 온도"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:89
msgid "Random seed for reproducibility"
msgstr "재현성을 위한 랜덤 시드"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:92
msgid "Enable debug mode"
msgstr "디버그 모드 활성화"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:97
msgid "Model download URL (env: LLAMA_ARG_MODEL_URL)"
msgstr "모델 다운로드 URL (환경 변수: LLAMA_ARG_MODEL_URL)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:101
msgid "Draft model file path"
msgstr "초안 모델 파일 경로"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:109
msgid ""
"Number of threads to use during generation (default: -1) (env: "
"LLAMA_ARG_THREADS)"
msgstr "생성 중에 사용할 스레드 수 (기본값: -1) (환경 변수: LLAMA_ARG_THREADS)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:119
msgid ""
"Number of layers to store in VRAM (env: LLAMA_ARG_N_GPU_LAYERS), set "
"1000000000 to use all layers"
msgstr "VRAM에 저장할 레이어 수 (환경 변수: LLAMA_ARG_N_GPU_LAYERS), 모든 레이어를 사용하려면 1000000000으로 설정"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:129
msgid "Logical maximum batch size (default: 2048) (env: LLAMA_ARG_BATCH)"
msgstr "논리적 최대 배치 크기 (기본값: 2048) (환경 변수: LLAMA_ARG_BATCH)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:138
msgid "Physical maximum batch size (default: 512) (env: LLAMA_ARG_UBATCH)"
msgstr "물리적 최대 배치 크기 (기본값: 512) (환경 변수: LLAMA_ARG_UBATCH)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:148
msgid ""
"Size of the prompt context (default: 4096, 0 = loaded from model) (env: "
"LLAMA_ARG_CTX_SIZE)"
msgstr "프롬프트 컨텍스트 크기 (기본값: 4096, 0 = 모델에서 로드) (환경 변수: LLAMA_ARG_CTX_SIZE)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:155
msgid "Group-attention factor (default: 1)"
msgstr "그룹 어텐션 계수 (기본값: 1)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:159
msgid "Group-attention width (default: 512)"
msgstr "그룹 어텐션 너비 (기본값: 512)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:167
msgid ""
"Number of tokens to predict (default: -1, -1 = infinity, -2 = until context "
"filled) (env: LLAMA_ARG_N_PREDICT)"
msgstr "예측할 토큰 수 (기본값: -1, -1 = 무한대, -2 = 컨텍스트가 채워질 때까지) (환경 변수: LLAMA_ARG_N_PREDICT)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:176
msgid "Path to save slot kv cache (default: disabled)"
msgstr "슬롯 키-값 캐시를 저장할 경로 (기본값: 비활성화)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:180
msgid "Number of slots for KV cache"
msgstr "KV 캐시를 위한 슬롯 수"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:186
msgid "Enable continuous batching (a.k.a dynamic batching)"
msgstr "연속 배치 처리(즉, 동적 배치 처리) 활성화"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:193
msgid ""
"Restrict to only support embedding use case; use only with dedicated "
"embedding models (env: LLAMA_ARG_EMBEDDINGS)"
msgstr "임베딩 사용 사례만 지원하도록 제한; 전용 임베딩 모델과 함께만 사용 (환경 변수: LLAMA_ARG_EMBEDDINGS)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:202
msgid "Enable reranking endpoint on server (env: LLAMA_ARG_RERANKING)"
msgstr "서버에서 재정렬 엔드포인트 활성화 (환경 변수: LLAMA_ARG_RERANKING)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:210
msgid ""
"Enable prometheus compatible metrics endpoint (env: "
"LLAMA_ARG_ENDPOINT_METRICS)"
msgstr "프로메테우스 호환 메트릭 엔드포인트 활성화 (환경 변수: LLAMA_ARG_ENDPOINT_METRICS)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:220
msgid "Enable slots monitoring endpoint (env: LLAMA_ARG_ENDPOINT_SLOTS)"
msgstr "슬롯 모니터링 엔드포인트 활성화 (환경 변수: LLAMA_ARG_ENDPOINT_SLOTS)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:230
msgid ""
"Number of tokens to draft for speculative decoding (default: 16) (env: "
"LLAMA_ARG_DRAFT_MAX)"
msgstr "추측 디코딩을 위해 작성할 토큰 수(기본값: 16) (환경 변수: LLAMA_ARG_DRAFT_MAX)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:237
msgid "Same as draft"
msgstr "초안과 동일"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:244
msgid ""
"Minimum number of draft tokens to use for speculative decoding (default: 5)"
msgstr "추측 디코딩에 사용할 최소 초안 토큰 수(기본값: 5)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:253
msgid "API key to use for authentication (env: LLAMA_API_KEY)"
msgstr "인증에 사용할 API 키 (환경 변수: LLAMA_API_KEY)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:260
msgid "Path to LoRA adapter (can be repeated to use multiple adapters)"
msgstr "LoRA 어댑터 경로(여러 어댑터를 사용하려면 반복 가능)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:267
msgid "Disables context shift on infinite text generation"
msgstr "무한 텍스트 생성 시 컨텍스트 이동 비활성화"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:271
msgid "Disable web UI"
msgstr "웹 UI 비활성화"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_adapter.py:274
msgid "Server startup timeout in seconds"
msgstr "서버 시작 제한 시간(초)"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:102
msgid ""
"Directory to download and load the weights, default to the default cache dir "
"of huggingface."
msgstr "가중치를 다운로드하고 로드할 디렉토리, 기본값은 Hugging Face의 기본 캐시 디렉토리입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:112
msgid ""
"The format of the model weights to load.\n"
"\n"
"* \"auto\" will try to load the weights in the safetensors format and fall "
"back to the pytorch bin format if safetensors format is not available.\n"
"* \"pt\" will load the weights in the pytorch bin format.\n"
"* \"safetensors\" will load the weights in the safetensors format.\n"
"* \"npcache\" will load the weights in pytorch format and store a numpy "
"cache to speed up the loading.\n"
"* \"dummy\" will initialize the weights with random values, which is mainly "
"for profiling.\n"
"* \"tensorizer\" will load the weights using tensorizer from CoreWeave. See "
"the Tensorize vLLM Model script in the Examples section for more "
"information.\n"
"* \"runai_streamer\" will load the Safetensors weights using Run:aiModel "
"Streamer \n"
"* \"bitsandbytes\" will load the weights using bitsandbytes quantization.\n"
msgstr "로드할 모델 가중치의 형식입니다.\n"
"\n"
"* \"auto\": safetensors 형식으로 가중치를 로드하려고 시도하며, safetensors 형식을 사용할 수 없는 경우 pytorch 바이너리 형식으로 대체합니다.\n"
"* \"pt\": pytorch 바이너리 형식으로 가중치를 로드합니다.\n"
"* \"safetensors\": safetensors 형식으로 가중치를 로드합니다.\n"
"* \"npcache\": pytorch 형식으로 가중치를 로드하고 numpy 캐시를 저장하여 로드 속도를 높입니다.\n"
"* \"dummy\": 주로 프로파일링을 위해 가중치를 임의의 값으로 초기화합니다.\n"
"* \"tensorizer\": CoreWeave의 tensorizer를 사용하여 가중치를 로드합니다. 자세한 내용은 예제 섹션의 Tensorize vLLM 모델 스크립트를 참조하세요.\n"
"* \"runai_streamer\": Run:aiModel Streamer를 사용하여 Safetensors 가중치를 로드합니다.\n"
"* \"bitsandbytes\": bitsandbytes 양자화를 사용하여 가중치를 로드합니다.\n"

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:149
msgid ""
"The format of the model config to load.\n"
"\n"
"* \"auto\" will try to load the config in hf format if available else it "
"will try to load in mistral format "
msgstr "로드할 모델 구성의 형식입니다.\n"
"\n"
"* \"auto\": 사용 가능한 경우 hf 형식으로 구성을 로드하려고 시도하고, 그렇지 않으면 미스트랄 형식으로 로드하려고 시도합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:164
msgid ""
"Data type for model weights and activations.\n"
"\n"
"* \"auto\" will use FP16 precision for FP32 and FP16 models, and BF16 "
"precision for BF16 models.\n"
"* \"half\" for FP16. Recommended for AWQ quantization.\n"
"* \"float16\" is the same as \"half\".\n"
"* \"bfloat16\" for a balance between precision and range.\n"
"* \"float\" is shorthand for FP32 precision.\n"
"* \"float32\" for FP32 precision."
msgstr ""
"모델 가중치 및 활성화 함수에 사용되는 데이터 유형입니다.\n"
"\n"
"* \"auto\"는 FP32 및 FP16 모델에 FP16 정밀도를 사용하고, BF16 모델에 BF16 정밀도를 사용합니다.\n"
"* \"half\"는 FP16을 나타냅니다. AWQ 양자화에 권장됩니다.\n"
"* \"float16\"은 \"half\"와 동일합니다.\n"
"* \"bfloat16\"은 정밀도와 범위 간의 균형을 위해 사용됩니다.\n"
"* \"float\"은 FP32 정밀도의 약어입니다.\n"
"* \"float32\"는 FP32 정밀도를 나타냅니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:180
msgid ""
"Data type for kv cache storage. If \"auto\", will use model data type. CUDA "
"11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. ROCm (AMD GPU) supports fp8 "
"(=fp8_e4m3)"
msgstr ""
"KV 캐시 저장에 사용되는 데이터 유형입니다. \"auto\"로 설정하면 모델의 데이터 유형을 사용합니다. CUDA 11.8 이상은 fp8(=fp8_e4m3)과 fp8_e5m2를 지원합니다. ROCm(AMD GPU)은 fp8(=fp8_e4m3)를 지원합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:190
msgid "Random seed for operations."
msgstr "연산을 위한 난수 시드입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:197
msgid ""
"Model context length. If unspecified, will be automatically derived from the "
"model config."
msgstr ""
"모델의 컨텍스트 길이입니다. 지정하지 않으면 모델 구성에서 자동으로 파생됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:206
msgid ""
"Backend to use for distributed model workers, either \"ray\" or \"mp\" "
"(multiprocessing). If the product of pipeline_parallel_size and "
"tensor_parallel_size is less than or equal to the number of GPUs available, "
"\"mp\" will be used to keep processing on a single host. Otherwise, this "
"will default to \"ray\" if Ray is installed and fail otherwise. Note that "
"tpu only supports Ray for distributed inference."
msgstr ""
"분산 모델 작업자에 사용할 백엔드입니다. \"ray\" 또는 \"mp\"(멀티프로세싱) 중 하나를 선택할 수 있습니다. pipeline_parallel_size와 tensor_parallel_size의 곱이 사용 가능한 GPU 수보다 작거나 같으면 단일 호스트에서 처리하기 위해 \"mp\"가 사용됩니다. 그렇지 않을 경우 Ray가 설치되어 있으면 기본값으로 \"ray\"가 사용되며, 설치되어 있지 않으면 실패합니다. 참고로 TPU는 분산 추론에 Ray만 지원합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:220
msgid "Number of pipeline stages."
msgstr "파이프라인 단계 수입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:226
msgid "Number of tensor parallel replicas."
msgstr "텐서 병렬 복제본 수입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:233
msgid ""
"Load model sequentially in multiple batches, to avoid RAM OOM when using "
"tensor parallel and large models."
msgstr "모델을 여러 배치로 순차적으로 로드하여 텐서 병렬 및 대형 모델 사용 시 RAM OOM을 방지합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:242
msgid ""
"Token block size for contiguous chunks of tokens. This is ignored on neuron "
"devices and set to ``--max-model-len``. On CUDA devices, only block sizes up "
"to 32 are supported. On HPU devices, block size defaults to 128."
msgstr "연속적인 토큰 청크에 대한 토큰 블록 크기입니다. 이는 뉴런 장치에서는 무시되며 ``--max-model-len``으로 설정됩니다. CUDA 장치에서는 최대 32까지의 블록 크기만 지원됩니다. HPU 장치에서는 블록 크기가 기본적으로 128로 설정됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:253
msgid "Enables automatic prefix caching. "
msgstr "자동 접두사 캐싱을 활성화합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:259
msgid "CPU swap space size (GiB) per GPU."
msgstr "GPU당 CPU 스왑 공간 크기(GiB)입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:266
msgid ""
"The space in GiB to offload to CPU, per GPU. Default is 0, which means no "
"offloading. Intuitively, this argument can be seen as a virtual way to "
"increase the GPU memory size. For example, if you have one 24 GB GPU and set "
"this to 10, virtually you can think of it as a 34 GB GPU. Then you can load "
"a 13B model with BF16 weight, which requires at least 26GB GPU memory. Note "
"that this requires fast CPU-GPU interconnect, as part of the model is loaded "
"from CPU memory to GPU memory on the fly in each model forward pass."
msgstr "GPU당 CPU로 오프로드할 공간(GiB)입니다. 기본값은 0이며, 이는 오프로드하지 않음을 의미합니다. 직관적으로 이 인수는 GPU 메모리 크기를 가상으로 늘리는 방법으로 볼 수 있습니다. 예를 들어, 24GB GPU가 하나 있고 이를 10으로 설정하면 사실상 34GB GPU로 생각할 수 있습니다. 그러면 적어도 26GB GPU 메모리가 필요한 BF16 가중치를 가진 13B 모델을 로드할 수 있습니다. 각 모델 순전파에서 모델의 일부가 CPU 메모리에서 GPU 메모리로 즉시 로드되므로 이를 위해서는 빠른 CPU-GPU 상호 연결이 필요합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:283
#, python-format
msgid ""
"The fraction of GPU memory to be used for the model executor, which can "
"range from 0 to 1. For example, a value of 0.5 would imply 50%% GPU memory "
"utilization. If unspecified, will use the default value of 0.9. This is a "
"per-instance limit, and only applies to the current vLLM instance.It does "
"not matter if you have another vLLM instance running on the same GPU. For "
"example, if you have two vLLM instances running on the same GPU, you can set "
"the GPU memory utilization to 0.5 for each instance."
msgstr ""
"모델 실행기에 사용할 GPU 메모리의 비율로 0에서 1 사이의 값을 가질 수 있습니다. "
"예를 들어, 0.5의 값은 50% GPU 메모리 사용률을 의미합니다. "
"지정하지 않으면 기본값인 0.9를 사용합니다. 이는 인스턴스별 제한이며 "
"현재 vLLM 인스턴스에만 적용됩니다. 동일한 GPU에서 다른 vLLM 인스턴스를 실행 중이더라도 "
"영향을 받지 않습니다. 예를 들어, 동일한 GPU에서 두 개의 vLLM 인스턴스를 실행 중인 경우 "
"각 인스턴스의 GPU 메모리 사용률을 0.5로 설정할 수 있습니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:298
msgid "Maximum number of batched tokens per iteration."
msgstr "반복당 배치 처리된 토큰의 최대 개수입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:304
msgid "Maximum number of sequences per iteration."
msgstr "반복당 시퀀스의 최대 개수입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:311
msgid ""
"Max number of log probs to return logprobs is specified in SamplingParams."
msgstr ""
"반환할 로그 확률의 최대 개수입니다. 로그 확률은 SamplingParams에 지정됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:320
msgid ""
"The specific model version to use. It can be a branch name, a tag name, or a "
"commit id. If unspecified, will use the default version."
msgstr ""
"사용할 특정 모델 버전입니다. 브랜치 이름, 태그 이름 또는 커밋 ID일 수 있습니다. "
"지정하지 않으면 기본 버전을 사용합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:330
msgid ""
"The specific revision to use for the model code on Hugging Face Hub. It can "
"be a branch name, a tag name, or a commit id. If unspecified, will use the "
"default version."
msgstr ""
"Hugging Face Hub에서 모델 코드에 사용할 특정 리비전입니다. 브랜치 이름, 태그 이름 또는 "
"커밋 ID일 수 있습니다. 지정하지 않으면 기본 버전을 사용합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:340
msgid ""
"Revision of the huggingface tokenizer to use. It can be a branch name, a tag "
"name, or a commit id. If unspecified, will use the default version."
msgstr ""
"사용할 Hugging Face 토크나이저의 리비전입니다. 브랜치 이름, 태그 이름 또는 커밋 ID일 수 있습니다. "
"지정하지 않으면 기본 버전을 사용합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:350
msgid ""
"The tokenizer mode.\n"
"\n"
"* \"auto\" will use the fast tokenizer if available.\n"
"* \"slow\" will always use the slow tokenizer. \n"
"* \"mistral\" will always use the `mistral_common` tokenizer."
msgstr ""
"토크나이저 모드입니다.\n"
"\n"
"* \"auto\"는 가능한 경우 빠른 토크나이저를 사용합니다.\n"
"* \"slow\"는 항상 느린 토크나이저를 사용합니다. \n"
"* \"mistral\"은 항상 `mistral_common` 토크나이저를 사용합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:362
msgid ""
"Method used to quantize the weights. If None, we first check the "
"`quantization_config` attribute in the model config file. If that is None, "
"we assume the model weights are not quantized and use `dtype` to determine "
"the data type of the weights."
msgstr ""
"가중치를 양자화하는 데 사용되는 방법입니다. None인 경우 모델 구성 파일의 "
"`quantization_config` 속성을 먼저 확인합니다. 그 값이 None이면 "
"모델 가중치가 양자화되지 않았다고 가정하고 `dtype`을 사용하여 "
"가중치의 데이터 유형을 결정합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:400
msgid ""
"Maximum sequence length covered by CUDA graphs. When a sequence has context "
"length larger than this, we fall back to eager mode. Additionally for "
"encoder-decoder models, if the sequence length of the encoder input is "
"larger than this, we fall back to the eager mode."
msgstr ""
"CUDA 그래프가 지원하는 최대 시퀀스 길이입니다. 시퀀스의 컨텍스트 길이가 이보다 길면 "
"즉시 실행 모드로 돌아갑니다. 또한 인코더-디코더 모델의 경우 인코더 입력의 시퀀스 길이가 "
"이보다 길면 즉시 실행 모드로 돌아갑니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:411
msgid "The worker class to use for distributed execution."
msgstr "분산 실행에 사용할 워커 클래스입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/vllm_adapter.py:415
msgid "Extra parameters, it will be passed to the vllm engine."
msgstr "추가 매개변수이며, vllm 엔진에 전달됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/embed_metadata.py:27
msgid ""
"The embedding model are trained by BAAI, it support more than 100 working "
"languages."
msgstr ""
"임베딩 모델은 BAAI에서 학습시켰으며 100가지 이상의 작업 언어를 지원합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/embed_metadata.py:36
msgid "The embedding model are trained by BAAI, it support Chinese."
msgstr "임베딩 모델은 BAAI에서 학습시켰으며 중국어를 지원합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/embed_metadata.py:44
msgid "The embedding model are trained by BAAI, it support English."
msgstr "임베딩 모델은 BAAI에서 학습시켰으며 영어를 지원합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/embed_metadata.py:54
msgid ""
"The embedding model are trained by Jina AI, it support multiple languages. "
"And it has 0.57B parameters."
msgstr ""
"임베딩 모델은 Jina AI에서 학습시켰으며 다국어를 지원합니다. "
"또한 0.57B개의 매개변수를 가지고 있습니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/embed_metadata.py:65
msgid "The reranker model are trained by BAAI, it support multiple languages."
msgstr "재정렬 모델은 BAAI에서 학습되었으며, 다국어를 지원합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/embed_metadata.py:73
msgid "The reranker model are trained by BAAI, it support Chinese and English."
msgstr "재정렬 모델은 BAAI에서 학습되었으며, 중국어와 영어를 지원합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/embed_metadata.py:85
msgid ""
"The reranker model are trained by Jina AI, it support multiple languages."
msgstr "재정렬 모델은 Jina AI에서 학습되었으며, 다국어를 지원합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:39
msgid "Random seed for llama-cpp models. -1 for random"
msgstr "llama-cpp 모델의 랜덤 시드입니다. -1은 랜덤을 의미합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:45
msgid ""
"Number of threads to use. If None, the number of threads is automatically "
"determined"
msgstr "사용할 스레드 수입니다. None으로 설정하면 스레드 수가 자동으로 결정됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:54
msgid ""
"Maximum number of prompt tokens to batch together when calling llama_eval"
msgstr "llama_eval을 호출할 때 함께 배치할 최대 프롬프트 토큰 수입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:63
msgid ""
"Number of layers to offload to the GPU, Set this to 1000000000 to offload "
"all layers to the GPU."
msgstr "GPU로 오프로드할 레이어 수입니다. 모든 레이어를 GPU로 오프로드하려면 이 값을 1000000000으로 설정하세요."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:70
msgid "Grouped-query attention. Must be 8 for llama-2 70b."
msgstr "그룹화된 쿼리 어텐션입니다. llama-2 70b 모델에서는 8로 설정해야 합니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:73
msgid "5e-6 is a good value for llama-2 models."
msgstr "5e-6은 llama-2 모델에 적합한 값입니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:79
msgid ""
"Maximum cache capacity. Examples: 2000MiB, 2GiB. When provided without "
"units, bytes will be assumed. "
msgstr "최대 캐시 용량입니다. 예: 2000MiB, 2GiB. 단위를 지정하지 않으면 바이트 단위로 간주됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/adapter/llama_cpp_py_adapter.py:88
msgid ""
"If a GPU is available, it will be preferred by default, unless "
"prefer_cpu=False is configured."
msgstr "GPU가 사용 가능한 경우 기본적으로 GPU를 우선 사용합니다. 단, prefer_cpu=False로 구성된 경우는 제외합니다."

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:94
msgid "Database configuration for model registry"
msgstr "모델 등록을 위한 데이터베이스 구성"

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:106
msgid "Model registry configuration. If None, use embedded registry"
msgstr "모델 등록 구성입니다. None인 경우 내장 등록기를 사용합니다."

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:125
msgid "Model API server deploy port"
msgstr "모델 API 서버 배포 포트"

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:129
msgid "The Model controller address to connect"
msgstr "연결할 모델 컨트롤러 주소"

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:189
msgid "Model worker configuration"
msgstr "모델 워커 구성"

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:192
msgid "Model API"
msgstr "모델 API"

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:196
msgid "Model controller"
msgstr "모델 컨트롤러"

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:206
msgid ""
"Default LLM model name, used to specify which model to use when you have "
"multiple LLMs"
msgstr "기본 LLM 모델 이름입니다. 여러 개의 LLM이 있는 경우 사용할 모델을 지정하는 데 사용됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:215
msgid ""
"Default embedding model name, used to specify which model to use when you "
"have multiple embedding models"
msgstr "기본 임베딩 모델 이름입니다. 여러 개의 임베딩 모델이 있는 경우 사용할 모델을 지정하는 데 사용됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/parameter.py:224
msgid ""
"Default reranker model name, used to specify which model to use when you "
"have multiple reranker models"
msgstr "기본 재정렬 모델 이름입니다. 여러 개의 재정렬 모델이 있는 경우 사용할 모델을 지정하는 데 사용됩니다."

#:../packages/dbgpt-core/src/dbgpt/model/utils/chatgpt_utils.py:176
msgid "OpenAI Streaming Output Operator"
msgstr "OpenAI 스트리밍 출력 연산자"

#:../packages/dbgpt-core/src/dbgpt/model/utils/chatgpt_utils.py:180
msgid "The OpenAI streaming LLM operator."
msgstr "OpenAI 스트리밍 LLM 연산자입니다."

#:../packages/dbgpt-core/src/dbgpt/model/utils/chatgpt_utils.py:184
msgid "Upstream Model Output"
msgstr "상위 모델 출력"

#:../packages/dbgpt-core/src/dbgpt/model/utils/chatgpt_utils.py:188
msgid "The model output of upstream."
msgstr "상위 모델의 출력입니다."

#:../packages/dbgpt-core/src/dbgpt/model/utils/chatgpt_utils.py:193
#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:97
#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:152
msgid "Model Output"
msgstr "모델 출력"

#:../packages/dbgpt-core/src/dbgpt/model/utils/chatgpt_utils.py:198
msgid "The model output after transformed to openai stream format."
msgstr "OpenAI 스트림 형식으로 변환된 후의 모델 출력입니다."

#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:73
msgid "LLM Operator"
msgstr "LLM 연산자"

#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:76
msgid "The LLM operator."
msgstr "LLM 연산자입니다."

#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:79
#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:134
msgid "LLM Client"
msgstr "LLM 클라이언트"

#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:84
#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:139
msgid "The LLM Client."
msgstr "LLM 클라이언트입니다."

#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:89
#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:144
msgid "Model Request"
msgstr "모델 요청"

#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:92
#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:147
msgid "The model request."
msgstr "모델 요청입니다."

#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:100
#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:155
msgid "The model output."
msgstr "모델 출력입니다."

#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:127
msgid "Streaming LLM Operator"
msgstr "스트리밍 LLM 연산자"

#:../packages/dbgpt-core/src/dbgpt/model/operators/llm_operator.py:131
msgid "The streaming LLM operator."
msgstr "스트리밍 LLM 算子입니다."